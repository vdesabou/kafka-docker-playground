name: playground
version: 1.0.0
dependencies:
  docker: visit https://docs.docker.com/get-docker to install
help: |-
  🧠 CLI for Kafka Docker Playground 🐳

  👉 Check documentation https://kafka-docker-playground.io/#/cli
filters:
- docker_running

# Use bash code here directly or a function name that references code elsewhere.
help_header_override: |
  echo
  echo $'██████  ██       █████  ██    ██  ██████  ██████   ██████  ██    ██ ███    ██ ██████  '
  echo $'██   ██ ██      ██   ██  ██  ██  ██       ██   ██ ██    ██ ██    ██ ████   ██ ██   ██ '
  echo $'██████  ██      ███████   ████   ██   ███ ██████  ██    ██ ██    ██ ██ ██  ██ ██   ██ '
  echo $'██      ██      ██   ██    ██    ██    ██ ██   ██ ██    ██ ██    ██ ██  ██ ██ ██   ██ '
  echo $'██      ███████ ██   ██    ██     ██████  ██   ██  ██████   ██████  ██   ████ ██████  '
  echo
  
flags:
- long: --vvv
  short: -v
  help: |
    🐛 set verbose output (set -x)

    ❗ it can print sensitive information ❗

- long: --output-level
  short: -o
  arg: level
  allowed: [INFO, WARN, ERROR]
  required: false
  help: |-
    ❕Log level used by all commands

    Default is INFO (all INFO, WARN and ERROR will be printed in command output)

commands:

- name: help
  help: Show help about a command
  args:
  - name: command
    help: 🆘 Help command

- name: status
  help: 🗺️ Show a status

### private commands for completion
- name: get-docker-ports
  help: Return some completion for docker ports
  private: true

- name: get-connector-list
  help: Return some completion for connector list
  private: true

- name: get-ec2-instance-list
  help: Return some completion for ec2 instance list
  private: true
  args:
  - name: cur
    required: false
    help: correspond to completion $cur

- name: get-ec2-cloudformation-list
  help: Return some completion for ec2 cloudformation list
  private: true
  args:
  - name: cur
    required: false
    help: correspond to completion $cur

- name: get-zazkia-connection-list
  help: Return some completion for zazkia connections list
  private: true

- name: generate-fzf-find-files
  help: force call to generate_fzf_find_files
  private: true

- name: generate-tag-list
  help: generate the confluent platform tag list
  private: true

- name: generate-connector-plugin-list
  help: generate the confluent hub plugin list
  private: true

- name: generate-kafka-region-list
  help: generate the confluent kafka region list
  private: true
  dependencies:
    confluent: visit https://docs.confluent.io/confluent-cli/current/overview.html to install
    
- name: get-connector-plugin
  help: Return some completion for connector plugin
  private: true
  args:
  - name: cur
    required: false
    help: correspond to completion $cur

- name: get-ccloud-environment-list
  help: Return some completion for ccloud environment
  private: true
  args:
  - name: cur
    required: false
    help: correspond to completion $cur

- name: get-ccloud-cluster-list
  help: Return some completion for ccloud cluster
  private: true
  args:
  - name: cur
    required: false
    help: correspond to completion $cur

- name: get-tag-list
  help: Return some completion for confluent platform tag
  private: true
  args:
  - name: cur
    required: false
    help: correspond to completion $cur
  flags:
  - long: --connect-only
    required: false
    help: |-
      connect only

- name: get-kafka-region-list
  help: Return some completion for confluent cloud kafka cluster region list
  private: true
  dependencies:
    confluent: visit https://docs.confluent.io/confluent-cli/current/overview.html to install
  args:
  - name: cur
    required: false
    help: correspond to completion $cur

- name: get-topic-list
  help: Return some completion for topic list
  private: true
  flags:
  - long: --skip-connect-internal-topics
    required: false

- name: get-subject-list
  help: Return some completion for subject list
  private: true
  flags:
  - long: --deleted
    required: false
    help: |-
      🧟 Include soft deleted schemas

- name: get-examples-list-with-fzf
  help: Return some completion for examples list
  private: true
  args:
  - name: cur
    required: false
    help: correspond to completion $cur
  flags:
  - long: --without-repro
    required: false
  - long: --sink-only
    required: false
  - long: --ccloud-only
    required: false
  - long: --connector-only
    required: false
  - long: --repro-only
    required: false
  - long: --environment-only
    required: false
  - long: --fully-managed-connector-only
    required: false
  - long: --ksql-only
    required: false
  - long: --schema-registry-only
    required: false
  - long: --rest-proxy-only
    required: false
  - long: --academy-only
    required: false
  - long: --other-playgrounds-only
    required: false

- name: get-zip-or-jar-with-fzf
  help: Return some completion for zip or jar list
  private: true
  args:
  - name: cur
    required: false
    help: correspond to completion $cur
  flags:
  - long: --type
    required: false
    arg: type
    allowed: [zip, jar]

- name: get-specific-file-extension
  help: Return some completion for extension provided
  private: true
  args:
  - name: cur
    required: false
    help: correspond to completion $cur
  flags:
  - long: --extension
    required: false
    arg: extension

- name: get-any-file-with-fzf
  help: Return some completion for any files
  private: true
  args:
  - name: cur
    required: false
    help: correspond to completion $cur

- name: get-playground-repro-export-with-fzf
  help: Return some completion for export tgz files
  private: true
  args:
  - name: cur
    required: false
    help: correspond to completion $cur

- name: get-predefined-schemas
  help: Return some completion for predefined schemas
  private: true
  args:
  - name: cur
    required: false
    help: correspond to completion $cur

- name: update-readme
  private: true
  environment_variables:
  - name: GH_TOKEN 
    help: GH_TOKEN
  flags:
    - long: --tags
      required: false
      validate: not_empty
      arg: tags
      help: |-
        💯 list of tags
    - long: --generate-for-kb
      required: false

- name: force-ci
  private: true
  args:
  - name: filename
    help: pattern of file to remove. A confirmation will be asked
  flags:
    - long: --all
      required: false
    - long: --force
      required: false

- name: update-docs
  private: true

- name: bashly-reload
  private: true
  dependencies:
    bashly: visit https://bashly.dannyb.co/installation/ to install

- name: state
  private: true
  commands:
  - name: show
    private: true
    help: Show the entire playground.ini file

  - name: get
    help: Read a value from the playground.ini file
    private: true
    args:
    - name: key
      required: true
      help: playground.ini key
    examples:
    - playground state get hello
    - playground state get user.name

  - name: set
    help: Save a value in the playground.ini file
    private: true
    args:
    - name: key
      required: true
      help: playground.ini key
    - name: value
      required: true
      help: playground.ini value
    examples:
    - playground state set hello world
    - playground state set user.email me@example.com

  - name: del
    help: Remove a value from the playground.ini file
    private: true
    args:
    - name: key
      required: true
      help: playground.ini key
    examples:
    - playground state del hello
    - playground state del user.name


- name: config
  help: ⚙️ Configure CLI
  commands:
  - name: show
    private: true
    help: Show the entire playground_config.ini file

  - name: get
    private: true
    help: Read a value from the playground_config.ini file

    args:
    - name: key
      required: true
      help: Config key
    examples:
    - playground config get hello
    - playground config get user.name

  - name: set
    private: true
    help: Save a value in the playground_config.ini file
    args:
    - name: key
      required: true
      help: playground_config.ini key
    - name: value
      required: true
      help: playground_config.ini value
    examples:
    - playground config set hello world
    - playground config set user.email me@example.com

  - name: editor
    help: editor to use to open files
    args:
    - name: editor
      required: true
      help: editor
      validate: editor_exists
    examples:
    - playground config editor vi
    - playground config editor code

  - name: folder_zip_or_jar
    help: |- 
      📂 list of folders where to search for zip or jar
      current folder is always included

      default is ~ (home dir)
    args:
    - name: folder
      required: true
      help: folder
      unique: true
      repeatable: true
      validate: dir_exists
    examples:
    - playground config folder_zip_or_jar ~/Downloads ~/Documents/github/kafka-connect-*
    - playground config folder_zip_or_jar ~/Downloads

  - name: clipboard
    help: copy to clipboard connector config (only working on MacOS)
    args:
    - name: enabled
      required: false
      default: "true"
    examples:
    - playground config clipboard false
    - playground config clipboard true

  - name: open-ccloud-connector-in-browser
    help: |
      when running a fully managed connector example, it opens the connector in browser
    commands:
      - name: automatically
        help: automatically open the connector in browser
        args:
        - name: automatically
          required: false
          default: "true"
        examples:
        - playground config open-ccloud-connector-in-browser automatically false
        - playground config open-ccloud-connector-in-browser automatically true
      - name: browser
        help: browser to use
        args:
        - name: browser
          required: false
          default: ""
        examples:
        - playground config open-ccloud-connector-in-browser browser Safari

  - name: open-grafana-in-browser
    help: |
      when running an example with --enable-jmx-grafana flag, it opens grafana in browser
    commands:
      - name: automatically
        help: automatically open grafana in browser
        args:
        - name: automatically
          required: false
          default: "true"
        examples:
        - playground config open-grafana-in-browser automatically false
        - playground config open-grafana-in-browser automatically true
      - name: browser
        help: browser to use
        args:
        - name: browser
          required: false
          default: ""
        examples:
        - playground config open-grafana-in-browser browser Safari

  - name: container-kill-all-before-run
    help: |
      when running an example, always call playground container kill-all first. If set to false, it will call playground stop instead.
    args:
    - name: enabled
      required: false
      default: "false"
    examples:
    - playground config container-kill-all-before-run false
    - playground config container-kill-all-before-run true

  - name: check-and-update-repo-version
    help: |
      when running an example, always check if repo version is older than 3 days, if disabled, it will skip this check.
    args:
    - name: enabled
      required: false
      default: "true"
    examples:
    - playground config check-and-update-repo-version false
    - playground config check-and-update-repo-version true

- name: ai
  dependencies:
    gemini: visit https://github.com/google-gemini/gemini-cli/tree/main?tab=readme-ov-file#-installation to install
    kafka-mcp-server: visit https://docs.tuannvm.com/kafka-mcp-server#installation to install
  help: |-
    🧞‍♂️  AI
    
    It is using Gemini CI (https://google-gemini.github.io/gemini-cli/) in interactive mode.

    MCP servers are available:

    1. mcp-playground-cli - 
    
      Tools:
      - playground_command_help:
          Get detailed help for playground commands
      - playground_command_suggest:
          Get command suggestions and completions for the Kafka Docker Playground CLI
      - playground_command_validate:
          Validate a complete playground command and suggest corrections

    2. mcp-kafka - Kafka MCP Server (https://docs.tuannvm.com/kafka-mcp-server)

      Tools:
      - cluster_overview:
          Aggregates high-level cluster health data, such as controller, broker/topic/partition counts, and under-replicated/offline partitions.
      - consume_messages:
          Consume a batch of messages from Kafka topics.
      - describe_configs:
          Fetches configuration entries for a specific resource (topic or broker).
      - describe_consumer_group:
          Shows details for a specific consumer group, including state, members, and optionally partition offsets and lag.
      - describe_topic:
          Provides detailed metadata for a specific Kafka topic, including partition leaders, replicas, and ISRs.
      - list_brokers:
          Lists the configured Kafka broker addresses.
      - list_consumer_groups:
          Enumerates active consumer groups known by the Kafka cluster.
      - list_topics:
          Retrieves all topic names along with partition counts and replication factors.
      - produce_message:
          Produce a message to a Kafka topic

      Prompts:
      - kafka_cluster_overview:
          Provides a summary of Kafka cluster health and metrics
      - kafka_consumer_lag_report:
          Provide a detailed report on consumer lag across all consumer groups
      - kafka_health_check:
          Run a comprehensive health check on the Kafka cluster
      - kafka_under_replicated_partitions:
          List topics and partitions where ISR count is less than replication factor

    3. mcp-ccloud - Confluent Cloud MCP Server (https://github.com/confluentinc/mcp-confluent)

      Tools:
      - add-tags-to-topic:
          Assign existing tags to Kafka topics in Confluent Cloud.
      - alter-topic-config:
          Alter topic configuration in Confluent Cloud.
      - create-connector:
          Create a new connector. Returns the new connector information if successful.
      - create-topic-tags:
          Create new tag definitions in Confluent Cloud.
      - create-topics:
          Create one or more Kafka topics.
      - delete-connector:
          Delete an existing connector. Returns success message if deletion was successful.
      - delete-tag:
          Delete a tag definition from Confluent Cloud.
      - delete-topics:
          Delete the topic with the given names.
      - get-topic-config:
          Retrieve configuration details for a specific Kafka topic.
      - list-clusters:
          Get all clusters in the Confluent Cloud environment
      - list-connectors:
          Retrieve a list of "names" of the active connectors. You can then make a read request for a specific connector by name.
      - list-environments:
          Get all environments in Confluent Cloud with pagination support
      - list-schemas:
          List all schemas in the Schema Registry.
      - list-tags:
          Retrieve all tags with definitions from Confluent Cloud Schema Registry.
      - list-topics:
          List all topics in the Kafka cluster.
      - read-connector:
          Get information about the connector.
      - read-environment:
          Get details of a specific environment by ID
      - remove-tag-from-entity:
          Remove tag from an entity in Confluent Cloud.
      - search-topics-by-name:
          List all topics in the Kafka cluster matching the specified name.
      - search-topics-by-tag:
          List all topics in the Kafka cluster with the specified tag.

  catch_all:
    label: gemini cli arguments
    help: arguments to pass to gemini cli, see https://google-gemini.github.io/gemini-cli/docs/cli/configuration.html#command-line-arguments for all options
  examples:
    - playground ai --model gemini-1.5-pro-latest --output-format json

- name: ccloud-costs
  dependencies:
    confluent: visit https://docs.confluent.io/confluent-cli/current/overview.html to install
  help: |-
    💰  Retrieve ccloud costs for a range of dates
    
    All you have to do is to be already logged in with confluent CLI. It is using https://docs.confluent.io/cloud/current/billing/overview.html#retrieve-costs-for-a-range-of-dates

    Cost data can take up to 72 hours to become available.

    Start date can reach a maximum of one year into the past

    One month is the maximum window between start and end dates

    For accuracy, Confluent recommends using a start_date that is at least 72 hours prior to the current date and time.
  flags:
    - long: --start-date
      arg: start_date
      validate: date_format
      required: false
      help: |- 
        🗓️ start date in format yyyy-mm-dd

        Default is 30 days ago
    - long: --end-date
      arg: end_date
      validate: date_format
      required: false
      help: |- 
        🗓️ end date in format yyyy-mm-dd

          Default is 3 days ago
    - long: --display-only-total-cost
      private: true
      help: |-
        💰 display only total cost

- name: ccloud-costs-history
  dependencies:
    confluent: visit https://docs.confluent.io/confluent-cli/current/overview.html to install
  help: |-
    👛  Retrieve ccloud costs for each month since last year
    
    All you have to do is to be already logged in with confluent CLI. It is using https://docs.confluent.io/cloud/current/billing/overview.html#retrieve-costs-for-a-range-of-dates

    By default it will only display total cost in dollars. Use the --detailed flag to see more information.

  flags:
    - long: --detailed
      help: |-
        💹 display all details for each month

- name: run
  group: Run
  dependencies:
    fzf: visit https://github.com/junegunn/fzf#installation to install
  help: |-
    🕹️ Run any example !

    🔥 It start an interactive mode where you'll be fully guided !


    ⛅ When running Confluent Cloud (ccloud) example:

      All you have to do is to be already logged in with confluent CLI.

      By default, a new Confluent Cloud environment with a Cluster will be created.

      You can configure the new cluster by setting:
    
      --cluster-type (or CLUSTER_TYPE environment variable): The type of cluster (possible values: basic, standard and dedicated, default basic)
      --cluster-cloud (or CLUSTER_CLOUD environment variable): The Cloud provider (possible values: aws, gcp and azure, default aws)
      --cluster-region (or CLUSTER_REGION environment variable): The Cloud region (use confluent kafka region list to get the list, default eu-west-2 for aws, westeurope for azure and europe-west2 for gcp)
      --cluster-environment (or ENVIRONMENT environment variable) (optional): The environment id where want your new cluster (example: txxxxx) 

      In case you want to use your own existing cluster, you need to setup, in addition to previous ones:

      --cluster-name (or CLUSTER_NAME environment variable): The cluster name
      --cluster-creds (or CLUSTER_CREDS environment variable): The Kafka api key and secret to use, it should be separated with colon (example: <API_KEY>:<API_KEY_SECRET>)
      --cluster-schema-registry-creds (or SCHEMA_REGISTRY_CREDS environment variable) (optional, if not set, new one will be created): The Schema Registry api key and secret to use, it should be separated with colon (example: <SR_API_KEY>:<SR_API_KEY_SECRET>)

    ❗ this command will kill all containers using "playground container kill-all", you can disable this by running "playground config container-kill-all-before-run false"

  flags:
  - &environment-run
    long: --environment
    required: false
    default: "plaintext"
    arg: environment
    allowed: [ccloud, plaintext, sasl-ssl, sasl-plain, 2way-ssl, sasl-scram, kerberos, ssl_kerberos, ldap-authorizer-sasl-plain, ldap-sasl-plain, rbac-sasl-plain]
    help: |-
      🔐 The environment to start when running a connector example 
      
      - plaintext
      - ccloud
      - 2way-ssl
      - kerberos
      - ldap-authorizer-sasl-plain
      - ldap-sasl-plain
      - rbac-sasl-plain
      - sasl-plain
      - sasl-scram
      - sasl-ssl
      - ssl_kerberos

      Default is plaintext.
      This is only supported when example is a connector example
  - long: --file
    short: -f
    required: false
    
    validate: file_exists_with_trick
    arg: file
    completions:
      - $(playground get-examples-list-with-fzf "$cur")
    help: |-
      🔖 Example file to run

      ❕ It must be absolute full path

      🎓 Tip: use <tab> completion to trigger fzf completion
  - long: --open
    
    short: -o
    help: 📖 Opening example files (including docker-compose file) with text editor set with playground config editor <editor> (default is code)
  - &tag
    long: --tag
    
    arg: tag
    required: false
    completions:
      - $(playground get-tag-list "$cur")
    help: |-
      🎯 Confluent Platform (CP) version to use. Applies to all components (broker, connect, schema registry, ksqlDB, etc...)

      It sets TAG environment variable

      Must be greater or equal to 5.3.0

      🎓 Tip: use <tab> completion to trigger fzf completion
  - &connect-tag
    long: --connect-tag
    arg: connect_tag
    required: false
    completions:
      - $(playground get-tag-list "$cur" --connect-only)
    help: |-
      🔗 Confluent Platform (CP) version to use. Applies to connect only.

      It sets CP_CONNECT_TAG environment variable

      Must be greater or equal to 5.3.0

      🎓 Tip: use <tab> completion to trigger fzf completion
  - &connector-tag
    long: --connector-tag
    arg: connector_tag
    
    required: false
    help: |- 
      🔗 Connector version to use

      By default, for each connector, the latest available version on Confluent Hub is used

      🎓 Tip: set to " " in order to select the version dynamically

    conflicts: [--connector-zip]
  - &connector-zip
    long: --connector-zip
    arg: connector_zip
    
    required: false
    validate: file_exists_with_trick
    completions:
      - $(playground get-zip-or-jar-with-fzf --type zip "$cur")
    conflicts: [--connector-tag]
    help: |- 
      🤐 Connector zip to use

      ❕ It must be absolute full path

      🎓 Tip: use <tab> completion to trigger fzf completion 
              use playground config folder_zip_or_jar <folder1> <folder2>... (default is home folder and current folder is always included) to configure where to search the files
              use <option+enter> to use the value you typed manually

  - &connector-jar
    long: --connector-jar
    arg: connector_jar
    
    required: false
    validate: file_exists_with_trick
    completions:
      - $(playground get-zip-or-jar-with-fzf --type jar "$cur")
    help: |-
      🤎 Connector jar to use

      ❕ It must be absolute full path

      🎓 Tip: use <tab> completion to trigger fzf completion 
              use playground config folder_zip_or_jar <folder1> <folder2>... (default is home folder and current folder is always included) to configure where to search the files
              use <option+enter> to use the value you typed manually

  - &enable-ksqldb
    long: --enable-ksqldb
    required: false
    
    help: |-
      🚀 Enable ksqlDB 
      
      ❗ not supported with ccloud examples

      By default, ksqldb-server and ksqldb-cli containers are not started for every test
  - &enable-rest-proxy
    long: --enable-rest-proxy
    required: false
    
    help: |-
      🧲 Enable Rest Proxy

      ❗ not supported with ccloud examples

      By default, rest-proxy container is not started for every test
  - &enable-control-center
    long: --enable-control-center
    required: false
    
    help: |-
      💠 Enable Control Center

      By default, control-center container is not started for every test

      Control Center is reachable at http://127.0.0.1:9021
  - &enable-flink
    long: --enable-flink
    required: false
    
    help: |-
      🐿️ Enable Flink

      By default, flink is not started for every test

      Once enabled, the CLI will ask if you need to download any connectors. Based on the response, you can download one or more connectors from Flink maven repository.

      Flink UI is reacheable using http://127.0.0.1:8081 within the flink child directory. If you enable Flink by starting connector deployment, http://127.0.0.1:18081 will be used.
  - &enable-conduktor
    long: --enable-conduktor
    required: false
    
    help: |- 
      🐺 Enable Conduktor Platform

      By default, Conduktor Platform container is not started for every test

      Conduktor is reachable at http://127.0.0.1:8080/console (admin/admin)
  - &enable-multiple-brokers
    long: --enable-multiple-brokers
    required: false
    
    help: |- 
      3️⃣ Enable multiple brokers

      By default, there is only one broker node enabled
  - &enable-multiple-connect-workers
    long: --enable-multiple-connect-workers
    required: false
    
    help: |-
      🥉 Enable multiple connect worker node

      By default, there is only one connect worker node enabled
  - &enable-jmx-grafana
    long: --enable-jmx-grafana
    required: false
    
    help: |-
      Enable Grafana, Prometheus and Pyroscope

      📊 Grafana is reachable at http://127.0.0.1:3000 (login/password is admin/password)

      🛡️ Prometheus is reachable at http://127.0.0.1:9090
      
      📛 Pyroscope is reachable at http://127.0.0.1:4040
  - &enable-kcat
    long: --enable-kcat
    required: false
    
    help: |-
      🐈‍⬛ Enable kcat

      You can use it with:

      $ docker exec kcat kcat -b broker:9092 -L
  - &enable-sql-datagen
    long: --enable-sql-datagen
    required: false
    
    help: |-
      🌪️ Enable SQL Datagen injection

      This only works for Oracle, MySql, Postgres and Microsoft Sql Server source connector examples with JDBC and Debezium

  - &cluster-cloud
    long: --cluster-cloud
    required: false
    
    arg: cluster-cloud
    allowed: [aws, gcp, azure]
    help: |-
      🌤 The cloud provider: aws, gcp or azure. Default is aws

      🎓 Tip: you can also use CLUSTER_CLOUD environment variable
  - &cluster-type
    long: --cluster-type
    required: false
    
    arg: cluster-type
    allowed: [basic, standard, dedicated]
    help: |-
      🔋 The cluster type: basic, standard or dedicated. Default is basic

      🎓 Tip: you can also use CLUSTER_TYPE environment variable
  - &cluster-region
    long: --cluster-region
    required: false
    
    arg: cluster-region
    completions:
      - $(playground get-kafka-region-list "$cur")
    help: |-
      🗺 The Cloud region. 
      
      🎓 Tip: you can also use CLUSTER_REGION environment variable
      🎓 Tip: use <tab> completion to trigger fzf completion

  - &cluster-environment
    long: --cluster-environment
    required: false
    
    arg: cluster-environment
    validate: not_empty
    completions:
      - $(playground get-ccloud-environment-list "$cur")
    help: |-
      🌐 The environment id where want your new cluster (example: txxxxx)

      ℹ️ Optional, if not set, new environment will be created

      🎓 Tip: you can also use ENVIRONMENT environment variable
      🎓 Tip: use <tab> completion to trigger fzf completion

  - &cluster-name
    long: --cluster-name
    required: false
    
    validate: not_empty
    completions:
      - $(playground get-ccloud-cluster-list "$cur")
    arg: cluster-name
    help: |-
      🎰 The cluster name. 
      
      ❣️ Only required if you want to use your own existing cluster

      🎓 Tip: you can also use CLUSTER_NAME environment variable
      🎓 Tip: use <tab> completion to trigger fzf completion
  - &cluster-creds
    long: --cluster-creds
    required: false
    
    validate: not_empty
    arg: cluster-creds
    help: |-
      🔒 The Kafka api key and secret to use, it should be separated with colon (example: <API_KEY>:<API_KEY_SECRET>)

      ❣️ Only required if you want to use your own existing cluster

      🎓 Tip: you can also use CLUSTER_CREDS environment variable
  - &cluster-schema-registry-creds
    long: --cluster-schema-registry-creds
    required: false
    
    validate: not_empty
    arg: cluster-schema-registry-creds
    help: |-
      🔒 The Schema Registry api key and secret to use, it should be separated with colon (example: <SR_API_KEY>:<SR_API_KEY_SECRET>)

      ℹ️ Optional, if not set, new credentials will be created

      ❣️ Only required if you want to use your own existing cluster
      
      🎓 Tip: you can also use SCHEMA_REGISTRY_CREDS environment variable

  - long: --force-interactive-re-run
    required: false
    private: true
    help: |-
      Force interactive mode
  - long: --force-interactive-repro
    required: false
    private: true
    help: |-
      Force interactive mode
  examples:
  - playground run (interactive mode)

- name: re-run
  group: Run
  help: |-
    ⚡ Simply re-run last example you ran with <playground run>
  dependencies:
    fzf: visit https://github.com/junegunn/fzf#installation to install
  examples:
  - playground re-run (interactive mode)

- name: get-ci-result
  group: Run
  help: 🤖 get CI result for current example
  
- name: history
  group: Run
  help: |-
    🏰 Get an history of the examples which were run with run command and run it again
  dependencies:
    fzf: visit https://github.com/junegunn/fzf#installation to install

- name: start-environment
  private: true
  group: Run
  help: |-
    🔐 Simply start an environment listed in http://tinyurl.com/y4ybbw32:

    - ccloud
    - 2way-ssl
    - kerberos
    - ldap-authorizer-sasl-plain
    - ldap-sasl-plain
    - mdc-kerberos
    - mdc-plaintext
    - mdc-sasl-plain
    - plaintext
    - rbac-sasl-plain
    - sasl-plain
    - sasl-scram
    - sasl-ssl
    - ssl_kerberos

    Note: when running an example with <playground run>, it is already automatically done
  flags:
    - &environment
      long: --environment
      required: false
      default: "plaintext"
      arg: environment
      allowed: [ccloud, 2way-ssl, kerberos, ldap-authorizer-sasl-plain, ldap-sasl-plain, mdc-kerberos, mdc-plaintext, mdc-sasl-plain, plaintext, rbac-sasl-plain, sasl-plain, sasl-scram, sasl-ssl, ssl_kerberos]
      help: |-
        🔐 The environment to start . 
        
        - ccloud
        - 2way-ssl
        - kerberos
        - ldap-authorizer-sasl-plain
        - ldap-sasl-plain
        - mdc-kerberos
        - mdc-plaintext
        - mdc-sasl-plain
        - plaintext
        - rbac-sasl-plain
        - sasl-plain
        - sasl-scram
        - sasl-ssl
        - ssl_kerberos

        Default is plaintext
    - long: --wait-for-control-center
      help: |- 
        😴 Wait for control-center instead of connect
    - long: --docker-compose-override-file
      short: -f
      validate: file_exists
      arg: docker-compose-override-file
      help: |-
        🔖 docker-compose override file

        ❕ It must be absolute full path
  examples:
  - playground start-environment
  - playground start-environment --environment rbac-sasl-plain

- name: switch-ccloud
  group: Run
  dependencies:
    confluent: visit https://docs.confluent.io/confluent-cli/current/overview.html to install
  help: |-
    🌩️  Switch to ccloud environment.
    
    It will bootstrap ccloud environment based on your previously ran ccloud example.

- name: switch-back
  group: Run
  help: |-
    💺  Switch back from previous environment before switch-ccloud was called.

- name: update-version
  group: Run
  help: |-
    ✨ Update current confluent platform components (all with --tag or only connect with --connect-tag) or connector(s) with new version(s)
  flags:
  - *tag
  - *connect-tag
  - *connector-tag
  - *connector-zip
  - *connector-jar
  examples:
  - playground update-version (interactive mode)
  - playground update-version --tag 6.2.0 --connector-tag=2.5.12,10.5.7
  - playground update-version --connect-tag 6.2.0 --connector-tag=2.5.12,10.5.7

- name: open
  group: Run
  dependencies:
    fzf: visit https://github.com/junegunn/fzf#installation to install
  help: |-
    👐 When --file is not provided, simply open last example you ran with <playground run>

    Otherwise, open any file from the playground using --file.
  flags:
  - long: --file
    short: -f
    required: false
    validate: file_exists_with_trick
    arg: file
    completions:
      - $(playground get-any-file-with-fzf "$cur")
    help: |-
      🔎 Search any file and open it.
      
      ❕ It must be absolute full path

      🎓 Tip: use <tab> completion to trigger fzf completion
              use <option+enter> to use the value you typed manually

  - long: --open-docker-compose
    required: false
    help: |-
      🐳 Also open associated docker-compose file for the current example
  - long: --wait
    private: true
    help: |- 
      😴 Wait

- name: stop
  group: Run
  help: |-
    🛑 Stop currently running example

- name: remove-all-docker-images
  group: Run
  help: |-
    🧨 Remove all docker images (including docker volumes)

- name: remove-cp-docker-images
  group: Run
  help: |-
    🧹 Remove all Confluent Platform docker images related to a version installed locally (a confirmation will be required for every version present)
  flags:
    - long: --version
      required: false
      arg: version
      completions:
        - $(docker image ls | grep confluentinc | grep -v "<none>" | awk '{print $2}' | sort | uniq)
      help: |-
        🔢 Specific version to remove

- name: refresh-cp-docker-images
  group: Run
  help: |-
    🔄 Refresh (pull from Docker) all Confluent Platform docker images related to a version installed locally

        It also refreshes images for aws, gcp and azure cli
  flags:
    - long: --version
      required: true
      arg: version
      completions:
        - $(docker image ls | grep confluentinc | grep -v "<none>" | awk '{print $2}' | sort | uniq)
      help: |-
        🔢 Specific version to pull 

- name: cleanup-cloud-details
  group: Run
  help: |-
    🧼 playground is actively caching ccloud details (https://kafka-docker-playground.io/#/how-to-use?id=%f0%9f%8c%a4%ef%b8%8f-confluent-cloud-examples)
       use this command if you notice that the playground is using unexpected ccloud details


- name: open-docs
  group: Run
  help: |-
    🧑‍🎓 Open Confluent documentation of currently running example
  flags:
  - long: --only-show-url
    help: |-
      🌐 Only show url

- name: open-changelog
  group: Run
  help: |-
    📜 Open playground changelog (https://kafka-docker-playground.io/#/changelog)
  flags:
  - long: --only-show-url
    help: |-
      🌐 Only show url

- name: cleanup-cloud-resources
  group: Run
  dependencies:
    confluent: visit https://docs.confluent.io/confluent-cli/current/overview.html to install
  environment_variables:
  - name: AZ_USER 
    help: Azure user
  - name: AZ_PASS 
    help: Azure password
  - name: GCP_PROJECT 
    help: GCP project
  - name: AWS_ACCESS_KEY_ID 
    help: AWS access key id
  - name: AWS_SECRET_ACCESS_KEY 
    help: AWS secret access key
  - name: GCP_KEYFILE_CONTENT 
    help: GCP keyfile (generated with "cat keyfile.json | jq -aRs .")
  help: |-
    🧹 Cleanup cloud resources that were created by running examples from the playground

    ❗it will remove all resources created by the playground, including topics, connectors, clusters, buckets, redshift cluster, etc...
  flags:
  - long: --force
    help: |-
      ☢️ do not ask for confirmation

      ❗use with caution
  - long: --user
    private: true
    arg: user
    help: |-
      ☢️ force user
  - long: --resource
    arg: resource
    help: |-
      🛁 resource to cleanup

      If not provided, all of them are cleaned up

      🎓 Tip: you can pass multiple resources by specifying --resource multiple times
    required: false
    repeatable: true
    unique: true
    default:
      - aws
      - gcp
      - azure
      - ccloud
      - salesforce
    allowed: 
      - aws
      - gcp
      - azure
      - ccloud
      - salesforce

- name: repro
  expose: always
  group: Repro
  help: |-
    👷‍♂️ Reproduction model commands
  environment_variables:
  - name: OUTPUT_FOLDER 
    help: 📁 Output folder where to generate bootstrapped files
    default: reproduction-models
  dependencies:
    fzf: visit https://github.com/junegunn/fzf#installation to install
  commands:

  - name: export
    help: |-
      📤 Export as tgz file uncommitted reproduction models from the folder of current reproduction model
    dependencies:
      git: visit https://git-scm.com/downloads to install
    flags:
      - long: --all
        required: false
        help: |-
          Export all uncommitted reproduction models

  - name: import
    help: |-
      📥 Import tgz file which was created with export command
    flags:
    - long: --file
      short: -f
      arg: file
      required: false
      validate: file_exists_with_trick
      completions:
        - $(playground get-playground-repro-export-with-fzf "$cur")
      help: |- 
        🤐 playground_repro_export.tgz file

        ❕ It must be absolute full path

        🎓 Tip: use <tab> completion to trigger fzf completion 
                use playground config folder_zip_or_jar <folder1> <folder2>... (default is home folder and current folder is always included) to configure where to search the files

  - name: bootstrap
    help: |-
      🛠  Bootstrap reproduction model, just run <playground repro bootstrap> !

      🔥 HIGHLY RECOMMENDED: start in interactive mode by simple running <playground repro bootstrap> !
      
      👉 Check documentation https://kafka-docker-playground.io/#/reusables?id=%f0%9f%9b%a0-bootstrap-reproduction-model
    flags:
    - long: --file
      short: -f
      required: false
      validate: file_exists_with_trick
      arg: file
      completions:
        - $(playground get-examples-list-with-fzf --without-repro "$cur")
      help: |-
        🔖 Example file to use as basis, if not set, currently running example is used
        
        ❕ It must be absolute full path

        🎓 Tip: use <tab> completion to trigger fzf completion

    - long: --description
      short: -d
      required: false
      validate: not_empty
      arg: description
      help: |-
        💭 Description for the reproduction model

    - long: --producer
      private: true
      short: -p
      arg: producer-type
      conflicts: [--pipeline]
      default: "none"
      allowed: 
        - none
        - avro
        - avro-with-key
        - protobuf
        - protobuf-with-key
        - json-schema
        - json-schema-with-key
      help: |-
        🤎 Java producer type to use
        
        One of avro, avro-with-key, protobuf, protobuf-with-key, json-schema, json-schema-with-key

        🎓 Tip: Most of times, it's much simpler to use 'playground topic produce'. Use java producer only if you have very specific requirements such as specifying record timestamp or to do perf testing (even though CLI is also good for that)

        🎓 Tip: 'with-key' will also produce key with selected converter, otherwise LongConverter is used

    - long: --nb-producers
      private: true
      short: -n
      arg: nb-producers
      required: false
      help: |-
        2️⃣ Number of java producers to generate

    - long: --producer-schema-key
      private: true
      required: false
      help: |-
        🔰 Schema to use for the key

        ✨ Copy and paste the schema you want to use for the key, save and close the file to continue

    - long: --producer-schema-value
      private: true
      required: false
      help: |-
        🔰 Schema to use for the value

        ✨ Copy and paste the schema you want to use for the key, save and close the file to continue

    - long: --custom-smt
      help: |-
        🔧 Add a custom SMT (which is a no-op)

    - long: --pipeline
      required: false
      validate: file_exists_with_trick
      arg: sink_file
      conflicts: [--producer]
      repeatable: true
      completions:
        - $(playground get-examples-list-with-fzf --without-repro --sink-only "$cur")
      help: |-
        🔖 Sink example file to use for creating a pipeline. multiple --pipeline flags can be used to create a pipeline with multiple sinks.
        
        ❕ It must be absolute full path. 

        🎓 Tip: use <tab> completion to trigger fzf completion

    examples:
    - playground repro bootstrap (interactive mode)

- name: get-docker-compose
  group: Kafka
  help: |-
    🐋 Get docker-compose

- name: schema
  expose: always
  group: Schema
  help: |-
     🔰 Schema commands
  commands:

  - name: get
    group: Schema
    filters:
    - not_mdc_environment
    - schema_registry_running
    help: |-
      🔰 Get all schemas versions for specified subject (if --subject is not specified, all subjects will be used)
    flags:
      - &subject
        long: --subject
        required: false
        arg: subject
        conflicts:
          - "id"
        completions:
          - $(playground get-subject-list)
        help: |-
          📛 Subject name

      - long: --id
        required: false
        validate: integer
        arg: id
        conflicts:
          - "subject"
          - "deleted"
        help: |-
           💯 Schema id

      - &verbose
        long: --verbose
        short: -v
        help: 🐞 Show command being ran.

      - long: --deleted
        required: false
        conflicts:
          - "id"
        help: |-
          🧟 Include soft deleted subjects

      - long: --store-in-tmp
        help: store in /tmp folder
        arg: folder
        private: true
    examples:
    - playground schema get
    - playground schema get --subject <SUBJECT>
    - playground schema get --deleted

  - name: register
    group: Schema
    filters:
    - not_mdc_environment
    - schema_registry_running
    help: |-
      ⏺️ Register a schema in specified subject
    flags:
      - &subject_required
        long: --subject
        required: true
        arg: subject
        completions:
          - $(playground get-subject-list)
        help: |-
          📛 Subject name
      - *verbose
      - long: --schema
        arg: schema
        completions:
          - $(playground get-predefined-schemas "$cur")
        default: "-"
        required: false
        help: |- 
          🔥 You can either:
          
          * Set your own schema (avro, json-schema, protobuf) with stdin (see example section). 

          * Use completion to select predefined schemas (or use your own schema file) 🎓 Tip: use <tab> completion to trigger fzf completion
      - long: --id
        required: false
        validate: integer
        arg: id
        help: |-
          ☢️ Force schema id

          ❗it will replace any schema which already exists at given id

      - long: --metadata-property
        arg: metadata-property
        help: |-
          🟡 Add metadata properties to schema contract
          
          See docs: https://docs.confluent.io/platform/current/schema-registry/fundamentals/data-contracts.html#metadata-properties

          🎓 Tip: you can pass multiple properties by specifying --metadata-property multiple times

          Example: --metadata-property "metadata1=value" --metadata-property "metadata2=value"
        required: false
        repeatable: true

    examples: |
      playground schema register --subject test-protobuf << 'EOF'
      syntax = "proto3";
      
      package com.github.vdesabou;
      
      message Customer {
          int64 count = 1;
          string first_name = 2;
          string last_name = 3;
          string address = 4;
      }
      EOF

      playground schema register --subject test-avro --metadata-property test=test --metadata-property test2=test << 'EOF'
      {
          "type": "record",
          "namespace": "com.github.vdesabou",
          "name": "Customer",
          "fields": [
              {
                  "name": "count",
                  "type": "long",
                  "doc": "count"
              },
              {
                  "name": "first_name",
                  "type": "string",
                  "doc": "First Name of Customer"
              },
              {
                  "name": "last_name",
                  "type": "string",
                  "doc": "Last Name of Customer"
              },
              {
                  "name": "address",
                  "type": "string",
                  "doc": "Address of Customer"
              }
          ]
      }
      EOF

  - name: get-compatibility
    filters:
    - not_mdc_environment
    - schema_registry_running
    group: Schema
    help: |-
      🛡️ Get subject-level compatibility
    flags:
    - *subject_required
    - *verbose

  - name: set-compatibility
    filters:
    - not_mdc_environment
    - schema_registry_running
    group: Schema
    help: |-
      🛡️ Set subject-level compatibility
    flags:
    - *subject_required
    - *verbose
    - &compatibility-required
      long: --compatibility
      arg: compatibility
      allowed: 
        - BACKWARD
        - BACKWARD_TRANSITIVE
        - FORWARD
        - FORWARD_TRANSITIVE
        - FULL
        - FULL_TRANSITIVE
        - NONE
      required: true
      help: |-
        Schema Registry compatibility rule

  - name: get-mode
    filters:
    - not_mdc_environment
    - schema_registry_running
    group: Schema
    help: |-
      🔏 Get subject-level mode
    flags:
    - *subject_required
    - *verbose

  - name: set-mode
    filters:
    - not_mdc_environment
    - schema_registry_running
    group: Schema
    help: |-
      🔏 Set subject-level mode

      To enable mode changes on a Schema Registry cluster, you must also set mode.mutability=true in the Schema Registry properties file before starting Schema Registry
    flags:
    - *subject_required
    - *verbose
    - long: --mode
      arg: mode
      allowed: 
        - IMPORT
        - READONLY
        - READWRITE
      required: true
      help: |-
        Schema Registry mode

  - name: set-normalize
    filters:
    - not_mdc_environment
    - schema_registry_running
    group: Schema
    help: |-
      🧽 Set normalize at schema registry level

      See https://docs.confluent.io/platform/current/schema-registry/fundamentals/serdes-develop/index.html#schema-normalization
    flags:
    - *verbose
    - long: --value
      arg: value
      allowed: 
        - "true"
        - "false"
      required: true
      help: |-
        true or false

  - name: delete
    filters:
    - not_mdc_environment
    - schema_registry_running
    group: Schema
    help: |-
      🧟 Delete schema
    flags:
    - &subject_not_required
      long: --subject
      required: false
      arg: subject
      completions:
        - $(playground get-subject-list)
      help: |-
        📛 Subject name to delete:
          
          if --version is provided, only that version will be deleted. Otherwise the complete subject will be deleted
    - long: --version
      required: false
      validate: integer
      arg: version
      help: |-
        🔢 Schema version of the provided subject to delete

        Can only be used when --subject is provided
    - long: --permanent
      required: false
      help: |-
        💀 Hard delete (default is soft delete)
    - *verbose

  - name: derive-schema
    group: Schema
    help: |-
      🪄 Derive a schema based on payload
    flags:
      - long: --payload
        arg: payload
        default: "-"
        required: false
        help: |-
          📦 Payload to derive schema from
      - long: --schema-type
        required: false
        default: "AVRO"
        arg: schema-type
        help: |-
            🧩 Schema Registry schema "type":

            - AVRO
            - JSON (json schema)
            - PROTOBUF

            Default is AVRO
        allowed: 
          - AVRO
          - JSON
          - PROTOBUF
    examples: |
        playground schema derive-schema << EOF
        {"name": "Foo", "Age": {"int": 12}}
        {"name": "Bar", "Age": {"string": "12"}}
        {"sport": "Football"}
        {"_id":{"key":"AA"},"duration":{"numberInt":"240"},"employeeNumber":{"numberInt":"12345"},"isActive":"Y","plannedAbsenceKey":"AA","startTimeMin":{"numberLong":"11599920"}}
        EOF

        playground schema derive-schema --payload '{"sport": "Football"}' --schema-type PROTOBUF

- name: tcp-proxy
  expose: always
  group: TCP Proxy
  help: |-
    🏚 Zazkia TCP Proxy commands

    Simulate TCP connection issues (reset,delay,throttle,corrupt) using emicklei/zazkia (https://github.com/emicklei/zazkia) TCP proxy
  commands:

  - name: start
    help: |-
      💗 Start the TCP proxy and automatically replace connector config with zazkia hostname and port 49998
    flags:
      - long: --hostname
        arg: hostname
        required: true
        completions:
          - $(docker ps --format '{{.Names}}')
        help: |-
          Hostname used by the tcp proxy to forward request
      - long: --port
        arg: port
        validate: integer
        required: true
        completions:
          - $(playground get-docker-ports)
        help: |-
          Port used by the tcp proxy to forward request
      - &throttle-service-response
        long: --throttle-service-response
        arg: throttle-service-response
        validate: integer
        required: false
        default: "0"
        help: |-
          🐌 Throttle service response. This is bytes per second.
      - long: --delay-service-response
        arg: delay-service-response
        validate: integer
        required: false
        default: "0"
        help: |-
          ⏲️ Add milliseconds delay to service response. Default is 0 ms.
      - long: --break-service-response
        arg: break-service-response
        validate: percentage
        required: false
        default: "0"
        help: |-
          💔 Percentage of broken connections. Default is 0%.
      - &service-response-corrupt
        long: --service-response-corrupt
        required: false
        help: |-
          🦹‍♂️ Corrupt service response with random mangled bytes. By default, there is no corruption.
      - long: --skip-automatic-connector-config
        required: false
        help: |-
          🤖 By default, script will attempt to modify automatically the running connector config to use Zazkia proxy.
          
          This flag allows to skip this automatic configuration (only useful if you want to manually update connector config with zazkia tcp proxy details)
    examples:
    - playground tcp-proxy start --hostname mysql --port 3306

  - name: get-connections
    help: |-
      🧲 Get Zazkia active TCP connections config and stats
    flags:
      - &connection-id
        long: --connection-id
        completions:
          - $(playground get-zazkia-connection-list)
        arg: connection-id
        validate: integer
        required: false
        help: |-
          🧲 Zazkia TCP connection id
                
  - name: delay
    help: |-
        ⏲️ Add milliseconds delay to service response.
        Set it to 0 to remove the delay.
    flags:
      - long: --delay-service-response
        arg: delay-service-response
        validate: integer
        required: true
        help: |-
          ⏲️ Add milliseconds delay to service response.
      - *connection-id

  - name: break
    help: |-
        💔 Break sending the response to the client.
        Set it to 0% to remove the delay.
    flags:
      - long: --break-service-response
        arg: break-service-response
        validate: percentage
        required: true
        help: |-
          💔 Percentage of broken connections.
      - *connection-id

  - name: close-connection
    help: |-
        ❌ Close the Zazkia active TCP connections
    flags:
      - *connection-id

  - name: close-all-connection-with-error
    help: |-
        🧹 Close all Zazkia TCP connections which are in error state (close all with error button in Zazkia UI)

  - name: toggle-accept-connections
    help: |-
        🙅‍♂️ Change whether new connections can be accepted

  - name: toggle-reads-client
    help: |-
        ✅ Change whether reading data from the client is enabled.

          See diagram in https://github.com/emicklei/zazkia#how-does-it-work- (or locally http://localhost:9191/help.html)
    flags:
      - *connection-id

  - name: toggle-reads-service
    help: |-
        ✅ Change whether reading data from the service is enabled.

          See diagram in https://github.com/emicklei/zazkia#how-does-it-work- (or locally http://localhost:9191/help.html)
    flags:
      - *connection-id

  - name: toggle-writes-client
    help: |-
        ✅ Change whether writing data to the client is enabled.

          See diagram in https://github.com/emicklei/zazkia#how-does-it-work- (or locally http://localhost:9191/help.html)
    flags:
      - *connection-id

  - name: toggle-writes-service
    help: |-
        ✅ Change whether reading data to the service is enabled.

          See diagram in https://github.com/emicklei/zazkia#how-does-it-work- (or locally http://localhost:9191/help.html)
    flags:
      - *connection-id

  - name: open-ui
    help: |-
        🌐 Just open Zazkia UI (http://localhost:9191) in your browser

- name: tools
  expose: always
  group: Tools
  help: |-
    🧰 Tools commands
  commands:
  - name: install-vscode-extension
    help: |-
      🪄 Install a slightly modified version of "Shell Script Command Completion" Visual Studio Code extension (https://marketplace.visualstudio.com/items?itemName=tetradresearch.vscode-h2o)

      After installation, install "playground" command:

      * Go on a .sh file

      * Type Ctrl+Shift+P (or ⌘+⇧+P on macOS) and choose "Shell Completion: Load Command Spec (experimental)"" and then type "playground"
      
      👉 Check documentation https://kafka-docker-playground.io/#/cli?id=%f0%9f%aa%84-setup-shell-script-command-completion-visual-studio-code-extension
    examples:
    - playground tools install-vscode-extension
    dependencies:
      code: visit https://code.visualstudio.com/docs/setup/mac#_launching-from-the-command-line to install
  - name: read-avro-file
    help: |-
      🔖 Read provided avro file
    flags:
    - long: --file
      short: -f
      required: false
      completions:
        - $(playground get-specific-file-extension "$cur" --extension "avro")
      validate: file_exists_and_avro
      arg: file
      help: |-
        🔖 Avro file to read

        ❕ It must be absolute full path

        🎓 Tip: use <tab> completion to trigger fzf completion
                use <option+enter> to use the value you typed manually

  - name: read-parquet-file
    help: |-
      🔖 Read provided parquet file
    flags:
    - long: --file
      short: -f
      required: false
      completions:
        - $(playground get-specific-file-extension "$cur" --extension "parquet")
      validate: file_exists_and_parquet
      arg: file
      help: |-
        🔖 Parquet file to read

        ❕ It must be absolute full path

        🎓 Tip: use <tab> completion to trigger fzf completion
                use <option+enter> to use the value you typed manually

  - name: certs-create
    help: |-
      🔐 Generate keys and certificates used for SSL
    flags:
    - long: --container
      arg: container
      help: |-
        🐳 container name

        🎓 Tip: you can pass multiple containers by specifying --container multiple times
      required: false
      repeatable: true
      unique: true
      default:
        - broker 
        - broker2 
        - broker3
        - client 
        - schema-registry 
        - restproxy
        - connect
        - connect2
        - connect3
        - control-center
        - clientrestproxy
        - ksqldb-server
        - conduktor
    - *verbose
    - long: --output-folder
      required: true
      arg: folder
      help: |-
        📁 Folder where certificates are created

- name: debug
  expose: always
  group: Debug
  help: |-
    🐞 Debug commands
  commands:

  - name: enable-remote-debugging
    help: |-
      ✨ Enable java remote debugging for container
      
      👉 Check documentation https://kafka-docker-playground.io/#/reusables?id=%e2%9c%a8-remote-debugging
    flags:
      - &container
        long: --container
        short: -c
        required: false
        default: "connect"
        arg: container
        repeatable: true
        completions:
          - $(docker ps --format '{{.Names}}')
        help: |-
          🐳 Container name

          🎓 Tip: you can pass multiple containers by specifying --container multiple times
    examples:
    - playground debug enable-remote-debugging
    - playground debug enable-remote-debugging --container broker
    - playground debug enable-remote-debugging -c broker
    - playground debug enable-remote-debugging -c connect -c broker
    - playground debug enable-remote-debugging --container schema-registry --container ksqldb-server --container control-center

  - name: testssl
    help: |-
      🔐 Testing TLS/SSL encryption using https://testssl.sh/
    flags:
      - long: --uri
        arg: uri
        required: true
        help: |-
          host|host:port|URL|URL:port   port 443 is default, URL can only contain HTTPS protocol
    catch_all:
      label: testssl arguments
      help: arguments to pass to testssl, see https://testssl.sh for all options
    examples:
    - playground debug testssl --uri "https://google.com" --fast
    - playground debug testssl --uri "pkc-xxxx.us-west-2.aws.confluent.cloud:9092"

  - name: generate-diagnostics
    help: |-
      ⛑️ Generate a diagnostic bundle with Diagnostics Bundle Tool

      ⚠️ only connect and broker containers are supported for now

      see https://docs.confluent.io/platform/current/tools/diagnostics-tool.html#collect-diagnostics
    flags:
      - *container
    examples:
    - playground debug generate-diagnostics
    - playground debug generate-diagnostics --container broker

  - name: thread-dump
    help: |-
      🎯 Take a java thread dump

      🔖 It will save output to a file and open with text editor set with playground config editor <editor> (default is code)
    flags:
      - *container
    examples:
    - playground debug thread-dump
    - playground debug thread-dump --container broker
    - playground debug thread-dump -c connect -c broker
    - playground debug thread-dump --container schema-registry --container ksqldb-server

  - name: heap-dump
    help: |-
      👻 Take a heap dump

      🔖 It will save output to a .hprof file. VisualVM (https://visualvm.github.io/) or MAT (https://www.eclipse.org/mat/) can be used to read the file.

      It will run a full gc first. If you don't want this, use 
    flags:
      - *container
      - long: --live
        required: false
        help: |-
          🧬 dump only live objects; if not specified, all objects in the heap are dumped
      - long: --histo
        required: false
        help: |-
          📊 print histogram of java object heap
    examples:
    - playground debug heap-dump
    - playground debug heap-dump --container broker
    - playground debug heap-dump -c connect -c broker
    - playground debug heap-dump --container schema-registry --live
    - playground debug heap-dump -c connect -c broker --histo

  - name: tcp-dump
    help: |-
      🕵️‍♂️ Take a tcp dump (sniffing network)
    flags:
      - *container
      - long: --port
        arg: port
        validate: integer
        required: false
        completions:
          - $(playground get-docker-ports)
        help: |-
          Port on which tcp dump should be done, if not set sniffing is done on every port
      - long: --duration
        arg: duration
        validate: integer
        required: false
        default: "30"
        help: |-
          Duration of the dump (default is 30 seconds).
    examples:
    - playground debug tcp-dump --container control-center --port 9021 --duration 60

  - name: block-traffic
    help: |-
      🚫 Blocking traffic using iptables
    flags:
      - *container
      - long: --destination
        arg: destination
        required: true
        help: |-
          Destination: it could be an ip address, a container name or a hostname
      - long: --port
        arg: port
        validate: integer
        required: false
        completions:
          - $(playground get-docker-ports)
        help: |-
          Port on which tcp traffic should be blocked
      - &action
        long: --action
        allowed: [start, stop]
        arg: action
        required: true
        help: |-
          🟢 start or stop
    examples:
    - playground debug block-traffic --destination google.com --action start
    - playground debug block-traffic --container broker --destination zookeeper --action start
    - playground debug block-traffic -c broker -c connect --destination zookeeper --action start
    - playground debug block-traffic --container schema-registry --destination broker --port 9092 --action start
    - playground debug block-traffic -c connect -c ksqldb-server --destination schema-registry --action stop

  - name: java-debug
    help: |-
      🤎 JVM arguments for SSL, Kerberos or Class Loading
    flags:
      - *container
      - long: --type
        arg: type
        allowed: [ssl_all, ssl_handshake, class_loading, kerberos]
        required: true
        help: |-
          - ssl_all: Enable all SSL debugging, i.e -Djavax.net.debug=all
          - ssl_handshake: Enable SSL handshake debugging, i.e -Djavax.net.debug=ssl:handshake
          - class_loading: Enable class loading debugging, i.e -verbose:class
          - kerberos: Enable Kerberos debugging, i.e -Dsun.security.krb5.debug=true
      - &action
        long: --action
        allowed: [enable, disable]
        arg: action
        required: false
        default: "enable"
        help: |-
          🟢 enable or disable
    examples:
    - playground debug java-debug --type class_loading
    - playground debug java-debug --container broker --action start

  - name: jscissors
    help: |-
      ✂️ jscissors is an instrumentation framework and can help to analyse control flow and perform some specific logging

      * Control Flow Tracing: By dynamically instrumenting Java code, the tool meticulously tracks the precise sequence of executed methods at runtime. This capability offers invaluable insights into how an application navigates its logic. The tool provides a way to generate a stack trace when a method is called. 
      * Additional Logging: This enhanced logging capability provides a way to print the input arguments to a method and return value from a method. 
      * Delve into heap area: Generating heap dump at a method call will be crucial to analyze the state of JVM at a given point. The tool has capabilities to generate thread dump as well to track the state of all the threads when a method is called. 

    flags:
      - *container
      - long: --operation
        arg: operation
        help: |-
          🥼 operation to perform:

          * VALUES: print method input arguments
          * RETURN_VALUE: print method return value
          * THREADS: print thread dump when method is called
          * HEAP: print heap dump when method is called
          * STACK: print stack trace when method is called
          * EXCEPTION_HEAP: print heap dump when an exception is thrown during method execution
          * DELAY: print time taken for method execution

          Default values: VALUES, RETURN_VALUE, DELAY and STACK

          🎓 Tip: you can pass multiple operations by specifying --operation multiple times
        required: false
        repeatable: true
        unique: true
        allowed: 
          - VALUES
          - RETURN_VALUE
          - THREADS
          - HEAP
          - STACK
          - EXCEPTION_HEAP
          - DELAY
        default:
          - VALUES
          - RETURN_VALUE
          - DELAY
          - STACK
      - long: --class
        arg: class
        required: true
        help: |-
          class name to instrument (it supports regex like oracle.jdbc.*)
      - long: --method
        arg: method
        required: false
        default: ".*"
        help: |-
          method name to instrument (it supports regex like .*find.*)
      - *action
    examples:
    - playground debug jscissors
    - playground debug jscissors --container broker


  - name: flight-recorder
    help: |-
      🛩️ Record flight recorder

      Read more about it at https://www.baeldung.com/java-flight-recorder-monitoring

      Open the jfr file with JDK Mission Control JMC(https://jdk.java.net/jmc/)
    flags:
      - *container
      - *action
    examples:
    - playground debug flight-recorder --action start
    - playground debug flight-recorder --action stop

  - name: log-level
    help: |-
      🧬 Set log level for any package
    filters:
    - connect_running
    commands:
    - name: get
      help: Get log levels
      flags:
      - &package
        long: --package
        short: -p
        required: false
        validate: not_empty
        arg: package
        help: |-
          Package name

    - name: set
      help: Set log level for specific logger
      flags:
      - long: --package
        short: -p
        required: true
        validate: not_empty
        arg: package
        help: |-
          📦 Package name

      - &level
        long: --level
        short: -l
        arg: level
        allowed: [INFO, WARN, DEBUG, TRACE]
        required: true
        help: |-
          ❕Log level

    examples:
    - playground debug log-level get
    - playground debug log-level get -p io.confluent.connect.oracle.cdc
    - playground debug log-level get --package io.confluent.connect.oracle.cdc
    - playground debug log-level set -p io.confluent.connect.oracle.cdc.logging.LogUtils -l TRACE

- name: get-jmx-metrics
  group: Kafka
  help: |-
    🔢 Get JMX metrics from a container
    
    👉 Check documentation https://kafka-docker-playground.io/#/how-to-use?id=%f0%9f%94%a2-jmx-metrics
  dependencies:
    java: visit https://openjdk.org/install/ to install
  flags:
  - *container
  - long: --open
    short: -o
    help: |- 
      🔖 Save output to a file and open with text editor set with playground config editor <editor> (default is code)

  - long: --domain
    short: -d
    required: false
    arg: domain
    help: |-
      Domain name

  examples:
  - playground get-jmx-metrics --container connect
  - playground get-jmx-metrics --container connect --domain "kafka.connect kafka.consumer kafka.producer"
  - playground get-jmx-metrics -c broker
  - playground get-jmx-metrics -c connect -c broker
  - playground get-jmx-metrics --container schema-registry --container ksqldb-server --open
  - playground get-jmx-metrics -c broker -c connect --domain "kafka.server"

- name: container
  expose: always
  group: Container
  help: |-
    🐳 Container commands
  commands:

    - name: get-properties
      help: |-
        📝 Get properties file from a container
        
        👉 Check documentation https://kafka-docker-playground.io/#/how-to-use?id=%f0%9f%93%9d-see-properties-file

      flags:
      - *container
      examples:
      - playground get-properties
      - playground get-properties --container broker
      - playground get-properties -c broker

    - name: recreate
      group: Container
      help: |-
        💫 Recreate container(s)
        
        👉 Check documentation https://kafka-docker-playground.io/#/how-to-use?id=%e2%99%bb%ef%b8%8f-re-create-containers
      flags:
      - long: --ignore-current-versions
        required: false
        help: |-
          Ignore current confluent platform version

          By default, the current version is used

    - name: get-ip-addresses
      group: Container
      help: |-
        🖥️  Get ip address of running containers
      examples:
      - playground get-ip-address-container

    - name: kill-all
      group: Container
      help: |-
        💀 Kill all containers

    - name: logs
      group: Container
      help: |-
        🕵️  Tail and follow container logs

      flags:
      - *container
      - &logsopen
        long: --open
        short: -o
        help: |- 
          🔖 Save output to a file and open with text editor set with playground config editor <editor> (default is code)
        conflicts: [--wait-for-log]
      - &logswaitforlog
        long: --wait-for-log
        short: -w
        arg: log
        validate: not_empty
        help: |- 
          😴 Wait until log appears
        conflicts: [--open]
      - &logsmaxwait
        long: --max-wait
        short: -m
        arg: max_wait
        validate: integer
        default: "600"
        help: |- 
          ⏳ Max time in seconds to wait when using --wait-for-log (default 600s)
        conflicts: [--open]

      examples:
      - playground container logs --container connect
      - playground container logs -c connect --open
      - playground container logs -c connect --wait-for-log "StackOverflowError"
      - playground container logs -c connect -c broker
      - playground container logs --container schema-registry --container ksqldb-server --open
      - playground container logs -c broker -c connect -c schema-registry
      - playground container logs -c connect -c broker --wait-for-log "ERROR" --max-wait 120

    - name: display-error-all-containers
      group: Container
      help: |-
        🔥 Display all ERROR/FATAL logs in all containers. Useful for quick troubleshooting

    - name: ssh
      group: Container
      help: |-
        🛬 SSH into container

      flags:
      - *container

      - long: --shell
        short: -s
        required: false
        default: "bash"
        arg: shell
        allowed: 
          - bash
          - sh
          - ksh
          - zsh
        help: |-
          💾 Shell to use (default is bash)

      examples:
      - playground ssh -c connect
      - playground ssh -c connect -s sh
      - playground ssh --container connect --shell sh
      - playground ssh -c broker
      - playground ssh --container schema-registry --shell zsh

    - name: change-jdk
      group: Container
      help: |-
        🤎 Change java JDK version using Azul JDK (https://www.azul.com/downloads/#downloads-table-zulu)

        PS: It works for UBI8 docker images only

      flags:
      - *container

      - long: --version
        required: true
        arg: version
        allowed: 
          - "8"
          - "11"
          - "17"
          - "21"
          - "22"
        help: |-
          🤎 JDK version to use

      examples:
      - playground change-jdk --container connect --version 17

    - name: exec
      group: Container
      help: |-
        🪄 Execute command in a container

      flags:
      - *container

      - long: --command
        required: true
        validate: not_empty
        arg: command
        help: |-
          📲 Command to execute

      - long: --root
        help: |-
          👑 Run command as root

      - long: --shell
        default: "bash"
        arg: shell
        allowed: 
          - bash
          - sh
          - ksh
          - zsh
        help: |-
          💾 Shell to use (default is bash)

      examples:
      - playground exec -c connect -d "date"
      - playground exec -c connect -d "whoami" --root
      - playground exec --container connect --command "whoami" --shell sh
      - playground exec -c broker -c connect -d "ps aux"
      - playground exec --container schema-registry --container ksqldb-server --command "free -h"
      - playground exec -c connect -c broker -d "netstat -tuln" --root

    - name: restart
      group: Container
      help: |-
        🔁 Restart a container

      flags:
      - *container

    - name: pause
      group: Container
      help: |-
        ⏸️  Pause a container

      flags:
      - *container

    - name: resume
      alias: unpause
      group: Container
      help: |-
        ⏯️  Resume a container

      flags:
      - *container

    - name: kill
      group: Container
      help: |-
        🔫 Kill a container

      flags:
      - *container
      examples:
      - playground container kill --container connect
      - playground container kill -c broker
      - playground container kill -c connect -c broker
      - playground container kill --container schema-registry --container ksqldb-server --container control-center

    - name: set-environment-variables
      group: Container
      help: |-
        📦  Set environment variable(s) for a container

        🎓 Tip: you can pass multiple environment variables by specifying --env multiple times

        Example: --env "KAFKA_OPTS: -verbose:class" --env "CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR,org.apache.kafka.connect.runtime.rest.RestServer=ERROR"

      flags:
      - *container
      - long: --env
        required: false
        repeatable: true
        conflicts:
          - --restore-original-values
        arg: env
        help: |-
          📦 Environment variables to set

          🎓 Tip: you can pass multiple environment variables by specifying --env multiple times
      - long: --restore-original-values
        conflicts:
          - --env
        required: false
        help: |-
          🧽 Restore back original values before any changes was made
      - long: --mount-jscissors-files
        required: false
        private: true
        help: |-
          Mount jscissors files for debugging

    - name: wait-for-connect-rest-api-ready
      group: Container
      private: true
      help: |-
        🚏 wait-for-connect-rest-api-ready
      flags:
        - long: --max-wait
          arg: max_wait
          validate: integer
          default: "300"
          help: |- 
            ⏳ Max time in seconds to wait

- name: topic
  expose: always
  group: Topic
  filters:
  - not_mdc_environment
  help: |-
    🗳 Topic commands
  commands:

    - name: get-number-records
      group: Topic
      help: |-
        💯 Get number of records in a topic
      flags:
      - &topic
        long: --topic
        short: -t
        required: false
        arg: topic
        completions:
          - $(playground get-topic-list)
        help: |-
          🗳 Topic name
      examples:
      - playground get-number-records --topic a-topic
      - playground get-number-records -t a-topic

    - name: display-consumer-offsets
      group: Topic
      help: |-
        📭 Display content of __consumer_offsets topic
      flags:
      - *verbose

    - name: list
      group: Topic
      help: |-
        🔘 List topics

    - name: describe
      group: Topic
      help: |-
        🔬 Describe topic
      flags:
      - *topic
      - *verbose

    - name: set-schema-compatibility
      filters:
      - schema_registry_running
      group: Topic
      help: |-
        🛡️ Change topic's schema compatibility
      flags:
      - *topic
      - *compatibility-required
      - *verbose

    - name: consume
      filters:
      - schema_registry_running
      group: Topic
      help: |-
        📥 Consume topic from beginning
      flags:
      - *verbose
      - *topic
      - long: --max-messages
        arg: max-messages
        validate: integer
        required: false
        default: "100"
        help: |-
          Max number of messages to display (default is 100)

          You can use -1 to display all messages
          
      - long: --min-expected-messages
        arg: min-expected-messages
        validate: integer
        required: false
        default: "0"
        help: |-
          Minimum expected number of messages to be present in topic, returns an error if this is not the case

          Note: --topic should be specified in this case.
      - long: --grep
        arg: grep
        required: false
        default: ""
        help: |-
          Verify that topic content contains record which contains specified string
      - long: --timeout
        arg: timeout
        validate: integer
        required: false
        default: "60"
        help: |-
          Max number of seconds to wait when --min-expected-messages is used.

          Default is 60 seconds
      - long: --tail
        required: false
        help: |-
          Tail on logs.
        conflicts: [--min-expected-messages, --max-messages, --open]

      - long: --plot-latencies-timestamp-field
        required: false
        arg: timestamp
        help: |-
          🗳 Timestamp field name that represents when record was created in source system

      - long: --key-subject
        required: false
        arg: key-subject
        completions:
          - $(playground get-subject-list)
        help: |-
          📛 Subject for key in schema-registry to use (useful when data was produced with --key-subject-name-strategy other than TopicNameStrategy)
          
          Note: --topic should be specified in this case.

      - long: --value-subject
        required: false
        arg: value-subject
        completions:
          - $(playground get-subject-list)
        help: |-
          📛 Subject for value in schema-registry to use (useful when data was produced with --value-subject-name-strategy other than TopicNameStrategy)
          
          Note: --topic should be specified in this case.

      - long: --max-characters
        arg: max-characters
        validate: integer
        required: false
        default: "3000"
        help: |-
          Max characters per message to display (default is 3000)

      - long: --open
        short: -o
        help: |- 
          🔖 Save full dump of topic to a file and open with text editor set with playground config editor <editor> (default is code)
        conflicts: [--max-characters, --tail, --min-expected-messages]

    - name: produce
      filters:
      - schema_registry_running
      group: Topic
      help: |-
         📤 Produce to a topic

         See video tutorial https://youtu.be/mbzHCewG_XE
      flags:

      - long: --key
        arg: key
        required: false
        completions:
          - $(playground get-predefined-schemas "$cur")
        help: |-
          🗝️ Key to use. If not set, no key is used.

          🔥 You can either:
          
          * Set your own schema (avro, json-schema, protobuf) within single quotes (see examples) 
          
          * You can also generate json data using json or sql format using syntax from https://github.com/MaterializeInc/datagen

          * Use completion to select predefined schemas (or use your own schema file) 🎓 Tip: use <tab> completion to trigger fzf completion

          * Directly set payload ("%g" can be used to generate a counter)

          In case of 'raw' data (i.e not using schema):

          If the key contain a number, it will be used as starting point and incremented for each record. 
          
          Example: key1 will start with key1, then key2, etc..
          Example: mykey-10-suffix will start with mykey-10-suffix then mykey-11-suffix, etc..

          "%g" can also be used to generate a counter

          Example: key%g will start with key1, then key2, etc..

          Otherwise, the key will be same for all records.

      - long: --value
        arg: value
        completions:
          - $(playground get-predefined-schemas "$cur")
        default: "-"
        required: false
        help: |- 
          🔥 You can either:
          
          * Set your own schema (avro, json-schema, protobuf) with stdin (see example section). 
          
          * You can also generate json data using json or sql format using syntax from https://github.com/MaterializeInc/datagen

          * Use completion to select predefined schemas (or use your own schema file) 🎓 Tip: use <tab> completion to trigger fzf completion

          * Directly set payload ("%g" can be used to generate a counter)

      - *verbose
      - long: --debug
        short: -d
        required: false
        private: true
        help: |-
          debug mode (internal)
      - long: --topic
        short: -t
        required: true
        arg: topic
        completions:
          - $(playground get-topic-list)
        help: |-
          🗳 Topic name
      - long: --nb-messages
        arg: nb-messages
        validate: integer
        required: false
        default: "1"
        help: |-
          💯 Number of messages to produce (default is 1)
             
          🎓  - if > <value of --max-nb-messages-per-batch> (default 300000), messages will be sent in batches of <value of --max-nb-messages-per-batch> (default 300000) records
              - if you set it to -1, an infinite number of records will also be sent in batches
      - long: --max-nb-messages-per-batch
        arg: max-nb-messages-per-batch
        validate: integer
        required: false
        default: "300000"
        help: |-
          🔼 Max number of messages to send per batch when --nb-messages > --max-nb-messages-per-batch
             if --nb-messages is set to -1, this is the number of messages sent per batch
             default is 300000
      - long: --max-nb-messages-to-generate
        arg: max-nb-messages-to-generate
        validate: integer
        required: false
        help: |-
          🔨 Max number of different messages to generate.

             - when protobuf is used, default is 50 as protobuf generation is really slow
             - when --record-size is set, default is 100
             --when nb-messages is set to -1, default is 1000
             - otherwise default is 100000
      - long: --sleep-time-between-batch
        arg: sleep-time-between-batch
        validate: integer
        required: false
        default: "0"
        help: |-
          💤 Sleep time in seconds between batches
             default is 0
      - long: --nb-partitions
        arg: nb-partitions
        validate: integer
        required: false
        default: "1"
        help: |-
          🔢 Number of partitions for the topic. (default is 1)
          
          ❌ Important: If topic is existing, it will be re-created before producing to topic.
      - long: --compression-codec
        arg: compression-codec
        required: false
        allowed: 
          - gzip
          - snappy
          - lz4
          - zstd
        help: |-
          🤐 The compression codec: either 'gzip', 'snappy', 'lz4', or 'zstd'
          If not set, there is no compression

      - &compatibility
        long: --compatibility
        arg: compatibility
        allowed: 
          - BACKWARD
          - BACKWARD_TRANSITIVE
          - FORWARD
          - FORWARD_TRANSITIVE
          - FULL
          - FULL_TRANSITIVE
          - NONE
        help: |-
          Schema Registry compatibility rule

      - &key-subject-name-strategy
        long: --key-subject-name-strategy
        arg: key-subject-name-strategy
        allowed: 
          - TopicNameStrategy
          - RecordNameStrategy
          - TopicRecordNameStrategy
        help: |-
          Key Subject Name Strategy

      - &value-subject-name-strategy
        long: --value-subject-name-strategy
        arg: value-subject-name-strategy
        allowed: 
          - TopicNameStrategy
          - RecordNameStrategy
          - TopicRecordNameStrategy
        help: |-
          Value Subject Name Strategy

      - long: --headers
        arg: headers
        required: false
        help: |-
          🚏 Headers to use for all records. If not set, no header is used.

          Example: --headers "header1:value1,header2:value2"

          Note: CP 7.2+ is required.
      - long: --forced-key
        arg: forced-key
        required: false
        help: |-
          ☢️ Key to use for all records. 
          
          🎓 Tip: use --generate-only first with avro, json-schema or protobuf to get skeleton of messages and then use --forced-key to send the message you need. 
      - long: --forced-value
        arg: forced-value
        required: false
        help: |-
          ☢️ Value to use for all records. 
          
          🎓 Tip: use --generate-only first with avro, json-schema or protobuf to get skeleton of messages and then use --forced-value to send the message you need. 
      - long: --generate-only
        required: false
        help: |-
          🚪 Only generate messages without sending to kafka topic.

          Used with --forced-value, this is a powerful way to send specific messages.
      - long: --tombstone
        required: false
        help: |-
          ⚰️ Generate tombstone (record with null value). 
          
          Setting --key is recommended when this flag is used. 
          If not set, the key will also be null, hence generating a null record (both key and value being null)

          Note: CP 7.2+ is required.
      - long: --validate
        required: false
        help: |-
          ☑️ Validate schema according to connect sink converter used

      - long: --derive-key-schema-as
        required: false
        arg: derive-key-schema-as
        help: |-
            🪄 Use playground schema derive-schema command to deduce schema from key payload

            Possible values:

            - AVRO
            - JSON (json schema)
            - PROTOBUF
        allowed: 
          - AVRO
          - JSON
          - PROTOBUF

      - long: --derive-value-schema-as
        required: false
        arg: derive-value-schema-as
        help: |-
            🪄 Use playground schema derive-schema command to deduce schema from value payload

            Possible values:

            - AVRO
            - JSON (json schema)
            - PROTOBUF
        allowed: 
          - AVRO
          - JSON
          - PROTOBUF

      - long: --no-null
        required: false
        help: |-
          🪹 Never generate null fields even for optional fields

          N.B: only work with avro and json-schema

      - long: --consume
        required: false
        help: |-
          📥 After producing, directly consume topic.

      - long: --delete-topic
        required: false
        help: |-
          ❌ Delete topic and associated schema/subject if applicable before producing data.

      - long: --reference
        arg: reference
        private: true
        help: |-
          🖇️ Schema reference
          
          See docs: https://docs.confluent.io/platform/7.6/schema-registry/fundamentals/serdes-develop/index.html#schema-references

          🎓 Tip: you can pass multiple references by specifying --reference multiple times

          🔥 You can either:
          
          * Set your own schema (avro, json-schema, protobuf) within single quotes (see examples) 
          
          * Use completion to select predefined schemas (or use your own schema file) 🎓 Tip: use <tab> completion to trigger fzf completion
        required: false
        repeatable: true
        completions:
          - $(playground get-predefined-schemas "$cur")

      - long: --validate-config
        arg: validate-config
        help: |-
          🔩 Converter configuration parameters to use 
          
          See docs: https://docs.confluent.io/platform/current/schema-registry/connect.html#using-kconnect-long-with-sr

          🎓 Tip: you can pass multiple parameters by specifying --validate-config multiple times
        required: false
        repeatable: true
        allowed: 
          - scrub.invalid.names=true
          - enhanced.avro.schema.support=true
          - connect.meta.data=false
          - object.additional.properties=false
          - use.optional.for.nonrequired=true
          - ignore.default.for.nullables=true
          - generalized.sum.type.support=true
          - enhanced.protobuf.schema.support=true
          - generate.index.for.unions=false
          - int.for.enums=true
          - optional.for.nullables=true
          - generate.struct.for.nulls=true
          - wrapper.for.nullables=true
          - wrapper.for.raw.primitives=false

      - long: --producer-property
        arg: producer-property
        help: |-
          🔩 Producer configuration parameters to use 
          
          See docs: https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html#cp-config-producer

          🎓 Tip: you can pass multiple parameters by specifying --producer-property multiple times

          Example: --producer-property "max.request.size=990485760" --producer-property "client.id=myid"
        required: false
        repeatable: true

      - long: --record-size
        arg: record-size
        validate: integer
        required: false
        default: "0"
        help: |-
          🏋️ Record size in bytes, eg. 1048576 for 1MB

          📢 If size is > 1Mb, --producer-property max.request.size and topic max.message.bytes will be automatically set to support the record size.

      - long: --value-schema-id
        arg: value-schema-id
        private: true
        validate: integer
        required: false
        help: |-
          🔰 Do not auto register schema and specify schema id to use. 

          It sets --property value.schema.id=x --property auto.register=false --property use.latest.version=true

      examples: |

        playground topic produce --tombstone --topic a-topic --key mykey

        playground topic produce -t topic-json --nb-messages 10 << 'EOF'
        {
            "_meta": {
                "topic": "",
                "key": "",
                "relationships": []
            },
            "nested": {
                "phone": "faker.phone.imei()",
                "website": "faker.internet.domainName()"
            },
            "id": "iteration.index",
            "name": "faker.internet.userName()",
            "email": "faker.internet.exampleEmail()",
            "phone": "faker.phone.imei()",
            "website": "faker.internet.domainName()",
            "city": "faker.location.city()",
            "company": "faker.company.name()"
        }
        EOF

        playground topic produce -t topic-avro --nb-messages 10 << 'EOF'
        {
          "fields": [
            {
              "name": "count",
              "type": "long"
            },
            {
              "name": "first_name",
              "type": "string"
            },
            {
              "name": "last_name",
              "type": "string"
            },
            {
              "default": null,
              "name": "address",
              "type": [
                "null",
                "string"
              ]
            },
            {
              "name": "last_sale_date",
              "type": {
                "logicalType": "timestamp-millis",
                "type": "long"
              }
            },
            {
              "name": "last_sale_price",
              "type": {
                "logicalType": "decimal",
                "precision": 15,
                "scale": 2,
                "type": "bytes"
              }
            },
            {
              "name": "last_connection",
              "type": {
                "logicalType": "date",
                "type": "int"
              }
            }
          ],
          "name": "Customer",
          "namespace": "com.github.vdesabou",
          "type": "record"
        }
        EOF

        playground topic produce -t topic-json-schema --nb-messages 3 << 'EOF'
        {
          "$schema": "http://json-schema.org/draft-07/schema#",
          "additionalProperties": false,
          "$id": "http://lh.test/Customer.schema.json",
          "title": "Customer",
          "description": "Customer description",
          "type": "object",
          "properties": {
            "name": {
              "description": "Customer name",
              "type": "string",
              "maxLength": 25
            },
            "surname": {
              "description": "Customer surname",
              "type": "string",
              "minLength": 2
            },
            "email": {
              "description": "Email",
              "type": "string",
              "pattern": "^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$"
            }
          },
          "required": [
            "name",
            "surname"
          ]
        }
        EOF


        playground topic produce -t topic-proto --nb-messages 1 << 'EOF'
        syntax = "proto3";

        package com.github.vdesabou;

        message Customer {
            int64 count = 1;
            string first_name = 2;
            string last_name = 3;
            string address = 4;
        }
        EOF

        playground topic produce -t topic-jsql --nb-messages 10 << 'EOF'
        CREATE TABLE "notused"."notused" (
          "id" int PRIMARY KEY,
          "name" varchar COMMENT 'faker.internet.userName()',
          "merchant_id" int NOT NULL COMMENT 'faker.datatype.number()',
          "price" int COMMENT 'faker.datatype.number()',
          "status" int COMMENT 'faker.datatype.boolean()',
          "created_at" datetime DEFAULT (now())
        );
        EOF

        playground topic produce -t topic-json --nb-messages 1 --producer-property "max.request.size=990485760" < bigjson.json

        playground topic produce -t topic-string --nb-messages 5000 << 'EOF'
        Ad et ut pariatur officia eos.
        Nesciunt fugit nam libero ut qui itaque sed earum at itaque nesciunt eveniet atque.
        Quidem libero quis quod et illum excepturi voluptas et in perspiciatis iusto neque.
        Quibusdam commodi explicabo dolores molestiae qui delectus dolorum fugiat molestiae natus assumenda omnis expedita.
        Et sunt aut architecto suscipit fugiat qui voluptate iure vel doloremque eum culpa.
        Qui enim facilis eos similique aperiam totam eius et at dolor dolores.
        Ut sunt quia qui quia consectetur aut reiciendis.
        Modi adipisci iusto aut voluptatem dolores laudantium.
        Sequi sint quia quibusdam molestias minus et aliquid voluptatum aliquam.
        Rerum aut amet quo possimus nihil velit quisquam ut cumque.
        Pariatur ad officiis voluptatibus quia vel corporis ea fugit adipisci porro.
        EOF

        # key and headers
        # mykey1 %g can also be used
        playground topic produce -t topic-json-multiple-lines --nb-messages 10 --key "mykey1" --headers "header1:value1,header2:value2" << 'EOF'
        {"u_name": "scissors", "u_price": 2.75, "u_quantity": 3}
        {"u_name": "tape", "u_price": 0.99, "u_quantity": 10}
        {"u_name": "notebooks", "u_price": 1.99, "u_quantity": 5}
        EOF

        # avro key
        playground topic produce -t topic-avro-with-key --nb-messages 10 --key '
        {
          "fields": [
            {
              "name": "id",
              "type": "long"
            }
          ],
          "name": "Key",
          "namespace": "com.github.vdesabou",
          "type": "record"
        }
        ' << 'EOF'
        {
          "fields": [
            {
              "doc": "count",
              "name": "count",
              "type": "long"
            },
            {
              "doc": "First Name of Customer",
              "name": "first_name",
              "type": "string"
            },
            {
              "doc": "Last Name of Customer",
              "name": "last_name",
              "type": "string"
            }
          ],
          "name": "Customer",
          "namespace": "com.github.vdesabou",
          "type": "record"
        }
        EOF

        # tombstone
        playground topic produce -t topic-json-multiple-lines --tombstone --key "mykey1"

        # input file
        playground topic produce -t topic-avro-example3 < avro-schema.avsc

        # record-size
        playground topic produce -t topic-avro-example-big-size --nb-messages 3 --record-size 10000000 << 'EOF'
        {
          "fields": [
            {
              "doc": "count",
              "name": "count",
              "type": "long"
            },
            {
              "doc": "First Name of Customer",
              "name": "first_name",
              "type": "string"
            },
            {
              "doc": "Last Name of Customer",
              "name": "last_name",
              "type": "string"
            },
            {
              "doc": "Address of Customer",
              "name": "address",
              "type": "string"
            }
          ],
          "name": "Customer",
          "namespace": "com.github.vdesabou",
          "type": "record"
        }
        EOF

        # validate
        playground topic produce -t topic-json-schema-validate --nb-messages 3 --validate << 'EOF'
        {
          "$schema": "http://json-schema.org/draft-07/schema#",
          "additionalProperties": false,
          "$id": "http://lh.test/Customer.schema.json",
          "title": "Customer",
          "description": "Customer description",
          "type": "object",
          "properties": {
            "name": {
              "description": "Customer name",
              "type": "string",
              "maxLength": 25
            },
            "surname": {
              "description": "Customer surname",
              "type": "string",
              "minLength": 2
            },
            "email": {
              "description": "Email",
              "type": "string",
              "pattern": "^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$"
            },
            "holiday": {
              "oneOf": [
                {
                  "title": "Not included",
                  "type": "null"
                },
                {}
              ]
            },
            "f2": {}
          },
          "required": [
            "name",
            "surname"
          ]
        }
        EOF

        #  --value-subject-name-strategy
        playground topic produce -t topic-avro-example-value-subject-name-strategy --nb-messages 10 --value-subject-name-strategy TopicRecordNameStrategy << 'EOF'
        {
          "fields": [
            {
              "doc": "count",
              "name": "count",
              "type": "long"
            },
            {
              "doc": "First Name of Customer",
              "name": "first_name",
              "type": "string"
            },
            {
              "doc": "Last Name of Customer",
              "name": "last_name",
              "type": "string"
            },
            {
              "doc": "Address of Customer",
              "name": "address",
              "type": "string"
            }
          ],
          "name": "Customer",
          "namespace": "com.github.vdesabou",
          "type": "record"
        }
        EOF

        # --generate-only
        playground topic produce -t topic-avro-example-forced-value --nb-messages 10  --generate-only << 'EOF'
        {
          "fields": [
            {
              "doc": "count",
              "name": "count",
              "type": "long"
            },
            {
              "doc": "First Name of Customer",
              "name": "first_name",
              "type": "string"
            },
            {
              "doc": "Last Name of Customer",
              "name": "last_name",
              "type": "string"
            },
            {
              "doc": "Address of Customer",
              "name": "address",
              "type": "string"
            },
            {
              "name": "createdDate",
              "type": {
                "logicalType": "timestamp-millis",
                "type": "long"
              }
            },
            {
              "default": null,
              "name": "warranty_expiration",
              "type": [
                "null",
                {
                  "logicalType": "date",
                  "type": "int"
                }
              ]
            }
          ],
          "name": "Customer",
          "namespace": "com.github.vdesabou",
          "type": "record"
        }
        EOF

        # --forced-value
        playground topic produce -t topic-avro-example-forced-value --nb-messages 1 --forced-value '{"count":4,"first_name":"Vincent","last_name":"de Saboulin","address":"xxx","createdDate":1697852606000,"warranty_expiration":{"int":19653}}' << 'EOF'
        {
          "fields": [
            {
              "doc": "count",
              "name": "count",
              "type": "long"
            },
            {
              "doc": "First Name of Customer",
              "name": "first_name",
              "type": "string"
            },
            {
              "doc": "Last Name of Customer",
              "name": "last_name",
              "type": "string"
            },
            {
              "doc": "Address of Customer",
              "name": "address",
              "type": "string"
            },
            {
              "name": "createdDate",
              "type": {
                "logicalType": "timestamp-millis",
                "type": "long"
              }
            },
            {
              "default": null,
              "name": "warranty_expiration",
              "type": [
                "null",
                {
                  "logicalType": "date",
                  "type": "int"
                }
              ]
            }
          ],
          "name": "Customer",
          "namespace": "com.github.vdesabou",
          "type": "record"
        }
        EOF

        # json schema references
        playground topic produce --value /path/to/scripts/cli/predefined-schemas/json-schema/schema-reference/customer.json --reference /path/to/scripts/cli/predefined-schemas/json-schema/schema-reference/address.json --reference /path/to/scripts/cli/predefined-schemas/json-schema/schema-reference/email.json --topic customers

        # --derive-value-schema-as
        playground topic produce --topic tocpic --derive-value-schema-as PROTOBUF --derive-key-schema-as PROTOBUF --key '{"id": "1"}' << 'EOF'
        {"name": "Foo", "Age": {"int": 12}}
        EOF
    - name: create
      group: Topic
      help: |-
        🆕 Create topic
      flags:
      - *verbose
      - long: --topic
        short: -t
        required: true
        arg: topic
        help: |-
          🗳 Topic name
      - long: --nb-partitions
        arg: nb-partitions
        validate: integer
        required: false
        default: "1"
        help: |-
          Number of partitions for the topic. (default is 1)
      catch_all:
        label: arguments
        help: |-
          Any arguments to be used with kafka-topics --create
        required: false
      examples: |
        playground topic create --topic atopic
        playground topic create --topic atopic --nb-partitions 8 --config retention.ms=30000 --config cleanup.policy=compact
        
    - name: delete
      group: Topic
      help: |-
        ❌ Delete topic and associated schema/subject if applicable
      flags:
      - *verbose
      - long: --topic
        short: -t
        required: true
        arg: topic
        completions:
          - $(playground get-topic-list)
        help: |-
          🗳 Topic name
      - long: --skip-delete-schema
        required: false
        help: |-
          🔰 Do not delete subject/schema

    - name: alter
      group: Topic
      help: |-
        🪛 Alter topic config
      flags:
      - *verbose
      - long: --topic
        short: -t
        required: true
        arg: topic
        completions:
          - $(playground get-topic-list)
        help: |-
          🗳 Topic name
      catch_all:
        label: arguments
        help: |-
          Any arguments to be used with kafka-configs --alter. If the topic does not exist, it is created first.
        required: false
      examples: |
        playground topic alter --topic atopic --add-config max.message.bytes=5242940

- name: connector-plugin
  expose: always
  group: Connector-Plugin
  help: |-
    🔌 Connector-plugin commands

  commands:
  - name: search-jar
    help: ☕ List jars for a connector plugin from confluent hub https://www.confluent.io/hub/
          Search for specific class and display method signatures
    dependencies:
      javap: visit https://openjdk.org/install/ to install
    flags:
    - &connector-plugin
      long: --connector-plugin
      short: -c
      required: true
      arg: connector-plugin
      completions:
        - $(playground get-connector-plugin "$cur")
      help: |-
        🔌 Connector plugin name

        🎓 Tip: use <tab> completion to trigger fzf completion
    - *connector-tag
    - long: --class
      arg: class
      required: false
      help: |- 
        ☕ Java class name to search for in all jars
    examples: |
      playground connector-plugin search-jar --connector-plugin confluentinc/kafka-connect-s3 --class WebIdentityTokenCredentialsProvider

  - name: versions
    help: 💯 List versions for a connector plugin from confluent hub https://www.confluent.io/hub/
    flags:
    - *connector-plugin
    - &force_refresh
      long: --force-refresh
      required: false
      help: |-
        ☢️ Force refresh.
    - long: --last
      arg: last
      required: false
      validate: integer
      conflicts: [--all]
      help: |-
        🆕 Number of last versions to show
    examples: |
      playground connector-plugin versions --connector-plugin confluentinc/kafka-connect-s3

  - name: display-last-updated
    help: 🆕 List last updated connector plugins from confluent hub https://www.confluent.io/hub/
    flags:
    - long: --days
      arg: days
      required: false
      default: "3"
      validate: integer
      help: |-
        📅 Number of days to look back

        Default is 3 days
    - long: --vendor
      required: false
      validate: not_empty
      arg: vendor
      help: |-
        🏢 Only display results for this vendor
    examples: |
      playground connector-plugin display-last-updated --days 7
      playground connector-plugin display-last-updated --days 7 --vendor confluentinc
  - name: sourcecode
    help: |
      🧑‍💻 work with source code

    commands:
    - name: open
      help: |
        🏹 open on github
      flags:
      - *connector-plugin
      - *connector-tag
      
- name: connector
  expose: always
  group: Connector
  filters:
  - not_mdc_environment
  help: |-
    🔗 Connector commands

  commands:
  - name: status
    help: 🧩 Show status of all connectors
    flags:
    - *verbose
    - &connector
      long: --connector
      short: -c
      required: false
      arg: connector
      completions:
        - $(playground get-connector-list)
      help: |-
        🔗 Connector name

        🎓 Tip: If not specified, the command will apply to all connectors

  - name: oracle-cdc-xstream
    help: |
      🅾️ Specific Oracle CDC Xstream commands
    filters:
    - oracle_running
    commands:
    - name: generate-report
      help: |
        ⚙️ Generate and open oracle cdc xstream connector diagnostics, see https://docs.confluent.io/kafka-connectors/oracle-xstream-cdc-source/current/troubleshooting.html#connector-diagnostics-script
    - name: debug
      help: |
        🐞 Execute various SQL commands to debug xstream components

  - name: offsets
    help: |
      💈 Handle source and sink connectors offsets

        Note: First-class offsets (KIP-875) is only available if CP > 7.6
    commands:
    - name: get
      help: |
        🏹 Get current offsets for source and sink connectors

        ⚠️ Available for ccloud source connectors (see https://docs.confluent.io/cloud/current/connectors/offsets.html)
      flags:
      - *verbose
      - *connector
    - name: reset
      help: |
        🆕 Reset offsets for source and sink connectors

        ⚠️ Available for ccloud connectors (see https://docs.confluent.io/cloud/current/connectors/offsets.html)
      flags:
      - *verbose
      - *connector
    - name: alter
      help: |
        ⛏️ Alter offsets for source and sink connectors

        ⚠️ Available for ccloud connectors (see https://docs.confluent.io/cloud/current/connectors/offsets.html)
      flags:
      - *verbose
      - *connector
    - name: get-offsets-request-status
      help: |
        👁️‍🗨️ Get the status of the previous offset request

        ⚠️ Available for ccloud source connectors (see https://docs.confluent.io/cloud/current/connectors/offsets.html)
      flags:
      - *verbose
      - *connector

  - name: plugins
    help: 🎨 Show all connector plugins installed. You can also display transforms, converters and predicates using --all flag
    flags:
    - *verbose
    - long: --all
      required: false
      help: |-
        🌕 Show also transforms, converters, predicates available

  - name: pause
    help: ⏸️  Pause connector
    flags:
    - *verbose
    - *connector

  - name: versions
    help:  |
      🧞 Get current and latest versions available on Confluent Hub for connector(s) used in example

  - name: restart
    help: ♻️  Restart connector
    flags:
    - *verbose
    - *connector
    - long: --task-id
      short: -t
      arg: task_id
      validate: integer
      required: false
      help: |-
        🔧 Restart specific task ID only (instead of restarting entire connector)

  - name: stop
    help: 🛑 Stop connector (only available if CP > 7.5)
    flags:
    - *verbose
    - *connector

  - name: resume
    alias: unpause
    help: ⏯️  Resume connector
    flags:
    - *verbose
    - *connector

  - name: delete
    help: 🗑️  Delete connector
    flags:
    - *verbose
    - *connector

  - name: show-lag
    help: |
      🐢 Show lag of sink connector

      It will run until all lag becomes 0 (press ctrl-c to exit)
    flags:
    - *verbose
    - *connector
    - long: --interval
      arg: interval
      validate: integer
      required: false
      default: "20"
      help: |-
        Interval between lag checks (default is 20 seconds).
    - long: --max-wait
      arg: max_wait
      validate: integer
      default: "0"
      help: |- 
        ⏳ Max time in seconds to wait for lag to become 0. If set to 0 (default), it will run until all lag becomes 0.

  - name: show-config
    help:  |
      🧰 Show current connector config that was applied
      
      use --force-rest-endpoint to get results with REST API /config endpoint (https://docs.confluent.io/platform/current/connect/references/restapi.html#get--connectors-(string-name)-config)
    flags:
    - *verbose
    - *connector
    - &noclipboard
      long: --no-clipboard
      required: false
      private: true
      help: |-
        do not copy to clipboard (internal)
    - long: --force-rest-endpoint
      required: false
      help: |-
        ☢️ Force using REST API /config endpoint (https://docs.confluent.io/platform/current/connect/references/restapi.html#get--connectors-(string-name)-config)

  - name: show-config-parameters
    help: 🔩 Show all possible configuration parameters of connector
    flags:
    - *verbose
    - *connector
    - long: --open
      short: -o
      help: |- 
        🔖 Save output to a file and open with text editor set with playground config editor <editor> (default is code)
    - *force_refresh
    -  &only_show_file_path
      long: --only-show-file-path
      required: false
      private: true
      help: |-
        📂 Only show the path of the file containing the configuration parameters
    - &only_show_json
      long: --only-show-json
      required: false
      help: |-
        📗 Only show list of all available parameters for connector (with default value when applicable)
    -  &only_show_json_file_path
      long: --only-show-json-file-path
      required: false
      private: true
      help: |-
        📂 Only show the path of the json file containing the configuration parameters

  - name: select-config
    help: |-
      🗜️ Easily select config from all possible configuration parameters of connector

      🎓 Tip: use <tab> to select multiple config at once !
    dependencies:
      fzf: visit https://github.com/junegunn/fzf#installation to install
    flags:
    - *connector

  - name: snippets
    help: 🔌 useful snippets
    flags:
    - long: --converter
      required: false
      arg: converter
      allowed: 
        - avro
        - protobuf
        - json-schema
        - json
        - json-schema-enabled
        - string
        - bytearray
      help: |-
        🔌 Converter
    - long: --dlq
      required: false
      help: |- 
        💀 dlq
    examples: |
      playground connector snippets --converter avro --dlq

  - name: open-docs
    help: |-
      🧑‍🎓 Open connector documentation of currently running conector(s)
    flags:
    - long: --only-show-url
      help: |-
        🌐 Only show url

  - name: log-level
    help: |-
      🧬 Set connect log level

      🎓 Tip: it will also set io.confluent.kafka.schemaregistry.client.rest.RestService (to see schema registry rest requests) and org.apache.kafka.connect.runtime.TransformationChain (to see records before and after SMTs)
              it will also set org.apache.kafka.connect.runtime.WorkerSinkTask for sink and org.apache.kafka.connect.runtime.WorkerSourceTask for source connectors.
    flags:
    - *connector
    - *level

  - name: logs
    group: Connect
    help: |-
      🕵️  Tail and follow connect logs

      For onprem connectors, this is basically a shortcut for "playground container logs --container connect", --connector flag is not relevant

      For Fully Managed connectors, limitations apply (see https://docs.confluent.io/cloud/current/connectors/logging-cloud-connectors.html#using-ccloud-cli)
    flags:
    - *connector
    - *logsopen
    - *logswaitforlog
    - *verbose

  - name: open-ccloud-connector-in-browser
    group: Connect
    help: |-
      🤖 Open Fully Managed connector in browser (Confluent Cloud dashboard)
    flags:
    - *connector
    - long: --browser
      required: false
      arg: browser
      help: |-
        🌎 browser name

        Default browser from your system if not set

  - name: connect-migration-utility
    help: |
      🧩 Run Kafka Connector Migration Utility (see https://github.com/confluentinc/connect-migration-utility/) on running connect cluster

      The connector example should be ran with --environment flag set to <ccloud>

      See a full example at https://github.com/vdesabou/kafka-docker-playground/blob/master/ccloud/connect-migration-utility/README.md
    filters:
    - ccloud_environment
    dependencies:
      confluent: visit https://docs.confluent.io/confluent-cli/current/overview.html to install
    commands:
    - name: discovery
      help: |
        👨‍🔬 Discover connectors in the local connect cluster and export their configurations to files
          
          see https://github.com/confluentinc/connect-migration-utility/tree/master?tab=readme-ov-file#step-1-specify-the-connector-configuration
      flags:
      - *verbose

    - name: migrate
      help: |
        🪄 Migrate discovered connectors to fully managed connectors

          see https://github.com/confluentinc/connect-migration-utility/tree/master?tab=readme-ov-file#step-2-migrate-the-connector
          ⚠️ <discovery> command should be run first

      flags:
      - long: --migration-mode
        arg: migration-mode
        allowed: [stop_create_latest_offset, create, create_latest_offset]
        required: false
        default: create
        help: |-
          - create: Create connector without any offset consideration
          - stop_create_latest_offset: Create a connector with no data loss/duplication (the python script stops the self-managed connector and fetches the latest offset and creates a fully-managed connector on Confluent Cloud using the fetched offset). See https://github.com/confluentinc/connect-migration-utility/tree/master?tab=readme-ov-file#create-a-connector-with-no-data-lossduplication.
          - create_latest_offset: Create a connector with no downtime (the python script fetches the latest offset without stopping the connector and creates a fully-managed connector on Confluent Cloud using the fetched offset. This option may cause data duplication as the self-managed connector is still running). See https://github.com/confluentinc/connect-migration-utility/tree/master?tab=readme-ov-file#create-a-connector-with-no-downtime.

  - name: create-or-update
    help: 🧑‍🎨  Create or update connector
    args:
    - name: json
      
      # Set the default path value to -, which is the standard operator for stdin.
      default: "-"
      help: json (reads from stdin if empty)

    flags:
    - *verbose
    - *noclipboard
    - long: --connector
      short: -c
      required: true
      arg: connector
      completions:
        - $(playground get-connector-list)
      help: |-
        🔗 Connector name
    - long: --level
      short: -l
      arg: level
      allowed: [INFO, WARN, DEBUG, TRACE]
      required: false
      help: |-
        ❕Log level

        ⚠️ Not available for ccloud connectors
    - *package
    - long: --wait-for-zero-lag
      help: |- 
        😴 Wait until lag becomes 0
    - long: --validate
      required: false
      help: |-
        ✅ Validate config using PUT /connector-plugins/(string:name)/config/validate (https://docs.confluent.io/platform/current/connect/references/restapi.html#put--connector-plugins-(string-name)-config-validate)
    - long: --skip-automatic-connector-config
      required: false
      help: |-
        🤖 If example is run (playground run) with --environment flag, automatic configuration to adapt to the environment is added. 
        
        This flag allows to skip this automatic configuration (only useful to reproduce issues)
    - long: --offsets
      arg: offsets
      validate: json
      required: false
      help: |-
        📍 Create connector with offsets (https://docs.confluent.io/cloud/current/connectors/offsets.html#create-connectors-with-offsets)

        ⚠️ Only available for ccloud connectors, the connector should not really exists

    - long: --initial-state
      arg: initial-state
      allowed: 
        - RUNNING
        - PAUSED
        - STOPPED
      required: false
      help: |-
        🪵 Create connector with specific status (https://cwiki.apache.org/confluence/display/KAFKA/KIP-980%3A+Allow+creating+connectors+in+a+stopped+state)

        Only available if CP > 7.7

        ⚠️ not available for ccloud connectors

    examples: |
      playground connector create-or-update -c filestream-sink << EOF
      {
          "tasks.max": "1",
          "connector.class": "org.apache.kafka.connect.file.FileStreamSinkConnector",
          "topics": "filestream",
          "file": "/tmp/output.json",
          "key.converter": "org.apache.kafka.connect.storage.StringConverter",
          "value.converter": "org.apache.kafka.connect.json.JsonConverter",
          "value.converter.schemas.enable": "false"
      }
      EOF

      playground connector create-or-update -c filestream-sink --offsets '[{"partition":{"kafka_topic":"filestream","kafka_partition":0},"offset":{"kafka_offset":8}}]' << EOF
      {
          "tasks.max": "1",
          "connector.class": "org.apache.kafka.connect.file.FileStreamSinkConnector",
          "topics": "filestream",
          "file": "/tmp/output.json",
          "key.converter": "org.apache.kafka.connect.storage.StringConverter",
          "value.converter": "org.apache.kafka.connect.json.JsonConverter",
          "value.converter.schemas.enable": "false"
      }
      EOF

  - name: update
    help: 🛠️ Update connector configuration by opening current connector config in text editor set with playground config editor <editor> (default is code). Once file is saved, the new configuration is updated.
    flags:
    - *connector
    examples: |
      playground connector update -c filestream-sink

  examples:
  - playground connector status
  - playground connector status --json
  - playground connector resume --connector <connector-name>
  - playground connector pause -c <connector-name>
  - playground connector delete -c <connector-name>

- name: ec2
  expose: always
  group: EC2
  filters:
    - aws_ec2_permissions
  help: |-
    ✨ Create and manage AWS EC2 instances (using Cloud Formation) to run kafka-docker-playground

    🪄 Open EC2 instances directly in Visual Studio code using Remote Development (over SSH)

  commands:
  - name: create
    help: |-
      👷 Create kafka-docker-playground EC2 instance using AWS Cloud Formation

      🔐 AWS EC2 pem file for the ec2 instance will be created and stored in root folder (make sure to do backup)

      🌍 Region being used will be the one set in your environment (`aws configure get region`) either by AWS_REGION environment variable or ~/.aws/config
    flags:
    - long: --instance-type
      required: false
      arg: instance-type
      default: t3.2xlarge
      allowed: 
        - c1.medium
        - c1.xlarge
        - c3.2xlarge
        - c3.4xlarge
        - c3.8xlarge
        - c3.large
        - c3.xlarge
        - c4.2xlarge
        - c4.4xlarge
        - c4.large
        - c4.xlarge
        - m1.large
        - m1.medium
        - m1.small
        - m1.xlarge
        - m2.2xlarge
        - m2.4xlarge
        - m2.xlarge
        - m3.2xlarge
        - m3.large
        - m3.medium
        - m3.xlarge
        - m4.10xlarge
        - m4.2xlarge
        - m4.4xlarge
        - m4.large
        - m4.xlarge
        - t1.micro
        - t2.large
        - t2.medium
        - t2.micro
        - t2.nano
        - t2.small
        - t3.2xlarge
      help: |-
        🧑‍💻 instance type. default is t3.2xlarge

    - long: --size
      required: false
      arg: size
      default: "1000"
      validate: integer
      help: |-
        💾 instance size in Gb. default is 1000 Gb

    - long: --suffix
      required: false
      arg: suffix
      help: |-
        📮 suffix to add to instance name pg-${username}-${suffix} 
        
        if not set, default is pg-${username}-${6-chars-random-string}

  - name: delete
    help: |- 
      ❌ Delete an EC2 instance created with Cloud Formation

      WARNING:  This will delete your cloud formation and associated EC2 instance
    flags:
      - long: --instance
        short: -i
        required: false
        arg: instance
        validate: not_empty
        completions:
          - $(playground get-ec2-cloudformation-list $cur)
        help: |-
          🌀 ec2 instance cloudformation (need to use completion to get all required details)

          🎓 Tip: If not specified, the command will apply to all ec2 instances

  - name: open
    help: |-
      👨‍💻 Open an EC2 instance using Visual Studio code

      🔐 Only your current ip address will be allowed to connect
    flags:
      - &instance
        long: --instance
        short: -i
        required: false
        arg: instance
        validate: not_empty
        completions:
          - $(playground get-ec2-instance-list $cur)
        help: |-
          🖥️ ec2 instance (need to use completion to get all required details)

          🎓 Tip: If not specified, the command will apply to all ec2 instances
      - long: --enable-sync-repro-folder
        required: false
        help: |-
          👉 Enable sync reproduction-models folder between local and ec2 instance

  - name: allow-my-ip
    private: true
    help: 🛂 Allow your current ip to connect to ec2 instance via ssh
    flags:
      - *instance
      
  - name: list
    help: 🔘 List all EC2 instance

  - name: stop
    help: 🔴 Stop an EC2 instance
    flags:
      - *instance

  - name: start
    help: 🟢 Start an EC2 instance
    flags:
      - *instance

  - name: status
    help: 🗺️ Show a status
    private: true
    flags:
      - long: --instance
        short: -i
        required: true
        arg: instance
        validate: not_empty
        help: |-
          🖥️ EC2 instance name
      - long: --all
        help: |- 
          return all details

  - name: sync-repro-folder
    dependencies:
      rsync: rsync needs to be installed
    help: |
        ↔️ Synchronize reproduction-models folder bewteen local and ec2 instance

        Note: rsync needs to be installed
    commands:
    - name: local-to-ec2
      help: 👉 Sync local reproduction-models folder to ec2 instance
      flags:
        - *instance

    - name: ec2-to-local
      help: 👈 Sync ec2 instance reproduction-models folder to local
      flags:
        - *instance
