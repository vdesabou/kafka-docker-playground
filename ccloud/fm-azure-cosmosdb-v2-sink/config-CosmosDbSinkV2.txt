==========================
Azure credentials
==========================
ðŸ”˜ provider.integration.id



	 - Type: false
	 - Default: STRING
	 - Importance: Azure provider-integration to mint Microsoft Entra ID application tokens.
	 - Required: HIGH

ðŸ”˜ authentication.method



	 - Type: false
	 - Default: STRING
	 - Importance: How Confluent Cloud authenticates with Azure.
	 - Required: HIGH

==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
Input messages
==========================
ðŸ”˜ input.data.format

Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: true

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

==========================
CSFLE
==========================
ðŸ”˜ csfle.enabled

Determines whether the connector honours CSFLE rules or not

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ sr.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: Select the service account that has appropriate permissions to schemas and encryption keys in the Schema Registry.
	 - Required: HIGH

ðŸ”˜ csfle.onFailure

Configures the behavior for decryption failures. If set to ERROR, the connector will behave as configured for error behaviour. If set to NONE, the connector will ignore the decryption failure and proceed to write the data in its encrypted form.

	 - Type: STRING
	 - Default: ERROR
	 - Importance: MEDIUM
	 - Required: false

==========================
Which topics do you want to get data from?
==========================
ðŸ”˜ topics.regex



	 - Type: false
	 - Default: STRING
	 - Importance: A regular expression that matches the names of the topics to consume from. This is useful when you want to consume from multiple topics that match a certain pattern without having to list them all individually.
	 - Required: LOW

ðŸ”˜ topics



	 - Type: true
	 - Default: LIST
	 - Importance: Identifies the topic name or a comma-separated list of topic names.
	 - Required: HIGH

ðŸ”˜ errors.deadletterqueue.topic.name

The name of the topic to be used as the dead letter queue (DLQ) for messages that result in an error when processed by this sink connector, or its transformations or converters. Defaults to 'dlq-${connector}' if not set. The DLQ topic will be created automatically if it does not exist. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.

	 - Type: STRING
	 - Default: dlq-${connector}
	 - Importance: LOW
	 - Required: false

==========================
Connect to your Azure Cosmos DB
==========================
ðŸ”˜ azure.cosmos.account.endpoint



	 - Type: true
	 - Default: STRING
	 - Importance: Cosmos endpoint URL. For example: https://connect-cosmosdb.documents.azure.com:443/.
	 - Required: HIGH

ðŸ”˜ azure.cosmos.sink.containers.topicMap



	 - Type: true
	 - Default: STRING
	 - Importance: A comma delimited list of Kafka topics mapped to Cosmos containers. For example: topic1#con1,topic2#con2.
	 - Required: HIGH

ðŸ”˜ azure.cosmos.sink.database.name



	 - Type: true
	 - Default: STRING
	 - Importance: Cosmos target database to write records into.
	 - Required: HIGH

==========================
Account details
==========================
ðŸ”˜ azure.cosmos.account.environment

The azure environment of the Cosmos DB account: `Azure`, `AzureChina`, `AzureUsGovernment`, `AzureGermany`.

	 - Type: STRING
	 - Default: AZURE
	 - Importance: MEDIUM
	 - Required: true

ðŸ”˜ azure.cosmos.mode.gateway

Flag to indicate whether to use gateway mode. By default it is false, means SDK uses direct mode. https://learn.microsoft.com/azure/cosmos-db/nosql/sdk-connection-modes

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ azure.cosmos.preferredRegionList



	 - Type: false
	 - Default: STRING
	 - Importance: Preferred regions list to be used for a multi region Cosmos DB account. This is a comma separated value (e.g., `[East US, West US]` or `East US, West US`) provided preferred regions will be used as hint. You should use a collocated kafka cluster with your Cosmos DB account and pass the kafka cluster region as preferred region. See list of azure regions - https://docs.microsoft.com/dotnet/api/microsoft.azure.documents.locationnames?view=azure-dotnet&preserve-view=true.
	 - Required: LOW

ðŸ”˜ azure.cosmos.account.key



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Cosmos DB account key (only required in case of `auth.type` as `MasterKey`).
	 - Required: MEDIUM

ðŸ”˜ azure.cosmos.auth.aad.clientId



	 - Type: false
	 - Default: STRING
	 - Importance: The clientId/ApplicationId of the service principal. Required for `ServicePrincipal` authentication.
	 - Required: MEDIUM

ðŸ”˜ azure.cosmos.auth.aad.clientSecret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The client secret/password of the service principal. Required for `ServicePrincipal` authentication.
	 - Required: MEDIUM

ðŸ”˜ azure.cosmos.account.tenantId



	 - Type: false
	 - Default: STRING
	 - Importance: The tenantId of the Cosmos DB account. Required for `ServicePrincipal` authentication.
	 - Required: MEDIUM

==========================
Consumer configuration
==========================
ðŸ”˜ max.poll.interval.ms

The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 300000 milliseconds (5 minutes).

	 - Type: LONG
	 - Default: 300000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.poll.records

The maximum number of records to consume from Kafka in a single request. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 500 records.

	 - Type: LONG
	 - Default: 500
	 - Importance: LOW
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Write configuration details
==========================
ðŸ”˜ azure.cosmos.sink.bulk.enabled

Flag to indicate whether Cosmos DB bulk mode is enabled for Sink connector. By default it is true.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ azure.cosmos.sink.bulk.maxConcurrentCosmosPartitions

Cosmos DB item write max concurrent cosmos partitions. If not specified it will be determined based on the number of the container's physical partitions - which would indicate every batch is expected to have data from all Cosmos physical partitions. If specified it indicates from at most how many Cosmos Physical Partitions each batch contains data. So this config can be used to make bulk processing more efficient when input data in each batch has been repartitioned to balance to how many Cosmos partitions each batch needs to write. This is mainly useful for very large containers (with hundreds of physical partitions).

	 - Type: INT
	 - Default: -1
	 - Importance: LOW
	 - Required: false

ðŸ”˜ azure.cosmos.sink.bulk.initialBatchSize

Cosmos DB initial bulk micro batch size - a micro batch will be flushed to the backend when the number of documents enqueued exceeds this size - or the target payload size is met. The micro batch size is getting automatically tuned based on the throttling rate. By default the initial micro batch size is 1. Reduce this when you want to avoid that the first few requests consume too many RUs.

	 - Type: INT
	 - Default: 1
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ azure.cosmos.sink.write.strategy

Cosmos DB item write strategy: `ItemOverwrite` (using upsert), `ItemAppend` (using create, ignore pre-existing items i.e., Conflicts), `ItemDelete` (deletes based on id/pk of data frame), `ItemDeleteIfNotModified` (deletes based on id/pk of data frame if etag hasn't changed since collecting id/pk), `ItemOverwriteIfNotModified` (using create if etag is empty, update/replace with etag pre-condition otherwise, if document was updated the pre-condition failure is ignored), `ItemPatch` (Partial update all documents based on the patch config)

	 - Type: STRING
	 - Default: ItemOverwrite
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ azure.cosmos.sink.write.patch.operationType.default

Default Cosmos DB patch operation type. Supported ones include none, add, set, replace, remove, increment. Choose none for no-op, for others please reference - https://docs.microsoft.com/azure/cosmos-db/partial-document-update#supported-operations for full context.

	 - Type: STRING
	 - Default: Set
	 - Importance: LOW
	 - Required: true

ðŸ”˜ azure.cosmos.sink.write.patch.property.configs



	 - Type: false
	 - Default: STRING
	 - Importance: Cosmos DB patch json property configs. It can contain multiple definitions matching the following patterns separated by comma. property(jsonProperty).op(operationType) or property(jsonProperty).path(patchInCosmosdb).op(operationType) - The difference of the second pattern is that it also allows you to define a different cosmosdb path. Note: It does not support nested json property config.
	 - Required: LOW

ðŸ”˜ azure.cosmos.sink.write.patch.filter



	 - Type: false
	 - Default: STRING
	 - Importance: Used for Conditional patch. Ref - https://docs.microsoft.com/azure/cosmos-db/partial-document-update-getting-started#java
	 - Required: LOW

ðŸ”˜ azure.cosmos.sink.maxRetryCount

Cosmos DB max retry attempts on write failures. By default, the connector will retry on transient write errors for up to 10 times.

	 - Type: INT
	 - Default: 10
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ azure.cosmos.sink.errors.tolerance.level

Error tolerance level after exhausting all retries. `None` for fail on error. `All` for log and continue.

	 - Type: STRING
	 - Default: None
	 - Importance: HIGH
	 - Required: false

==========================
ID Strategy details
==========================
ðŸ”˜ azure.cosmos.sink.id.strategy

The IdStrategy class name to use for generating a unique document id (id). `FullKeyStrategy` uses the full record key as ID. `KafkaMetadataStrategy` uses a concatenation of the kafka topic, partition, and offset as ID, with dashes as separator. i.e. `${topic}-${partition}-${offset}`. `ProvidedInKeyStrategy` and `ProvidedInValueStrategy` use the `id` field found in the key and value objects respectively as ID.

	 - Type: STRING
	 - Default: FullKeyStrategy
	 - Importance: HIGH
	 - Required: false

==========================
Throughput control details
==========================
ðŸ”˜ azure.cosmos.throughputControl.enabled

A flag to indicate whether throughput control is enabled.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ azure.cosmos.throughputControl.auth.type

There are two auth types are supported currently: `MasterKey`(PrimaryReadWriteKeys, SecondReadWriteKeys, PrimaryReadOnlyKeys, SecondReadWriteKeys), `ServicePrincipal`

	 - Type: STRING
	 - Default: MasterKey
	 - Importance: LOW
	 - Required: true

ðŸ”˜ azure.cosmos.throughputControl.account.key



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Cosmos DB throughput control account key (only required in case of `throughputControl.auth.type` as `MasterKey`)
	 - Required: LOW

ðŸ”˜ azure.cosmos.throughputControl.auth.aad.clientId



	 - Type: false
	 - Default: STRING
	 - Importance: The clientId/applicationId of the service principal. Required for `ServicePrincipal` authentication.
	 - Required: LOW

ðŸ”˜ azure.cosmos.throughputControl.auth.aad.clientSecret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The client secret/password of the service principal. Required for `ServicePrincipal` authentication.
	 - Required: LOW

ðŸ”˜ azure.cosmos.throughputControl.account.tenantId



	 - Type: false
	 - Default: STRING
	 - Importance: The tenantId of the Cosmos DB account. Required for `ServicePrincipal` authentication.
	 - Required: LOW

ðŸ”˜ azure.cosmos.throughputControl.account.environment

The azure environment of the Cosmos DB account: `Azure`, `AzureChina`, `AzureUsGovernment`, `AzureGermany`.

	 - Type: STRING
	 - Default: AZURE
	 - Importance: LOW
	 - Required: true

ðŸ”˜ azure.cosmos.throughputControl.account.endpoint



	 - Type: false
	 - Default: STRING
	 - Importance: Cosmos DB throughput control account endpoint uri.
	 - Required: LOW

ðŸ”˜ azure.cosmos.throughputControl.mode.gateway

Flag to indicate whether to use gateway mode. By default it is false, means SDK uses direct mode. https://learn.microsoft.com/azure/cosmos-db/nosql/sdk-connection-modes

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ azure.cosmos.throughputControl.preferredRegionList



	 - Type: false
	 - Default: STRING
	 - Importance: Preferred regions list to be used for a multi region Cosmos DB account. This is a comma separated value (e.g., `[East US, West US]` or `East US, West US`) provided preferred regions will be used as hint. You should use a collocated kafka cluster with your Cosmos DB account and pass the kafka cluster region as preferred region. See list of azure regions - https://docs.microsoft.com/dotnet/api/microsoft.azure.documents.locationnames?view=azure-dotnet&preserve-view=true
	 - Required: LOW

ðŸ”˜ azure.cosmos.throughputControl.group.name



	 - Type: false
	 - Default: STRING
	 - Importance: Throughput control group name. Since customer is allowed to create many groups for a container, the name should be unique.
	 - Required: MEDIUM

ðŸ”˜ azure.cosmos.throughputControl.targetThroughput



	 - Type: false
	 - Default: INT
	 - Importance: Throughput control group target throughput. The value should be larger than 0.
	 - Required: MEDIUM

ðŸ”˜ azure.cosmos.throughputControl.targetThroughputThreshold



	 - Type: false
	 - Default: DOUBLE
	 - Importance: Throughput control group target throughput threshold. The value should be between (0,1].
	 - Required: MEDIUM

ðŸ”˜ azure.cosmos.throughputControl.priorityLevel

Throughput control group priority level. The value can be None, High or Low.

	 - Type: STRING
	 - Default: None
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ azure.cosmos.throughputControl.globalControl.database.name



	 - Type: false
	 - Default: STRING
	 - Importance: Database which will be used for throughput global control.
	 - Required: MEDIUM

ðŸ”˜ azure.cosmos.throughputControl.globalControl.container.name



	 - Type: false
	 - Default: STRING
	 - Importance: Container which will be used for throughput global control.
	 - Required: MEDIUM

ðŸ”˜ azure.cosmos.throughputControl.globalControl.renewIntervalInMS

This controls how often the client is going to update the throughput usage of itself and adjust its own throughput share based on the throughput usage of other clients. Default is 5s, the allowed min value is 5s.

	 - Type: INT
	 - Default: 5000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ azure.cosmos.throughputControl.globalControl.expireIntervalInMS



	 - Type: false
	 - Default: INT
	 - Importance: This controls how quickly we will detect the client has been offline and hence allow its throughput share to be taken by other clients. Default is 11s, the allowed min value is 2 * renewIntervalInMS + 1
	 - Required: LOW

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.replace.null.with.default

Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.schemas.enable

Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: all
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

