==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ datapreview.schemas.enable

This config key only applies to data preview requests and governs whether the data preview output has record schema with it.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
How do you want to prefix table names?
==========================
ðŸ”˜ topic.prefix



	 - Type: true
	 - Default: STRING
	 - Importance: Prefix to prepend to table names to generate the name of the Apache KafkaÂ® topic to publish data to.
	 - Required: HIGH

==========================
How should we connect to your database?
==========================
ðŸ”˜ connection.host



	 - Type: true
	 - Default: STRING
	 - Importance: Depending on the service environment, certain network access limitations may exist. Make sure the connector can reach your service. Do not include jdbc:xxxx:// in the connection hostname property (e.g. database-1.abc234ec2.us-west.rds.amazonaws.com).
	 - Required: HIGH

ðŸ”˜ connection.port



	 - Type: true
	 - Default: INT
	 - Importance: JDBC connection port.
	 - Required: HIGH

ðŸ”˜ connection.user



	 - Type: true
	 - Default: STRING
	 - Importance: JDBC connection user.
	 - Required: HIGH

ðŸ”˜ connection.password



	 - Type: true
	 - Default: PASSWORD
	 - Importance: JDBC connection password.
	 - Required: HIGH

ðŸ”˜ db.name



	 - Type: true
	 - Default: STRING
	 - Importance: JDBC database name.
	 - Required: HIGH

ðŸ”˜ ssl.mode

What SSL mode should we use to connect to your database. `prefer` and `require` allows for the connection to be encrypted but does not do certificate validation on the server. `verify-ca` and `verify-full` require a file containing SSL CA certificate to be provided. The server's certificate will be verified to be signed by one of these authorities.`verify-ca` will verify that the server certificate is issued by a trusted CA. `verify-full` will verify that the server certificate is issued by a trusted CA and that the server hostname matches that in the certificate. Client authentication is not performed.

	 - Type: STRING
	 - Default: prefer
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ ssl.truststorefile



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The trust store containing server CA certificate. Only required if using `verify-ca` or `verify-full` ssl mode.
	 - Required: LOW

ðŸ”˜ ssl.truststorepassword



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The trust store password containing server CA certificate. Only required if using `verify-ca` or `verify-full` ssl mode.
	 - Required: LOW

==========================
Database details
==========================
ðŸ”˜ table.whitelist



	 - Type: true
	 - Default: LIST
	 - Importance: List of tables to include in copying. Use a comma-separated list to specify multiple tables (for example: "User, Address, Email").
	 - Required: MEDIUM

ðŸ”˜ timestamp.column.name



	 - Type: false
	 - Default: LIST
	 - Importance: Comma separated list of one or more timestamp columns to detect new or modified rows using the COALESCE SQL function. Rows whose first non-null timestamp value is greater than the largest previous timestamp value seen will be discovered with each poll. At least one column should not be nullable.
	 - Required: MEDIUM

ðŸ”˜ incrementing.column.name



	 - Type: false
	 - Default: STRING
	 - Importance: The name of the strictly incrementing column to use to detect new rows. Any empty value indicates the column should be autodetected by looking for an auto-incrementing column. This column may not be nullable.
	 - Required: MEDIUM

ðŸ”˜ table.types

By default, the JDBC connector will only detect tables with type TABLE from the source Database. This config allows a command separated list of table types to extract.

	 - Type: LIST
	 - Default: TABLE
	 - Importance: MEDIUM
	 - Required: true

ðŸ”˜ schema.pattern



	 - Type: false
	 - Default: STRING
	 - Importance: Schema pattern to fetch table metadata from the database.
	 - Required: HIGH

ðŸ”˜ db.timezone

Name of the JDBC timezone used in the connector when querying with time-based criteria. Defaults to UTC.

	 - Type: STRING
	 - Default: UTC
	 - Importance: MEDIUM
	 - Required: true

ðŸ”˜ timestamp.granularity

Define the granularity of the Timestamp column. `CONNECT_LOGICAL` (default): represents timestamp values using Kafka Connect built-in representations. `MICROS_LONG`: represents timestamp values as micros since epoch. `MICROS_STRING`: represents timestamp values as micros since epoch in string. `MICROS_ISO_DATETIME_STRING`: uses iso format for timestamps in micros. `NANOS_LONG`: represents timestamp values as nanos since epoch. `NANOS_STRING`: represents timestamp values as nanos since epoch in string. `NANOS_ISO_DATETIME_STRING`: uses iso format

	 - Type: STRING
	 - Default: CONNECT_LOGICAL
	 - Importance: LOW
	 - Required: false

ðŸ”˜ numeric.mapping

Map NUMERIC values by precision and optionally scale to integral or decimal types. Use ``none`` if all NUMERIC columns are to be represented by Connect's DECIMAL logical type. Use ``best_fit`` if NUMERIC columns should be cast to Connect's INT8, INT16, INT32, INT64, or FLOAT64 based upon the column's precision and scale. Use ``best_fit_eager_double`` if, in addition to the properties of best_fit described above, it is desirable to always cast NUMERIC columns with scale to Connect FLOAT64 type, despite potential of loss in accuracy. Use ``precision_only`` to map NUMERIC columns based only on the column's precision assuming that column's scale is 0. The ``none`` option is the default, but may lead to serialization issues with Avro since Connect's DECIMAL type is mapped to its binary representation, and ``best_fit`` will often be preferred since it maps to the most appropriate primitive type.

	 - Type: STRING
	 - Default: none
	 - Importance: LOW
	 - Required: false

==========================
Mode
==========================
ðŸ”˜ mode



	 - Type: false
	 - Default: STRING
	 - Importance: The mode for updating a table each time it is polled. BULK: perform a bulk load of the entire table each time it is polled. TIMESTAMP: use a timestamp (or timestamp-like) column to detect new and modified rows. This assumes the column is updated with each write, and that values are monotonically incrementing, but not necessarily unique. INCREMENTING: use a strictly incrementing column on each table to detect only new rows. Note that this will not detect modifications or deletions of existing rows. TIMESTAMP AND INCREMENTING: use two columns, a timestamp column that detects new and modified rows and a strictly incrementing column which provides a globally unique ID for updates so each row can be assigned a unique stream offset.
	 - Required: MEDIUM

ðŸ”˜ quote.sql.identifiers

When to quote table names, column names, and other identifiers in SQL statements. For backward compatibility, the default value is ALWAYS.

	 - Type: STRING
	 - Default: ALWAYS
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ transaction.isolation.mode

Isolation level determines how transaction integrity is visible to other users and systems. `DEFAULT`: This is the default isolation level configured at the Database Server. `READ_UNCOMMITTED`: This is the lowest isolation level. At this level, one transaction may see dirty reads (that is, not-yet-committed changes made by other transactions). `READ_COMMITTED`: This level guarantees that any data read is already committed at the moment it is read. `REPEATABLE_READ`: In addition to the guarantees of the `READ_COMMITTED` level, this option also guarantees that any data read cannot change, if the transaction reads the same data again. However, phantom reads are possible. `SERIALIZABLE`: This is the highest isolation level. In addition to everything `REPEATABLE_READ` guarantees, it also eliminates phantom reads.

	 - Type: STRING
	 - Default: DEFAULT
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ timestamp.initial



	 - Type: false
	 - Default: LONG
	 - Importance: The epoch timestamp used for initial queries that use timestamp criteria. The value -1 sets the initial timestamp to the current time. If not specified, the connector retrieves all data. Once the connector has managed to successfully record a source offset, this property has no effect even if changed to a different value later on.
	 - Required: MEDIUM

==========================
Connection details
==========================
ðŸ”˜ poll.interval.ms

Frequency in ms to poll for new data in each table.

	 - Type: INT
	 - Default: 5000
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ batch.max.rows

Maximum number of rows to include in a single batch when polling for new data. This setting can be used to limit the amount of data buffered internally in the connector.

	 - Type: INT
	 - Default: 100
	 - Importance: LOW
	 - Required: false

ðŸ”˜ timestamp.delay.interval.ms

How long to wait after a row with a certain timestamp appears before we include it in the result. You may choose to add some delay to allow transactions with an earlier timestamp to complete. The first execution will fetch all available records (starting at timestamp 0) until current time minus the delay. Every following execution will get data from the last time we fetched until current time minus the delay.

	 - Type: INT
	 - Default: 0
	 - Importance: HIGH
	 - Required: false

==========================
Output messages
==========================
ðŸ”˜ output.data.format

Sets the output Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: true

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.replace.null.with.default

Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.schemas.enable

Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: none
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

