==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ datapreview.schemas.enable

This config key only applies to data preview requests and governs whether the data preview output has record schema with it.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
How should we connect to your database?
==========================
ðŸ”˜ database.hostname



	 - Type: true
	 - Default: STRING
	 - Importance: The address of the PostgreSQL server.
	 - Required: HIGH

ðŸ”˜ database.port



	 - Type: true
	 - Default: INT
	 - Importance: Port number of the PostgreSQL server.
	 - Required: HIGH

ðŸ”˜ database.user



	 - Type: true
	 - Default: STRING
	 - Importance: The name of the PostgreSQL user that has the required authorization.
	 - Required: HIGH

ðŸ”˜ database.password



	 - Type: true
	 - Default: PASSWORD
	 - Importance: The password for the PostgreSQL user that has the required authorization.
	 - Required: HIGH

ðŸ”˜ database.dbname



	 - Type: true
	 - Default: STRING
	 - Importance: The name of the PostgreSQL database to connect to.
	 - Required: HIGH

ðŸ”˜ database.server.name



	 - Type: true
	 - Default: STRING
	 - Importance: The logical name of the PostgreSQL server/cluster. This logical name forms a namespace and is used in all the names of the Kafka topics and the Kafka Connect schema names. The logical name is also used for the namespaces of the corresponding Avro schema, if Avro data format is used. Kafka topics should/will be created with the prefix ``database.server.name``. Only alphanumeric characters, underscores, hyphens and dots are allowed.
	 - Required: HIGH

ðŸ”˜ database.sslmode

What SSL mode should we use to connect to your database. `require` establishes an encrypted connection or fails if one cannot be made for any reason. If your database server enforces SSL, please use `require`. `disable` uses an unencrypted connection.

	 - Type: STRING
	 - Default: disable
	 - Importance: LOW
	 - Required: true

==========================
Database details
==========================
ðŸ”˜ publication.name

The name of the PostgreSQL publication created for streaming changes when using pgoutput

	 - Type: STRING
	 - Default: dbz_publication
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ signal.data.collection



	 - Type: false
	 - Default: STRING
	 - Importance: Fully-qualified name of the data collection that needs to be used to send signals to the connector. Use ``schemaName.tableName`` format to specify the fully-qualified collection name. Note that you need to manually add the signal table to the publication if ``publication.autocreate.mode`` is set to ``filtered`` or ``disabled``.
	 - Required: MEDIUM

ðŸ”˜ publication.autocreate.mode

Determines how creation of a publication should work when using pgoutput. Possible options are: `all_tables`, `disabled`, and `filtered`. Check the documentation for more details on each option.

	 - Type: STRING
	 - Default: all_tables
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ table.include.list



	 - Type: false
	 - Default: LIST
	 - Importance: An optional comma-separated list of strings that match fully-qualified table identifiers for tables to be monitored. Any table not included in this config property is excluded from monitoring. Each identifier is in the form ``schemaName.tableName``. By default the connector monitors every non-system table in each monitored schema. May not be used with "Table excluded".
	 - Required: MEDIUM

ðŸ”˜ table.exclude.list



	 - Type: false
	 - Default: LIST
	 - Importance: An optional comma-separated list of strings that match fully-qualified table identifiers for tables to be excluded from monitoring. Any table not included in this config property is monitored. Each identifier is in the form ``schemaName.tableName``. May not be used with "Table included".
	 - Required: MEDIUM

ðŸ”˜ snapshot.mode

Specifies the criteria for running a snapshot upon startup of the connector. The default setting is ``initial``, and specifies the connector can run a snapshot only when no offsets have been recorded for the logical server name. The ``never`` option specifies that the connect should never use snapshots and that, upon first startup with a logical server name, the connector should read from where it last left off (last LSN position) *or* start from the beginning, based on the point of the view of the logical replication slot. The ``exported`` option specifies that the database snapshot will be based on the point in time when the replication slot was created and is an excellent way to perform the snapshot in a lock-free way.

	 - Type: STRING
	 - Default: initial
	 - Importance: LOW
	 - Required: false

ðŸ”˜ tombstones.on.delete

Controls whether a tombstone event should be generated after a delete event. When set to ``true``, the delete operations are represented by a delete event and a subsequent tombstone event. When set to ``false``, only a delete event is sent. Emitting the tombstone event (the default behavior) allows Kafka to completely delete all events pertaining to the given key, once the source record got deleted.

	 - Type: STRING
	 - Default: true
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ datatype.propagate.source.type



	 - Type: false
	 - Default: LIST
	 - Importance: A comma-separated list of regular expressions matching the database-specific data type names that adds the data type's original type and original length as parameters to the corresponding field schemas in the emitted change records.
	 - Required: LOW

ðŸ”˜ column.exclude.list



	 - Type: false
	 - Default: LIST
	 - Importance: Regular expressions matching columns to exclude from change events
	 - Required: MEDIUM

ðŸ”˜ plugin.name

The name of the Postgres logical decoding plugin installed on the server. Note: `pgoutput` is only available in PostgreSQL 10+.

	 - Type: STRING
	 - Default: pgoutput
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ slot.name

The name of the PostgreSQL logical decoding slot that was created for streaming changes from a particular plug-in and for a particular database/schema. The server uses this slot to stream events to the connector.

	 - Type: STRING
	 - Default: debezium
	 - Importance: LOW
	 - Required: false

==========================
Connection details
==========================
ðŸ”˜ poll.interval.ms

Positive integer value that specifies the number of milliseconds the connector should wait during each iteration for new change events to appear. Defaults to 1000 milliseconds, or 1 second.

	 - Type: INT
	 - Default: 1000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.batch.size

Positive integer value that specifies the maximum size of each batch of events that should be processed during each iteration of this connector.

	 - Type: INT
	 - Default: 1000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ event.processing.failure.handling.mode

Specifies how the connector should react to exceptions during processing of binlog events.

	 - Type: STRING
	 - Default: fail
	 - Importance: LOW
	 - Required: false

ðŸ”˜ heartbeat.interval.ms

Controls how frequently the connector sends heartbeat messages to a Kafka topic. The behavior of default value 0 is that the connector does not send heartbeat messages.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ heartbeat.action.query



	 - Type: false
	 - Default: STRING
	 - Importance: Specifies a query that the connector executes on the source database when the connector sends a heartbeat message.
	 - Required: LOW

==========================
Connector details
==========================
ðŸ”˜ provide.transaction.metadata

Stores transaction metadata information in a dedicated topic and enables the transaction metadata extraction together with event counting.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ decimal.handling.mode

Specifies how DECIMAL and NUMERIC columns should be represented in change events, including: 'precise' (the default) uses java.math.BigDecimal to represent values, which are encoded in the change events using a binary representation and Kafka Connect's 'org.apache.kafka.connect.data.Decimal' type; 'string' uses string to represent values; 'double' represents values using Java's 'double', which may not offer the precision but will be far easier to use in consumers.

	 - Type: STRING
	 - Default: precise
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ binary.handling.mode

Specifies how binary (blob, binary, etc.) columns should be represented in change events, including: 'bytes' (the default) represents binary data as byte array; 'base64' represents binary data as base64-encoded string; 'hex' represents binary data as hex-encoded (base16) string.

	 - Type: STRING
	 - Default: bytes
	 - Importance: LOW
	 - Required: false

ðŸ”˜ time.precision.mode

Time, date, and timestamps can be represented with different kinds of precisions, including: 'adaptive' (the default) bases the precision of time, date, and timestamp values on the database column's precision; 'adaptive_time_microseconds' like 'adaptive' mode, but TIME fields always use microseconds precision; 'connect' always represents time, date, and timestamp values using Kafka Connect's built-in representations for Time, Date, and Timestamp, which uses millisecond precision regardless of the database columns' precision.

	 - Type: STRING
	 - Default: adaptive
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ cleanup.policy

Set the topic cleanup policy

	 - Type: STRING
	 - Default: delete
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ hstore.handling.mode

Specifies how HSTORE columns should be represented in change events.

	 - Type: STRING
	 - Default: json
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ interval.handling.mode

Specifies how INTERVAL columns should be represented in change events.

	 - Type: STRING
	 - Default: numeric
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ schema.refresh.mode

Specify the conditions that trigger a refresh of the in-memory schema for a table. `columns_diff` (the default) is the safest mode, ensuring the in-memory schema stays in-sync with the database table's schema at all times. `columns_diff_exclude_unchanged_toast` instructs the connector to refresh the in-memory schema cache if there is a discrepancy between it and the schema derived from the incoming message, unless unchanged TOASTable data fully accounts for the discrepancy.

	 - Type: STRING
	 - Default: columns_diff
	 - Importance: MEDIUM
	 - Required: false

==========================
Output messages
==========================
ðŸ”˜ output.data.format

Sets the output Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ output.key.format

Sets the output Kafka record key format. Valid entries are AVRO, JSON_SR, PROTOBUF, STRING or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ after.state.only

Controls whether the generated Kafka record should contain only the state after applying change events.

	 - Type: STRING
	 - Default: true
	 - Importance: LOW
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.replace.null.with.default

Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.schemas.enable

Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: none
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Egress allowlist
==========================
ðŸ”˜ connector.egress.whitelist



	 - Type: false
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

