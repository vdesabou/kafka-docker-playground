==========================
Which topics do you want to get data from?
==========================
ðŸ”˜ topics.regex



	 - Type: false
	 - Default: STRING
	 - Importance: A regular expression that matches the names of the topics to consume from. This is useful when you want to consume from multiple topics that match a certain pattern without having to list them all individually.
	 - Required: LOW

ðŸ”˜ topics



	 - Type: true
	 - Default: LIST
	 - Importance: Identifies the topic name or a comma-separated list of topic names.
	 - Required: HIGH

ðŸ”˜ errors.deadletterqueue.topic.name

The name of the topic to be used as the dead letter queue (DLQ) for messages that result in an error when processed by this sink connector, or its transformations or converters. Defaults to 'dlq-${connector}' if not set. The DLQ topic will be created automatically if it does not exist. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.

	 - Type: STRING
	 - Default: dlq-${connector}
	 - Importance: LOW
	 - Required: false

ðŸ”˜ reporter.result.topic.name

The name of the topic to produce records to after successfully processing a sink record. Defaults to 'success-${connector}' if not set. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.

	 - Type: STRING
	 - Default: success-${connector}
	 - Importance: LOW
	 - Required: false

ðŸ”˜ reporter.error.topic.name

The name of the topic to produce records to after each unsuccessful record sink attempt. Defaults to 'error-${connector}' if not set. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.

	 - Type: STRING
	 - Default: error-${connector}
	 - Importance: LOW
	 - Required: false

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
Input messages
==========================
ðŸ”˜ input.data.format

Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: true

==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode, whenever possible.

	 - Type: STRING
	 - Default: ${connect.regional.connector}
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

==========================
CSFLE
==========================
ðŸ”˜ csfle.enabled

Determines whether the connector honours CSFLE rules or not

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ sr.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: Select the service account that has appropriate permissions to schemas and encryption keys in the Schema Registry.
	 - Required: HIGH

ðŸ”˜ csfle.onFailure

Configures the behavior for decryption failures. If set to ERROR, the connector will behave as configured for error behaviour. If set to NONE, the connector will ignore the decryption failure and proceed to write the data in its encrypted form.

	 - Type: STRING
	 - Default: ERROR
	 - Importance: MEDIUM
	 - Required: false

==========================
Consumer configuration
==========================
ðŸ”˜ max.poll.interval.ms

The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 300000 milliseconds (5 minutes).

	 - Type: LONG
	 - Default: 300000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.poll.records

The maximum number of records to consume from Kafka in a single request. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 500 records.

	 - Type: LONG
	 - Default: 500
	 - Importance: LOW
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
How should we connect to your Elasticsearch Service?
==========================
ðŸ”˜ connection.url



	 - Type: true
	 - Default: STRING
	 - Importance: Elasticsearch Service connection URI (e.g. https://123123.us-east-1.aws.found.io:9243).
	 - Required: HIGH

ðŸ”˜ elastic.server.version

The version of your Elasticsearch server. Select ``V8`` for Elasticsearch 8.x clusters or ``V9`` for Elasticsearch 9.x clusters. Default is ``V8``.

	 - Type: STRING
	 - Default: V8
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ auth.type

The authentication method for connecting to Elasticsearch. Set to ``BASIC`` to use username/password authentication, ``API_KEY`` to use Elasticsearch API key-based authentication, or ``NONE`` for no authentication. Default is ``BASIC``.

	 - Type: STRING
	 - Default: BASIC
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ connection.username



	 - Type: false
	 - Default: STRING
	 - Importance: The username used to authenticate with Elasticsearch Service.
	 - Required: MEDIUM

ðŸ”˜ connection.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The password used to authenticate with Elasticsearch Service.
	 - Required: MEDIUM

ðŸ”˜ api.key.value



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The Elasticsearch API key value used for authentication. Only required when ``auth.type`` is set to ``API_KEY``. You can generate API keys from the Elasticsearch Management interface or using the Elasticsearch API.
	 - Required: MEDIUM

==========================
Security
==========================
ðŸ”˜ elastic.ssl.enabled

Set to ``true`` to enable PKI authentication with SSL support using client certificates. When set to ``false``, all SSL configuration properties below are ignored. Note that the connector will still use SSL/TLS encryption if your connection URL uses ``https``, but without client certificate authentication.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ elastic.https.ssl.keystore.file



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The key store file. This is optional for client and can be used for two-way authentication for client.
	 - Required: MEDIUM

ðŸ”˜ elastic.https.ssl.key.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The password of the private key in the key store file. This is required for clients only if two-way authentication is configured.
	 - Required: MEDIUM

ðŸ”˜ elastic.https.ssl.keystore.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The store password for the key store file. This is optional for client and only needed if ``ssl.keystore.location`` is configured. Key store password is not supported for PEM format.
	 - Required: MEDIUM

ðŸ”˜ elastic.https.ssl.keystore.type

The file format of the key store file. This is optional for client.

	 - Type: STRING
	 - Default: JKS
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ elastic.https.ssl.truststore.file



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The Truststore file with the certificates of the trusted CAs.
	 - Required: MEDIUM

ðŸ”˜ elastic.https.ssl.truststore.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The password for the trust store file. If a password is not set, trust store file configured will still be used, but integrity checking is disabled. Trust store password is not supported for ``.PEM`` format.
	 - Required: MEDIUM

ðŸ”˜ elastic.https.ssl.truststore.type

The file format of the trust store file.

	 - Type: STRING
	 - Default: JKS
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ elastic.https.ssl.keymanager.algorithm

The algorithm used by key manager factory for SSL connections.

	 - Type: STRING
	 - Default: SunX509
	 - Importance: LOW
	 - Required: false

ðŸ”˜ elastic.https.ssl.trustmanager.algorithm

The algorithm used by trust manager factory for SSL connections.

	 - Type: STRING
	 - Default: PKIX
	 - Importance: LOW
	 - Required: false

ðŸ”˜ elastic.https.ssl.endpoint.identification.algorithm

The endpoint identification algorithm to validate server hostname using server certificate.

	 - Type: STRING
	 - Default: https
	 - Importance: LOW
	 - Required: false

==========================
Destination Configuration
==========================
ðŸ”˜ auto.create

Whether to automatically create Elasticsearch indices or data streams based on topic names. When set to ``true``, the connector automatically creates the necessary Elasticsearch resources (indices or data streams) before writing data. When set to ``false``, the resources must already exist in Elasticsearch or be specified via ``topic.to.resource.mapping``.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ resource.type

The type of Elasticsearch resource to write to. Set to ``INDEX`` for regular indices, ``DATASTREAM`` for data streams, ``ALIAS_INDEX`` for index aliases, or ``ALIAS_DATASTREAM`` for data stream aliases.

	 - Type: STRING
	 - Default: INDEX
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ topic.to.resource.mapping



	 - Type: false
	 - Default: LIST
	 - Importance: A list of topic-to-resource mappings in the format ``topic:resource`` (e.g., ``orders:elasticsearch-orders,users:elasticsearch-users``). When specified, the connector uses the provided resource name instead of the topic name for writing to Elasticsearch. These resources must already exist in Elasticsearch. The type of resource is determined by the ``resource.type`` configuration.
	 - Required: HIGH

==========================
Data Streams
==========================
ðŸ”˜ data.stream.type

Describes the generic type of data to be written to a data stream. Valid options are ``LOGS`` for log data or ``METRICS`` for metric/time-series data. This value is used alongside ``data.stream.dataset`` and ``data.stream.namespace`` to construct the data stream name in the format ``{data.stream.type}-{data.stream.dataset}-{data.stream.namespace}``. Custom index templates defined in the destination cluster are also supported.

	 - Type: STRING
	 - Default: LOGS
	 - Importance: LOW
	 - Required: false

ðŸ”˜ data.stream.dataset



	 - Type: false
	 - Default: STRING
	 - Importance: Describes the data ingested and its structure to be written to a data stream. This can be any arbitrary string, provided it is no longer than 100 characters, in all lowercase, and does not contain spaces or special characters ``/\*\"<>|,#:-``. When set, this value is used alongside ``data.stream.type`` and ``data.stream.namespace`` to construct the data stream name in the format ``{data.stream.type}-{data.stream.dataset}-{data.stream.namespace}``.
	 - Required: LOW

ðŸ”˜ data.stream.namespace

Describes a user-configurable arbitrary grouping for organizing data within a data stream. It can be any string up to 100 characters, in lowercase, without spaces or special characters ``/\*\"<>|,#:-``. When set, it is used with ``data.stream.type`` and ``data.stream.dataset`` to form the data stream name in the format ``{data.stream.type}-{data.stream.dataset}-{data.stream.namespace}``. Defaults to ``${topic}``, which resolves to the Kafka topic name.

	 - Type: STRING
	 - Default: ${topic}
	 - Importance: LOW
	 - Required: false

ðŸ”˜ data.stream.timestamp.field



	 - Type: false
	 - Default: LIST
	 - Importance: All documents sent to a data stream need an ``@timestamp`` field with values of type ``date`` or ``date_nanos``. List the field names to use for the ``@timestamp`` mapping. If multiple fields are provided, the first field listed that also appears in the record is used. If this configuration is left empty, all documents will use the Kafka record timestamp as the ``@timestamp`` field value. Note that ``@timestamp`` still needs to be explicitly listed if records already contain this field.
	 - Required: LOW

==========================
Data Conversion
==========================
ðŸ”˜ write.method

Method used for writing data to Elasticsearch. The default is ``INSERT``, in which the connector constructs a document from the record value and inserts that document into Elasticsearch, completely replacing any existing document with the same ID. The ``UPSERT`` method creates a new document if one with the specified ID does not yet exist, or updates an existing document with the same ID by adding or replacing only those fields present in the record value. The ``UPSERT`` method may require additional time and resources of Elasticsearch, so consider increasing ``read.timeout.ms`` and decreasing ``batch.size`` if you experience timeouts.

	 - Type: STRING
	 - Default: INSERT
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ key.ignore

Whether to ignore the record key for the purpose of forming the Elasticsearch document ID. When this is set to ``true``, document IDs are generated as topic+partition+offset taken from the record. When  set to ``false``, the record key is used as the Elasticsearch document ID.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ topic.key.ignore



	 - Type: false
	 - Default: LIST
	 - Importance: List of topics for which ``key.ignore`` should be ``true``.
	 - Required: LOW

ðŸ”˜ use.autogenerated.ids

Specifies whether to use auto-generated Elasticsearch document IDs for insertion requests. Note that this setting removes exactly once guarantees, and message delivery will be at least once. This only applies if the write method is set to ``INSERT``. When set to ``true``, the ``Ignore Key mode`` option is also ignored when sending data to Elasticsearch.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ schema.ignore

Whether to ignore schemas during indexing. When set to ``true``, the record schema is ignored for the purpose of registering an Elasticsearch mapping. Elasticsearch infers the mapping from the data (dynamic mapping needs to be enabled by the user).

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ topic.schema.ignore



	 - Type: false
	 - Default: LIST
	 - Importance: List of topics for which ``schema.ignore`` should be ``true``.
	 - Required: LOW

ðŸ”˜ compact.map.entries

Defines how map entries with string keys within record values should be written to JSON. When set to ``true``, these entries are written compactly as ``entryKey: entryValue``. When set to ``false``, map entries with string keys are written as a nested document ``{key: entryKey, value: entryValue}``.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ external.version.header



	 - Type: false
	 - Default: STRING
	 - Importance: Header name to pull value for external versioning, defaults to using the kafka record offset. Must have a numeric value.
	 - Required: LOW

==========================
Connection Details
==========================
ðŸ”˜ batch.size

The number of records to process as a batch when writing to Elasticsearch using the bulk API.

	 - Type: INT
	 - Default: 50
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ read.timeout.ms

How long to wait in milliseconds for the Elasticsearch server to send a response. The connector task fails if any read operation times out.

	 - Type: INT
	 - Default: 15000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ connection.compression

Whether to use GZip compression on the HTTP connection to Elasticsearch. To enable this, the ``http.compression`` setting must also be enabled on your Elasticsearch cluster.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.replace.null.with.default

Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.value.schema.id.deserializer

The class name of the schema ID deserializer for values. This is used to deserialize schema IDs from the message headers.

	 - Type: STRING
	 - Default: io.confluent.kafka.serializers.schema.id.DualSchemaIdDeserializer
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.use.schema.id



	 - Type: false
	 - Default: INT
	 - Importance: The schema ID to use for deserialization when using ConfigSchemaIdDeserializer. This allows you to specify a fixed schema ID to be used for deserializing message values. Only applicable when value.converter.value.schema.id.deserializer is set to ConfigSchemaIdDeserializer.
	 - Required: LOW

ðŸ”˜ value.converter.schemas.enable

Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: all
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.schema.id.deserializer

The class name of the schema ID deserializer for keys. This is used to deserialize schema IDs from the message headers.

	 - Type: STRING
	 - Default: io.confluent.kafka.serializers.schema.id.DualSchemaIdDeserializer
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.use.schema.guid



	 - Type: false
	 - Default: STRING
	 - Importance: The schema GUID to use for deserialization when using ConfigSchemaIdDeserializer. This allows you to specify a fixed schema GUID to be used for deserializing message keys. Only applicable when key.converter.key.schema.id.deserializer is set to ConfigSchemaIdDeserializer.
	 - Required: LOW

ðŸ”˜ value.converter.use.schema.guid



	 - Type: false
	 - Default: STRING
	 - Importance: The schema GUID to use for deserialization when using ConfigSchemaIdDeserializer. This allows you to specify a fixed schema GUID to be used for deserializing message values. Only applicable when value.converter.value.schema.id.deserializer is set to ConfigSchemaIdDeserializer.
	 - Required: LOW

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.use.schema.id



	 - Type: false
	 - Default: INT
	 - Importance: The schema ID to use for deserialization when using ConfigSchemaIdDeserializer. This allows you to specify a fixed schema ID to be used for deserializing message keys. Only applicable when key.converter.key.schema.id.deserializer is set to ConfigSchemaIdDeserializer.
	 - Required: LOW

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Egress allowlist
==========================
ðŸ”˜ connector.egress.whitelist



	 - Type: false
	 - Default: STRING
	 - Importance: List a comma-separated list of FQDNs, IPs, or CIDR ranges for secure, restricted network egress.
	 - Required: HIGH

