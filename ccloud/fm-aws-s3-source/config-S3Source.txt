==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Which topic(s) do you want to send data to?
==========================
ðŸ”˜ topic.regex.list



	 - Type: true
	 - Default: LIST
	 - Importance: A list of topics along with a regex expression of the files which are to be sent to that topic.  For example: "my-topic:.*" will send all files to "my-topic", while a list containing only the expression "special-topic:.*\.json" will send only files starting with ".json" to "special-topic", and all other files not matching any patterns will be ignored and not sourced. Files that match multiple mappings will be sent to the first topic in the list that maps the file. The ``topic.regex.list`` property matches the full path (for example, ``folder/file.txt``), not just the filename.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode, whenever possible.

	 - Type: STRING
	 - Default: ${connect.regional.connector}
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ datapreview.schemas.enable

This config key only applies to data preview requests and governs whether the data preview output has record schema with it.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
CSFLE
==========================
ðŸ”˜ csfle.enabled

Determines whether the connector honours CSFLE rules or not

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ sr.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: Select the service account that has appropriate permissions to schemas and encryption keys in the Schema Registry.
	 - Required: HIGH

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
Input and output messages
==========================
ðŸ”˜ input.data.format



	 - Type: true
	 - Default: STRING
	 - Importance: Sets the input message format. Valid entries are AVRO, JSON, or BYTES. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO.
	 - Required: HIGH

ðŸ”˜ output.data.format

Set the output message format for values. Valid entries are AVRO, JSON, JSON_SR, PROTOBUF, STRING, or BYTES. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR and PROTOBUF.

	 - Type: STRING
	 - Default: ${input.data.format}
	 - Importance: HIGH
	 - Required: true

==========================
AWS credentials
==========================
ðŸ”˜ authentication.method

Select how you want to authenticate with AWS.

	 - Type: STRING
	 - Default: Access Keys
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ aws.access.key.id



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The AWS Access Key used to connect to Amazon S3.
	 - Required: HIGH

ðŸ”˜ provider.integration.id



	 - Type: false
	 - Default: STRING
	 - Importance: Select an existing integration that has access to your resource. In case you need to integrate a new IAM role, use provider integration
	 - Required: HIGH

ðŸ”˜ aws.secret.access.key



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The AWS Secret Key used to connect to Amazon S3.
	 - Required: HIGH

==========================
How should we connect to your S3 bucket?
==========================
ðŸ”˜ s3.bucket.name



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ s3.region



	 - Type: true
	 - Default: STRING
	 - Importance: Set to the AWS region where your S3 bucket resides.
	 - Required: HIGH

ðŸ”˜ s3.part.retries

The number of times a single S3 API call should be retried in the case that it fails with a "retriable" error (such as a throttling exception). Once this limit is exceeded, the Kafka Connect poll itself may retry (based upon the Kafka Connect-based retry configuration).

	 - Type: INT
	 - Default: 3
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ s3.retry.backoff.ms

How long to wait in milliseconds before attempting the first retry of a failed S3 request. Upon a failure, this connector  may wait up to twice as long as the previous wait, up to the maximum number of retries. This avoids retrying in a tight loop under failure scenarios.

	 - Type: INT
	 - Default: 200
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ ui.s3.wan.mode

Use an S3 accelerated endpoint.

	 - Type: STRING
	 - Default: NO
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ ui.s3.path.style.access

Whether to use s3 path-style access.

	 - Type: STRING
	 - Default: NO
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ s3.http.send.expect.continue

Enable/disable use of the HTTP/1.1 handshake using EXPECT: 100-CONTINUE during multi-part upload. If true, the client waits for a 100 (CONTINUE) response before sending the request body. If false, the client uploads the entire request body without checking if the server is willing to accept the request.

	 - Type: STRING
	 - Default: YES
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ ui.s3.ssea.name

The S3 server-side encryption algorithm.

	 - Type: STRING
	 - Default: NONE
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ s3.sse.customer.key



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The S3 Server-Side Encryption customer-provided key (SSE-C).
	 - Required: MEDIUM

==========================
Storage
==========================
ðŸ”˜ topics.dir

Top-level directory (in the S3 bucket) where data to be ingested is stored.

	 - Type: STRING
	 - Default: topics
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ task.batch.size

The number of files assigned to each task at a time

	 - Type: INT
	 - Default: 10
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ file.discovery.starting.timestamp

A Unix timestamp (in epoch milliseconds since Jan 1, 1970 UTC) that denotes where to start processing files. The connector ignores any file with a creation time earlier than this timestamp. Note that the connector only uses this configuration property when no offsets are stored for a connector. This parameter allows new connectors to start from a specific timestamp instead of reading all files in a bucket.

	 - Type: LONG
	 - Default: 0
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ directory.delim

Directory delimiter pattern.

	 - Type: STRING
	 - Default: /
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ ui.behavior.on.error

Error handling behavior setting for storage connectors. Must be configured to one of the following: IGNORE, FAIL

	 - Type: STRING
	 - Default: FAIL
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ format.bytearray.separator



	 - Type: false
	 - Default: STRING
	 - Importance: String inserted between records for ByteArrayFormat. Defaults to `
` and may contain escape sequences like `
`.  An input record that contains the line separator looks like multiple records in the storage object input.
	 - Required: MEDIUM

ðŸ”˜ format.json.schema.enable

Enable reading of JSON messages with schema embedded

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ csv.separator.char

The character that separates each field in the form of an integer. Typically in a CSV file, this is a ``,`` (``44``) character. A TSV file would use a tab (``9``) character. Applicable only if ``input.data.format`` is set to ``CSV``.

	 - Type: INT
	 - Default: 44
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.first.row.as.header

Flag to indicate if the fist row of data contains the header of the file. Applicable only if ``input.data.format`` is set to ``CSV``.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ csv.null.field.indicator

Indicator to determine how the CSV Reader can determine if a field is null. For more information, see http://opencsv.sourceforge.net/apidocs/com/opencsv/enums/CSVReaderNullFieldIndicator.html. Applicable only if ``input.data.format`` is set to ``CSV``  .

	 - Type: STRING
	 - Default: NEITHER
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.schema



	 - Type: false
	 - Default: STRING
	 - Importance: The schema for the value written to Kafka. A default schema will be auto-generated if no value schema is provided. Applicable only if ``input.data.format`` is set to ``CSV``.
	 - Required: HIGH

ðŸ”˜ csv.file.charset

Character set to read file with. Applicable only if ``input.data.format`` is set to ``CSV``

	 - Type: STRING
	 - Default: UTF-8
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.skip.lines

The number of lines to skip in the beginning of the file. Applicable only if ``input.data.format`` is set to ``CSV``.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.escape.char

The character as an integer to use when a special character is encountered. The default escape character is typically a ``\`` (``92``). Applicable only if ``input.data.format`` is set to ``CSV``.

	 - Type: INT
	 - Default: 92
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.quote.char

The character that is used to quote a field. Typically in a CSV file,  this is a ``"`` (``34``) character. This happens when the ``csv.separator.char`` is within the data. Applicable only if ``input.data.format`` is set to ``CSV``.

	 - Type: INT
	 - Default: 34
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.ignore.leading.whitespace

Sets the ignore leading whitespace setting. If ``true``, the white space in front of a quote in a field is ignored. Applicable only if ``input.data.format`` is set to ``CSV``.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.ignore.quotations

Sets the ignore quotations mode. If ``true``, quotations are ignored. Applicable only if ``input.data.format`` is set to ``CSV``.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.strict.quotes

Sets the strict quotes setting. If ``true``, characters outside the quotes are ignored. Applicable only if ``input.data.format`` is set to ``CSV``.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
Data polling policy
==========================
ðŸ”˜ s3.poll.interval.ms

Frequency in milliseconds to poll for new or removed folders. This may result in updated task configurations starting to poll for data in added folders or stopping polling for data in removed folders

	 - Type: LONG
	 - Default: 60000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ record.batch.max.size

The maximum amount of records to return each time storage is polled.

	 - Type: INT
	 - Default: 200
	 - Importance: MEDIUM
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: The total number of tasks to run in parallel.
	 - Required: HIGH

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.replace.null.with.default

Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.schemas.enable

Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: none
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.schema.id.serializer

The class name of the schema ID serializer for keys. This is used to serialize schema IDs in the message headers.

	 - Type: STRING
	 - Default: io.confluent.kafka.serializers.schema.id.PrefixSchemaIdSerializer
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.value.schema.id.serializer

The class name of the schema ID serializer for values. This is used to serialize schema IDs in the message headers.

	 - Type: STRING
	 - Default: io.confluent.kafka.serializers.schema.id.PrefixSchemaIdSerializer
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Egress allowlist
==========================
ðŸ”˜ connector.egress.whitelist



	 - Type: false
	 - Default: STRING
	 - Importance: List a comma-separated list of FQDNs, IPs, or CIDR ranges for secure, restricted network egress.
	 - Required: HIGH

