==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
Input messages
==========================
ðŸ”˜ input.data.format

Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, JSON or BYTES. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ input.key.format

Sets the input Kafka record key format. Valid entries are AVRO, BYTES, JSON, JSON_SR, PROTOBUF, or STRING. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: false

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode, whenever possible.

	 - Type: STRING
	 - Default: ${connect.regional.connector}
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

==========================
Which topics do you want to get data from?
==========================
ðŸ”˜ topics.regex



	 - Type: false
	 - Default: STRING
	 - Importance: A regular expression that matches the names of the topics to consume from. This is useful when you want to consume from multiple topics that match a certain pattern without having to list them all individually.
	 - Required: LOW

ðŸ”˜ topics



	 - Type: true
	 - Default: LIST
	 - Importance: Identifies the topic name or a comma-separated list of topic names.
	 - Required: HIGH

==========================
How should we connect to your ClickHouse?
==========================
ðŸ”˜ hostname



	 - Type: true
	 - Default: STRING
	 - Importance: The hostname or IP address of the ClickHouse server
	 - Required: HIGH

ðŸ”˜ port

The ClickHouse port - default is 8443 (for HTTPS in the cloud), but for HTTP (the default for self-hosted) it should be 8123

	 - Type: INT
	 - Default: 8443
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ ssl

Enable SSL connection to ClickHouse

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ username

ClickHouse database username

	 - Type: STRING
	 - Default: default
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ password



	 - Type: true
	 - Default: PASSWORD
	 - Importance: ClickHouse database password
	 - Required: HIGH

ðŸ”˜ database

ClickHouse database name

	 - Type: STRING
	 - Default: default
	 - Importance: HIGH
	 - Required: false

==========================
Database details
==========================
ðŸ”˜ topic2TableMap



	 - Type: true
	 - Default: STRING
	 - Importance: Comma-separated list that maps topic names to table names (e.g. "topic1=table1, topic2=table2, etc...")
	 - Required: MEDIUM

ðŸ”˜ tableRefreshInterval

Time (in seconds) to refresh the table definition cache

	 - Type: INT
	 - Default: 0
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ keeperOnCluster



	 - Type: false
	 - Default: STRING
	 - Importance: Allows configuration of ON CLUSTER parameter for self-hosted instances (e.g. ON CLUSTER clusterNameInConfigFileDefinition) for exactly-once connect_state table
	 - Required: MEDIUM

ðŸ”˜ bypassRowBinary

Allows disabling use of RowBinary and RowBinaryWithDefaults for Schema-based data (Avro, Protobuf, etc.) - should only be used when data will have missing columns, and Nullable/Default are unacceptable

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dateTimeFormats



	 - Type: false
	 - Default: STRING
	 - Importance: Date time formats for parsing DateTime64 schema fields, separated by ; (e.g. someDateField=yyyy-MM-dd HH:mm:ss.SSSSSSSSS;someOtherDateField=yyyy-MM-dd HH:mm:ss)
	 - Required: MEDIUM

==========================
CSFLE
==========================
ðŸ”˜ csfle.enabled

Determines whether the connector honours CSFLE rules or not

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ sr.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: Select the service account that has appropriate permissions to schemas and encryption keys in the Schema Registry.
	 - Required: HIGH

ðŸ”˜ csfle.onFailure

Configures the behavior for decryption failures. If set to ERROR, the connector will behave as configured for error behaviour. If set to NONE, the connector will ignore the decryption failure and proceed to write the data in its encrypted form.

	 - Type: STRING
	 - Default: ERROR
	 - Importance: MEDIUM
	 - Required: false

==========================
Connection details
==========================
ðŸ”˜ jdbcConnectionProperties



	 - Type: false
	 - Default: STRING
	 - Importance: Connection properties when connecting to ClickHouse. Must start with ? and joined by & between param=value
	 - Required: MEDIUM

ðŸ”˜ exactlyOnce

Exactly Once Enabled

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ errors.tolerance

Connector Error Tolerance. Supported: none, all

	 - Type: STRING
	 - Default: none
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ errors.deadletterqueue.topic.name



	 - Type: false
	 - Default: STRING
	 - Importance: If set (with errors.tolerance=all), a DLQ will be used for failed batches (see Troubleshooting)
	 - Required: MEDIUM

ðŸ”˜ errors.deadletterqueue.context.headers.enable

Adds additional headers for the DLQ

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ clickhouseSettings



	 - Type: false
	 - Default: STRING
	 - Importance: Comma-separated list of ClickHouse settings (e.g. "insert_quorum=2, etc...")
	 - Required: MEDIUM

ðŸ”˜ tolerateStateMismatch

Allows the connector to drop records "earlier" than the current offset stored AFTER_PROCESSING (e.g. if offset 5 is sent, and offset 250 was the last recorded offset)

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

==========================
Consumer configuration
==========================
ðŸ”˜ max.poll.interval.ms

The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 300000 milliseconds (5 minutes).

	 - Type: LONG
	 - Default: 300000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.poll.records

The maximum number of records to consume from Kafka in a single request. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 500 records.

	 - Type: LONG
	 - Default: 500
	 - Importance: LOW
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.replace.null.with.default

Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.value.schema.id.deserializer

The class name of the schema ID deserializer for values. This is used to deserialize schema IDs from the message headers.

	 - Type: STRING
	 - Default: io.confluent.kafka.serializers.schema.id.DualSchemaIdDeserializer
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.use.schema.id



	 - Type: false
	 - Default: INT
	 - Importance: The schema ID to use for deserialization when using ConfigSchemaIdDeserializer. This allows you to specify a fixed schema ID to be used for deserializing message values. Only applicable when value.converter.value.schema.id.deserializer is set to ConfigSchemaIdDeserializer.
	 - Required: LOW

ðŸ”˜ value.converter.schemas.enable

Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.schema.id.deserializer

The class name of the schema ID deserializer for keys. This is used to deserialize schema IDs from the message headers.

	 - Type: STRING
	 - Default: io.confluent.kafka.serializers.schema.id.DualSchemaIdDeserializer
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.use.schema.guid



	 - Type: false
	 - Default: STRING
	 - Importance: The schema GUID to use for deserialization when using ConfigSchemaIdDeserializer. This allows you to specify a fixed schema GUID to be used for deserializing message keys. Only applicable when key.converter.key.schema.id.deserializer is set to ConfigSchemaIdDeserializer.
	 - Required: LOW

ðŸ”˜ value.converter.use.schema.guid



	 - Type: false
	 - Default: STRING
	 - Importance: The schema GUID to use for deserialization when using ConfigSchemaIdDeserializer. This allows you to specify a fixed schema GUID to be used for deserializing message values. Only applicable when value.converter.value.schema.id.deserializer is set to ConfigSchemaIdDeserializer.
	 - Required: LOW

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.use.schema.id



	 - Type: false
	 - Default: INT
	 - Importance: The schema ID to use for deserialization when using ConfigSchemaIdDeserializer. This allows you to specify a fixed schema ID to be used for deserializing message keys. Only applicable when key.converter.key.schema.id.deserializer is set to ConfigSchemaIdDeserializer.
	 - Required: LOW

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Egress allowlist
==========================
ðŸ”˜ connector.egress.whitelist



	 - Type: false
	 - Default: STRING
	 - Importance: List a comma-separated list of FQDNs, IPs, or CIDR ranges for secure, restricted network egress.
	 - Required: HIGH

