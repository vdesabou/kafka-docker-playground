==========================
Which topics do you want to get data from?
==========================
ðŸ”˜ topics.regex



	 - Type: false
	 - Default: STRING
	 - Importance: A regular expression that matches the names of the topics to consume from. This is useful when you want to consume from multiple topics that match a certain pattern without having to list them all individually.
	 - Required: LOW

ðŸ”˜ topics



	 - Type: true
	 - Default: LIST
	 - Importance: Identifies the topic name or a comma-separated list of topic names.
	 - Required: HIGH

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
Input messages
==========================
ðŸ”˜ input.data.format



	 - Type: true
	 - Default: STRING
	 - Importance: Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR and PROTOBUF. Note that you need to have Confluent Cloud Schema Registry configured
	 - Required: HIGH

==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

==========================
How should we connect to your Azure Search Service
==========================
ðŸ”˜ azure.search.service.name



	 - Type: true
	 - Default: STRING
	 - Importance: The name of the Azure Search service
	 - Required: HIGH

ðŸ”˜ azure.search.api.key



	 - Type: true
	 - Default: PASSWORD
	 - Importance: The api key for the Azure Search service
	 - Required: HIGH

ðŸ”˜ azure.search.client.id



	 - Type: true
	 - Default: PASSWORD
	 - Importance: Client ID of service principal of your subscription
	 - Required: HIGH

ðŸ”˜ azure.search.client.secret



	 - Type: true
	 - Default: PASSWORD
	 - Importance: Client Secret of service principal of your subscription
	 - Required: HIGH

ðŸ”˜ azure.search.tenant.id



	 - Type: true
	 - Default: PASSWORD
	 - Importance: Tenant ID of service principal of your subscription
	 - Required: HIGH

ðŸ”˜ azure.search.subscription.id



	 - Type: true
	 - Default: PASSWORD
	 - Importance: Azure Subscription ID for your Azure Account
	 - Required: HIGH

ðŸ”˜ azure.search.resourcegroup.name



	 - Type: true
	 - Default: STRING
	 - Importance: ResourceGroup in which Azure Search Service exists
	 - Required: HIGH

==========================
Search Service Write Details
==========================
ðŸ”˜ index.name



	 - Type: true
	 - Default: STRING
	 - Importance: The name of the index to write records as documents to. Use ``${topic}`` within the pattern to specify the topic of the record
	 - Required: HIGH

ðŸ”˜ write.method

The method used to write Kafka records to an index. Available methods are ``Upload`` - Functions like upsert. A document is inserted if it does not existed and updated/replaced if it does ``MergeOrUpload`` - Updates an existing document with the specified fields. If the document doesn't exist, behaves like ``Upload`` 

	 - Type: STRING
	 - Default: Upload
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ delete.enabled

Whether documents will be deleted if the record value is null

	 - Type: STRING
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ key.mode

Determines what will be used for the document key id. The available modes are:``KEY`` - the Kafka record key is used as the document key ``COORDINATES`` - the Kafka coordinates (topic, partition, and offset) are concatenated to form the document key. This allows for unique document keys

	 - Type: STRING
	 - Default: KEY
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ max.batch.size

The maximum number of Kafka records that will be sent per request. To disable batching of records, set this value to 1

	 - Type: INT
	 - Default: 1
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ max.retry.ms

The maximum amount of time in ms that the connector will attempt its request before aborting it

	 - Type: INT
	 - Default: 300000
	 - Importance: LOW
	 - Required: false

==========================
Consumer configuration
==========================
ðŸ”˜ max.poll.interval.ms

The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 300000 milliseconds (5 minutes).

	 - Type: LONG
	 - Default: 300000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.poll.records

The maximum number of records to consume from Kafka in a single request. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 500 records.

	 - Type: LONG
	 - Default: 500
	 - Importance: LOW
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

==========================
Egress allowlist
==========================
ðŸ”˜ connector.egress.whitelist



	 - Type: false
	 - Default: STRING
	 - Importance: List a comma-separated list of FQDNs, IPs, or CIDR ranges for secure, restricted network egress.
	 - Required: HIGH

