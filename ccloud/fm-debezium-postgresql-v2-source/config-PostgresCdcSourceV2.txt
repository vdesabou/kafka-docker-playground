==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ datapreview.schemas.enable

This config key only applies to data preview requests and governs whether the data preview output has record schema with it.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
Auth Mode
==========================
ðŸ”˜ authentication.method

Select how you want to authenticate the DB. Username Password or Google cloud service account impersonation.

	 - Type: STRING
	 - Default: Password
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ provider.integration.id



	 - Type: false
	 - Default: STRING
	 - Importance: Select an existing integration that has access to your resource. In case you need to integrate a new Google Service Account, use provider integration
	 - Required: HIGH

ðŸ”˜ database.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Password of the PostgreSQL database user that has the required authorization.
	 - Required: HIGH

==========================
How should we connect to your database?
==========================
ðŸ”˜ database.hostname



	 - Type: true
	 - Default: STRING
	 - Importance: IP address or hostname of the PostgreSQL database server.
	 - Required: HIGH

ðŸ”˜ database.port



	 - Type: true
	 - Default: INT
	 - Importance: Port number of the PostgreSQL database server.
	 - Required: HIGH

ðŸ”˜ database.user



	 - Type: true
	 - Default: STRING
	 - Importance: The name of the PostgreSQL database user that has the required authorization.
	 - Required: HIGH

ðŸ”˜ database.dbname



	 - Type: true
	 - Default: STRING
	 - Importance: The name of the PostgreSQL database from which to stream the changes.
	 - Required: HIGH

ðŸ”˜ database.sslmode

Whether to use an encrypted connection to the PostgreSQL server. Possible settings are: `disable`, `prefer`, and `require`. 

	 - Type: STRING
	 - Default: prefer
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ database.sslrootcert



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The path to the file that contains the root certificate(s) against which the server is validated.
	 - Required: HIGH

ðŸ”˜ database.sslcert



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Path to the SSL client certificate file for PostgreSQL connection. Only needed when mutual TLS authentication is enabled. Note: AWS RDS does not support client certificates.
	 - Required: MEDIUM

ðŸ”˜ database.sslkey



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Path to the SSL client private key file for PostgreSQL connection. Only needed when mutual TLS authentication is enabled (ssl_mode=verify-full or verify-ca). Must be used together with SSL Client Certificate. The private key must be in PKCS#8 format.
	 - Required: MEDIUM

ðŸ”˜ database.sslpassword



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Password for the SSL client private key file for PostgreSQL connection. Only needed when mutual TLS authentication is enabled and the private key is password-protected.
	 - Required: MEDIUM

==========================
Exactly Once Semantics
==========================
ðŸ”˜ exactly.once.enabled

When set to true, enables exactly-once support for this connector, ensuring each record reaches the destination exactly once.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ offsets.storage.topic



	 - Type: false
	 - Default: STRING
	 - Importance: The topic name used as offset storage topic to store the connector's offsets. Defaults to ``connect-offsets-${connector}``. Within the value, ``${connector}`` can be used as a placeholder for the logical cluster ID. The designated topic should not pre-exist.
	 - Required: MEDIUM

==========================
CSFLE
==========================
ðŸ”˜ csfle.enabled

Determines whether the connector honours CSFLE rules or not

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ sr.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: Select the service account that has appropriate permissions to schemas and encryption keys in the Schema Registry.
	 - Required: HIGH

==========================
Output messages
==========================
ðŸ”˜ output.data.format

Sets the output Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ output.key.format

Sets the output Kafka record key format. Valid entries are AVRO, JSON_SR, PROTOBUF, STRING or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ after.state.only

Controls whether the generated Kafka record should contain only the state of the row after the event occurred.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ tombstones.on.delete

Controls whether a tombstone event should be generated after a delete event. 

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
How should we name your topic(s)?
==========================
ðŸ”˜ topic.prefix



	 - Type: true
	 - Default: STRING
	 - Importance: Topic prefix that provides a namespace (logical server name) for the particular PostgreSQL database server or cluster in which Debezium is capturing changes. The prefix should be unique across all other connectors, since it is used as a topic name prefix for all Kafka topics that receive records from this connector. Only alphanumeric characters, hyphens, dots and underscores must be used. The connector automatically creates Kafka topics using the naming convention: `<topic.prefix>.<schemaName>.<tableName>`.
	 - Required: HIGH

==========================
Database config
==========================
ðŸ”˜ slot.name

The name of the PostgreSQL logical decoding slot that was created for streaming changes from a particular plug-in for a particular database/schema. The server uses this slot to stream events to the Debezium connector that you are configuring. Slot names must conform to PostgreSQL replication slot naming rules, which state: "Each replication slot has a name, which can contain lower-case letters, numbers, and the underscore character."

	 - Type: STRING
	 - Default: debezium
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ publication.name

The name of the PostgreSQL publication created for streaming changes when using `pgoutput`. Based on the value of ``publication.autocreate.mode`` the publication is created at start-up if it does not already exist and it includes all tables. Debezium then applies its own include/exclude list filtering, if configured, to limit the publication to change events for the specific tables of interest. The connector user must have superuser permissions to create this publication, so it is usually preferable to create the publication before starting the connector for the first time. If the publication already exists, either for all tables or configured with a subset of tables, Debezium uses the publication as it is defined.

	 - Type: STRING
	 - Default: dbz_publication
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ publication.autocreate.mode

Applies only when streaming changes by using the pgoutput plug-in. Possible settings are `all_tables`, `disabled`, and `filtered`. 

	 - Type: STRING
	 - Default: all_tables
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ publish.via.partition.root

This configuration is applicable only when the connector is responsible for creating the publication in the source database. It determines how change events from partitioned tables are captured and emitted.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ signal.data.collection



	 - Type: false
	 - Default: STRING
	 - Importance: Fully-qualified name of the data collection that needs to be used to send signals to the connector. Use ``schemaName.tableName`` format to specify the fully-qualified collection name. Note that the connector automatically adds the signal table to the publication only if the ``publication.autocreate.mode`` is set to ``filtered`` or ``all_tables``. You will need to add it manually if the mode is set to ``disabled``.
	 - Required: MEDIUM

ðŸ”˜ slot.failover

Specifies whether the connector creates a failover slot. If set to false (the default), or if the primary server runs PostgreSQL 16 or earlier, the connector does not create a failover slot.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

==========================
Connector config
==========================
ðŸ”˜ snapshot.mode

Specifies the criteria for running a snapshot upon startup of the connector. Possible settings are: `initial`, `never`(deprecated), `no_data`, `initial_only`, and `when_needed`. 

	 - Type: STRING
	 - Default: initial
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ table.include.list

To match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the table; it does not match substrings that might be present in a table name. 

	 - Type: false
	 - Default: LIST
	 - Importance: An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you want to capture. When this property is set, the connector captures changes only from the specified tables. Each identifier is of the form `schemaName.tableName`. By default, the connector captures changes in every non-system table in each schema whose changes are being captured. 
	 - Required: MEDIUM

ðŸ”˜ table.exclude.list

To match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the table; it does not match substrings that might be present in a table name. 

	 - Type: false
	 - Default: LIST
	 - Importance: An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you do not want to capture. Each identifier is of the form `schemaName.tableName`. When this property is set, the connector captures changes from every table that you do not specify. 
	 - Required: MEDIUM

ðŸ”˜ event.processing.failure.handling.mode

Specifies how the connector should react to exceptions during processing of events. Possible settings are: `fail`, `skip`, and `warn`. 

	 - Type: STRING
	 - Default: fail
	 - Importance: LOW
	 - Required: false

ðŸ”˜ column.exclude.list

To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; it does not match substrings that might be present in a column name.

	 - Type: false
	 - Default: LIST
	 - Importance: An optional, comma-separated list of regular expressions that match the fully-qualified names of columns to exclude from change event record values. Fully-qualified names for columns are of the form `schemaName.tableName.columnName`. 
	 - Required: MEDIUM

ðŸ”˜ schema.name.adjustment.mode

Specifies how schema names should be adjusted for compatibility with the message converter used by the connector. Possible settings are: `none`, `avro`, and `avro_unicode`. 

	 - Type: STRING
	 - Default: none
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ field.name.adjustment.mode

Specifies how field names should be adjusted for compatibility with the message converter used by the connector. Possible settings are: `none`, `avro`, and `avro_unicode`. 

	 - Type: STRING
	 - Default: none
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ heartbeat.interval.ms

Controls how frequently the connector sends heartbeat messages to a Kafka topic. The behavior of default value 0 is that the connector does not send heartbeat messages. Heartbeat messages are useful for monitoring whether the connector is receiving change events from the database. Heartbeat messages might help decrease the number of change events that need to be re-sent when a connector restarts. To send heartbeat messages, set this property to a positive integer, which indicates the number of milliseconds between heartbeat messages.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ heartbeat.action.query

This configuration helps address situations where capturing changes from a low-traffic database on the same host as a high-traffic one prevents Debezium from processing WAL records and acknowledging WAL positions with the database. To address this, create a heartbeat table in the low-traffic database, and set this property to a DML statement that periodically updates the table by either inserting a new row or repeatedly updating the same row. This allows the connector to receive changes from the low-traffic database and acknowledge their LSNs, preventing unbounded WAL growth on the database host. The heartbeat query executes at regular intervals, as specified by the ``heartbeat.interval.ms`` configuration property. 

	 - Type: false
	 - Default: STRING
	 - Importance: If specified, the connector executes this query on every heartbeat against the source database. The query must be a valid SQL DML statement, typically an ``INSERT`` or ``UPDATE``, that targets a dedicated heartbeat table. 
	 - Required: LOW

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ key.converter.reference.subject.name.strategy

Set the subject reference name strategy for key. Valid entries are `DefaultReferenceSubjectNameStrategy` or `QualifiedReferenceSubjectNameStrategy`. Note that the subject reference name strategy can be selected only for `PROTOBUF` format with the default strategy being `DefaultReferenceSubjectNameStrategy`.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: HIGH
	 - Required: false

==========================
How should we handle data types?
==========================
ðŸ”˜ decimal.handling.mode

Specifies how the connector should handle values for `DECIMAL` and `NUMERIC` columns. Possible settings are: `precise`, `double`, and `string`. 

	 - Type: STRING
	 - Default: precise
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ time.precision.mode

Time, date, and timestamps can be represented with different kinds of precisions: 

	 - Type: STRING
	 - Default: adaptive
	 - Importance: MEDIUM
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.replace.null.with.default

Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.schemas.enable

Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: none
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

