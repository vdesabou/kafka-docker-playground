==========================
Which topics do you want to get data from?
==========================
ðŸ”˜ topics.regex



	 - Type: false
	 - Default: STRING
	 - Importance: A regular expression that matches the names of the topics to consume from. This is useful when you want to consume from multiple topics that match a certain pattern without having to list them all individually.
	 - Required: LOW

ðŸ”˜ topics



	 - Type: true
	 - Default: LIST
	 - Importance: Identifies the topic name or a comma-separated list of topic names.
	 - Required: HIGH

ðŸ”˜ errors.deadletterqueue.topic.name

The name of the topic to be used as the dead letter queue (DLQ) for messages that result in an error when processed by this sink connector, or its transformations or converters. Defaults to 'dlq-${connector}' if not set. The DLQ topic will be created automatically if it does not exist. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.

	 - Type: STRING
	 - Default: dlq-${connector}
	 - Importance: LOW
	 - Required: false

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
Input messages
==========================
ðŸ”˜ input.data.format



	 - Type: true
	 - Default: STRING
	 - Importance: Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR, or PROTOBUF. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.
	 - Required: HIGH

ðŸ”˜ input.key.format



	 - Type: false
	 - Default: STRING
	 - Importance: Sets the input Kafka record key format. This need to be set to a proper format if using pk.mode=record_key. Valid entries are AVRO, JSON_SR, PROTOBUF, STRING. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.
	 - Required: HIGH

ðŸ”˜ delete.enabled

Whether to treat null record values as deletes. Requires pk.mode to be record_key.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

==========================
How should we connect to your database?
==========================
ðŸ”˜ connection.host



	 - Type: true
	 - Default: STRING
	 - Importance: Depending on the service environment, certain network access limitations may exist. Make sure the connector can reach your service. Do not include jdbc:xxxx:// in the connection hostname property (e.g. database-1.abc234ec2.us-west.rds.amazonaws.com).
	 - Required: HIGH

ðŸ”˜ connection.port



	 - Type: true
	 - Default: INT
	 - Importance: JDBC connection port.
	 - Required: HIGH

ðŸ”˜ connection.user



	 - Type: true
	 - Default: STRING
	 - Importance: JDBC connection user.
	 - Required: HIGH

ðŸ”˜ connection.password



	 - Type: true
	 - Default: PASSWORD
	 - Importance: JDBC connection password.
	 - Required: HIGH

ðŸ”˜ db.name



	 - Type: true
	 - Default: STRING
	 - Importance: JDBC database name.
	 - Required: HIGH

ðŸ”˜ ssl.mode

What SSL mode should we use to connect to your database. `prefer` allows for the connection to not be encrypted and `require` allows for the connection to be encrypted but does not do certificate validation on the server. `verify-ca` and `verify-full` require a file containing SSL CA certificate to be provided. The server's certificate will be verified to be signed by one of these authorities.`verify-ca` will verify that the server certificate is issued by a trusted CA. `verify-full` will verify that the server certificate is issued by a trusted CA and that the server hostname matches that in the certificate. Client authentication is not performed.

	 - Type: STRING
	 - Default: prefer
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ ssl.rootcertfile



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The server root cert file used for certificate validation. Only required if using `verify-ca` or `verify-full` ssl mode.
	 - Required: LOW

==========================
Database details
==========================
ðŸ”˜ insert.mode

The insertion mode to use. `INSERT` uses the standard INSERT row function. An error occurs if the row already exists in the table; `UPSERT` mode is similar to INSERT. However, if the row already exists, the `UPSERT` function overwrites column values with the new values provided.

	 - Type: STRING
	 - Default: INSERT
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ table.name.format

A format string for the destination table name, which may contain ${topic} as a placeholder for the originating topic name.

	 - Type: STRING
	 - Default: ${topic}
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ table.types

The comma-separated types of database tables to which the sink connector can write. By default this is ``TABLE``, but any combination of ``TABLE``, ``PARTITIONED TABLE`` and ``VIEW`` is allowed. Not all databases support writing to views, and when they do the sink connector will fail if the view definition does not match the records' schemas (regardless of ``auto.evolve``).

	 - Type: LIST
	 - Default: TABLE
	 - Importance: LOW
	 - Required: false

ðŸ”˜ fields.whitelist



	 - Type: false
	 - Default: LIST
	 - Importance: List of comma-separated record value field names. If empty, all fields from the record value are utilized, otherwise used to filter to the desired fields.
	 - Required: MEDIUM

ðŸ”˜ timestamp.fields.list



	 - Type: false
	 - Default: LIST
	 - Importance: List of comma-separated record value timestamp field names that should be converted to timestamps. These fields will be converted based on precision mode specified in Timestamp Precision Mode. The timestamp fields included here should be Long or String type and nested fields are not supported.
	 - Required: MEDIUM

ðŸ”˜ db.timezone

Name of the JDBC timezone used in the connector when querying with time-based criteria. Defaults to UTC.

	 - Type: STRING
	 - Default: UTC
	 - Importance: MEDIUM
	 - Required: true

ðŸ”˜ timestamp.precision.mode

Convert the Timestamp with precision. If set to microseconds the timestamp will be converted to microsecond precision. If set to nanoseconds the timestamp will be converted to nanoseconds precision.

	 - Type: STRING
	 - Default: microseconds
	 - Importance: MEDIUM
	 - Required: true

ðŸ”˜ date.timezone

Name of the JDBC timezone that should be used in the connector when inserting DATE type values. Defaults to DB_TIMEZONE that uses the timezone set for db.timzeone configuration (to maintain backward compatibility). It is recommended to set this to UTC to avoid conversion for DATE type values.

	 - Type: STRING
	 - Default: DB_TIMEZONE
	 - Importance: MEDIUM
	 - Required: true

ðŸ”˜ date.calendar.system

Conversion of time since epoch value in kafka topic record to DATE or TIMESTAMP depends on the calendar used to interpret it. If LEGACY is used, it will use the hybrid Gregorian/Julian calendar which was the default in the older java date time APIs. However, if 'PROLEPTIC_GREGORIAN' is used, then it will use the proleptic gregorian calendar which extends the Gregorian rules backward indefinitely and does not apply the 1582 cutover. This matches the behavior of modern Java date/time APIs (java.time). This is defaulted to LEGACY for backward compatibility. The ideal setting for this depends on whether the values in source topic were populated using old or new java date time APIs. Changing this configuration on an existing connector might lead to a drift in the DATE/TIMESTAMP column's values populated in the sink database.

	 - Type: STRING
	 - Default: LEGACY
	 - Importance: MEDIUM
	 - Required: false

==========================
Primary Key
==========================
ðŸ”˜ pk.mode

none: No keys utilized.

	 - Type: false
	 - Default: STRING
	 - Importance: The primary key mode, also refer to pk.fields documentation for interplay. Supported modes are:
	 - Required: HIGH

ðŸ”˜ pk.fields

none: Ignored as no fields are used as primary key in this mode.

	 - Type: false
	 - Default: LIST
	 - Importance: List of comma-separated primary key field names. The runtime interpretation of this config depends on the pk.mode:
	 - Required: HIGH

==========================
SQL/DDL Support
==========================
ðŸ”˜ auto.create

Whether to automatically create the destination table if it is missing.

	 - Type: STRING
	 - Default: false
	 - Importance: MEDIUM
	 - Required: true

ðŸ”˜ auto.evolve

Whether to automatically add columns in the table if they are missing.

	 - Type: STRING
	 - Default: false
	 - Importance: MEDIUM
	 - Required: true

ðŸ”˜ quote.sql.identifiers

When to quote table names, column names, and other identifiers in SQL statements. For backward compatibility, the default is â€˜alwaysâ€™.

	 - Type: STRING
	 - Default: ALWAYS
	 - Importance: MEDIUM
	 - Required: false

==========================
Connection details
==========================
ðŸ”˜ batch.sizes

Maximum number of rows to include in a single batch when polling for new data. This setting can be used to limit the amount of data buffered internally in the connector.

	 - Type: INT
	 - Default: 3000
	 - Importance: LOW
	 - Required: false

==========================
Consumer configuration
==========================
ðŸ”˜ max.poll.interval.ms

The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 300000 milliseconds (5 minutes).

	 - Type: LONG
	 - Default: 300000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.poll.records

The maximum number of records to consume from Kafka in a single request. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 500 records.

	 - Type: LONG
	 - Default: 500
	 - Importance: LOW
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: all
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Egress allowlist
==========================
ðŸ”˜ connector.egress.whitelist



	 - Type: false
	 - Default: STRING
	 - Importance: List a comma-separated list of FQDNs, IPs, or CIDR ranges for secure, restricted network egress.
	 - Required: HIGH

