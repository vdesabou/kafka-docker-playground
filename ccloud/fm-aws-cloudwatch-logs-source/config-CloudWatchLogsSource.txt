==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Behavior On Errors
==========================
ðŸ”˜ behavior.on.error

Error handling behavior setting for the connector. Must be configured to one of the following: IGNORE, FAIL

	 - Type: STRING
	 - Default: FAIL
	 - Importance: MEDIUM
	 - Required: false

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode, whenever possible.

	 - Type: STRING
	 - Default: ${connect.regional.connector}
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ datapreview.schemas.enable

This config key only applies to data preview requests and governs whether the data preview output has record schema with it.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
How do you want to define topic names?
==========================
ðŸ”˜ kafka.topic.format

Topic format to use for generating the names of the Apache KafkaÂ® topics to publish data to. This format string can contain ${log-group} and ${log-stream} as a placeholder for the original log group and log stream names.

	 - Type: STRING
	 - Default: ${log-group}.${log-stream}
	 - Importance: HIGH
	 - Required: true

==========================
Output messages
==========================
ðŸ”˜ output.data.format



	 - Type: true
	 - Default: STRING
	 - Importance: Sets the output Kafka record value format. Valid entries are AVRO, JSON or STRING. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO.
	 - Required: HIGH

==========================
AWS credentials
==========================
ðŸ”˜ authentication.method

Select how you want to authenticate with AWS.

	 - Type: STRING
	 - Default: Access Keys
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ provider.integration.id



	 - Type: false
	 - Default: STRING
	 - Importance: Select an existing integration that has access to your resource. In case you need to integrate a new IAM role, use provider integration
	 - Required: HIGH

ðŸ”˜ aws.access.key.id



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The Amazon Access Key used to connect to Amazon CloudWatch.
	 - Required: HIGH

ðŸ”˜ aws.secret.access.key



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The Amazon Secret Key used to connect to Amazon CloudWatch.
	 - Required: HIGH

==========================
How should we connect to Amazon CloudWatch Logs?
==========================
ðŸ”˜ aws.cloudwatch.logs.url



	 - Type: true
	 - Default: STRING
	 - Importance: The URL to use as the endpoint for connecting to Amazon CloudWatch for Logs. For example, `https://logs.us-east-1.amazonaws.com`.
	 - Required: HIGH

==========================
CloudWatch Logs details
==========================
ðŸ”˜ aws.cloudwatch.log.group



	 - Type: true
	 - Default: STRING
	 - Importance: Name of the log group on Amazon CloudWatch under which the desired log streams are contained.
	 - Required: HIGH

ðŸ”˜ log.message.format

Set the format of log messages ingested from CloudWatch Log Streams. Valid entries are JSON and STRING.

	 - Type: STRING
	 - Default: STRING
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ aws.cloudwatch.log.streams



	 - Type: false
	 - Default: LIST
	 - Importance: List of the log stream(s) on Amazon CloudWatch under which the desired log records are sent through. If the field is left empty, all log streams under the log group will be tracked.
	 - Required: HIGH

ðŸ”˜ aws.poll.interval.ms

Time in milliseconds to wait between two consecutive polls to the Amazon CloudWatch endpoint.

	 - Type: INT
	 - Default: 1000
	 - Importance: HIGH
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: none
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Egress allowlist
==========================
ðŸ”˜ connector.egress.whitelist



	 - Type: false
	 - Default: STRING
	 - Importance: List a comma-separated list of FQDNs, IPs, or CIDR ranges for secure, restricted network egress.
	 - Required: HIGH

