==========================
Which topics do you want to get data from?
==========================
ðŸ”˜ topics



	 - Type: true
	 - Default: LIST
	 - Importance: Identifies the topic name or a comma-separated list of topic names.
	 - Required: HIGH

ðŸ”˜ errors.deadletterqueue.topic.name

The name of the topic to be used as the dead letter queue (DLQ) for messages that result in an error when processed by this sink connector, or its transformations or converters. Defaults to 'dlq-${connector}' if not set. The DLQ topic will be created automatically if it does not exist. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.

	 - Type: STRING
	 - Default: dlq-${connector}
	 - Importance: LOW
	 - Required: false

ðŸ”˜ reporter.result.topic.name

The name of the topic to produce records to after successfully processing a sink record. Defaults to 'success-${connector}' if not set. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.

	 - Type: STRING
	 - Default: success-${connector}
	 - Importance: LOW
	 - Required: false

ðŸ”˜ reporter.error.topic.name

The name of the topic to produce records to after each unsuccessful record sink attempt. Defaults to 'error-${connector}' if not set. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.

	 - Type: STRING
	 - Default: error-${connector}
	 - Importance: LOW
	 - Required: false

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
Input messages
==========================
ðŸ”˜ input.data.format

Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, JSON or BYTES. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: true

==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

==========================
Consumer configuration
==========================
ðŸ”˜ max.poll.interval.ms

The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 300000 milliseconds (5 minutes).

	 - Type: LONG
	 - Default: 300000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.poll.records

The maximum number of records to consume from Kafka in a single request. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 500 records.

	 - Type: LONG
	 - Default: 500
	 - Importance: LOW
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Authentication
==========================
ðŸ”˜ instance.url



	 - Type: true
	 - Default: STRING
	 - Importance: The OpenSearch instance URL. For example: `https://your-opensearch-instance.com/`.
	 - Required: HIGH

ðŸ”˜ auth.type

Authentication type of the endpoint. Valid values are ``NONE``, ``BASIC``.

	 - Type: STRING
	 - Default: BASIC
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ connection.user



	 - Type: false
	 - Default: STRING
	 - Importance: The username to be used with an endpoint requiring basic authentication.
	 - Required: MEDIUM

ðŸ”˜ connection.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The password to be used with an endpoint requiring basic authentication.
	 - Required: MEDIUM

ðŸ”˜ opensearch.ssl.enabled

Whether or not to connect to the endpoint via SSL.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ opensearch.ssl.keystorefile



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The keystore that contains the client certificate and private key. Supported formats are JKS or PKCS12. File system paths are not supported.
	 - Required: LOW

ðŸ”˜ opensearch.ssl.keystore.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The store password for the key store file.
	 - Required: HIGH

ðŸ”˜ opensearch.ssl.key.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The password for the private key in the key store file.
	 - Required: HIGH

ðŸ”˜ opensearch.ssl.truststorefile



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The truststore that contains the server CA certificate(s). Supported formats are JKS or PKCS12. File system paths are not supported.
	 - Required: HIGH

ðŸ”˜ opensearch.ssl.truststore.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The trust store password containing a server CA certificate.
	 - Required: HIGH

ðŸ”˜ opensearch.ssl.protocol

The protocol to use for SSL connections

	 - Type: STRING
	 - Default: TLSv1.3
	 - Importance: MEDIUM
	 - Required: false

==========================
Behavior on error
==========================
ðŸ”˜ behavior.on.error

Error handling behavior setting for handling error response from HTTP requests.

	 - Type: STRING
	 - Default: FAIL
	 - Importance: LOW
	 - Required: false

==========================
Indexes
==========================
ðŸ”˜ indexes.num

The number of indexes to push data to. This value should be less than or equal to 5

	 - Type: INT
	 - Default: 1
	 - Importance: HIGH
	 - Required: true

==========================
Retry configurations
==========================
ðŸ”˜ retry.backoff.policy

The backoff policy to use in terms of retry - CONSTANT_VALUE or EXPONENTIAL_WITH_JITTER

	 - Type: STRING
	 - Default: EXPONENTIAL_WITH_JITTER
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ retry.backoff.ms

The initial duration in milliseconds to wait following an error before a retry attempt is made. Subsequent backoff attempts can be a constant value or exponential with jitter (can be configured using api*.retry.backoff.policy parameter). Jitter adds randomness to the exponential backoff algorithm to prevent synchronized retries.

	 - Type: INT
	 - Default: 3000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ retry.on.status.codes

Comma-separated list of HTTP status codes or range of codes to retry on. Ranges are specified with start and optional end code. Range boundaries are inclusive. For instance, 400- includes all codes greater than or equal to 400. 400-500 includes codes from 400 to 500, including 500. Multiple ranges and single codes can be specified together to achieve fine-grained control over retry behavior. For example, 404,408,500- will retry on 404 NOT FOUND, 408 REQUEST TIMEOUT, and all 5xx error codes. Note that some status codes will always be retried, such as unauthorized, timeouts and too many requests.

	 - Type: STRING
	 - Default: 400-
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ max.retries

The maximum number of times to retry on errors before failing the task.

	 - Type: INT
	 - Default: 3
	 - Importance: MEDIUM
	 - Required: false

==========================
Index 1 configuration
==========================
ðŸ”˜ index1.name



	 - Type: false
	 - Default: STRING
	 - Importance: The index name together with the OpenSearch Instance URL will form the complete HTTP(S) URL. This path can be templated with offset information.
	 - Required: HIGH

ðŸ”˜ index1.topic



	 - Type: false
	 - Default: STRING
	 - Importance: Topic from where data will be pulled for this Index
	 - Required: HIGH

ðŸ”˜ index1.behavior.on.null.values

How to handle records with a non-null key and a null value (i.e. Kafka tombstone records). Valid options are ``IGNORE``, ``DELETE`` and ``FAIL``

	 - Type: STRING
	 - Default: IGNORE
	 - Importance: LOW
	 - Required: false

ðŸ”˜ index1.batch.size

Size of the batch of records to be sent to the OpenSearch. Note that Basic and Standard Clusters may experience throughput limitations, even with a higher batch size.

	 - Type: INT
	 - Default: 1
	 - Importance: LOW
	 - Required: false

ðŸ”˜ index1.report.only.status.code.to.success.topic

Whether to report only the status code to the success topic. If the API response payload is huge, it is recommended to set this to true, for better throughput.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ index1.write.method

The write method determines how you add data to an index. The INSERT method inserts only new documents, while the UPSERT method updates existing documents or inserts new ones if they don't exist.

	 - Type: STRING
	 - Default: INSERT
	 - Importance: HIGH
	 - Required: false

==========================
Index 2 configuration
==========================
ðŸ”˜ index2.name



	 - Type: false
	 - Default: STRING
	 - Importance: The index name together with the OpenSearch Instance URL will form the complete HTTP(S) URL. This path can be templated with offset information.
	 - Required: HIGH

ðŸ”˜ index2.topic



	 - Type: false
	 - Default: STRING
	 - Importance: Topic from where data will be pulled for this Index
	 - Required: HIGH

ðŸ”˜ index2.behavior.on.null.values

How to handle records with a non-null key and a null value (i.e. Kafka tombstone records). Valid options are ``IGNORE``, ``DELETE`` and ``FAIL``

	 - Type: STRING
	 - Default: IGNORE
	 - Importance: LOW
	 - Required: false

ðŸ”˜ index2.batch.size

Size of the batch of records to be sent to the OpenSearch. Note that Basic and Standard Clusters may experience throughput limitations, even with a higher batch size.

	 - Type: INT
	 - Default: 1
	 - Importance: LOW
	 - Required: false

ðŸ”˜ index2.report.only.status.code.to.success.topic

Whether to report only the status code to the success topic. If the API response payload is huge, it is recommended to set this to true, for better throughput.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ index2.write.method

The write method determines how you add data to an index. The INSERT method inserts only new documents, while the UPSERT method updates existing documents or inserts new ones if they don't exist.

	 - Type: STRING
	 - Default: INSERT
	 - Importance: HIGH
	 - Required: false

==========================
Index 3 configuration
==========================
ðŸ”˜ index3.name



	 - Type: false
	 - Default: STRING
	 - Importance: The index name together with the OpenSearch Instance URL will form the complete HTTP(S) URL. This path can be templated with offset information.
	 - Required: HIGH

ðŸ”˜ index3.topic



	 - Type: false
	 - Default: STRING
	 - Importance: Topic from where data will be pulled for this Index
	 - Required: HIGH

ðŸ”˜ index3.behavior.on.null.values

How to handle records with a non-null key and a null value (i.e. Kafka tombstone records). Valid options are ``IGNORE``, ``DELETE`` and ``FAIL``

	 - Type: STRING
	 - Default: IGNORE
	 - Importance: LOW
	 - Required: false

ðŸ”˜ index3.batch.size

Size of the batch of records to be sent to the OpenSearch. Note that Basic and Standard Clusters may experience throughput limitations, even with a higher batch size.

	 - Type: INT
	 - Default: 1
	 - Importance: LOW
	 - Required: false

ðŸ”˜ index3.report.only.status.code.to.success.topic

Whether to report only the status code to the success topic. If the API response payload is huge, it is recommended to set this to true, for better throughput.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ index3.write.method

The write method determines how you add data to an index. The INSERT method inserts only new documents, while the UPSERT method updates existing documents or inserts new ones if they don't exist.

	 - Type: STRING
	 - Default: INSERT
	 - Importance: HIGH
	 - Required: false

==========================
Index 4 configuration
==========================
ðŸ”˜ index4.name



	 - Type: false
	 - Default: STRING
	 - Importance: The index name together with the OpenSearch Instance URL will form the complete HTTP(S) URL. This path can be templated with offset information.
	 - Required: HIGH

ðŸ”˜ index4.topic



	 - Type: false
	 - Default: STRING
	 - Importance: Topic from where data will be pulled for this Index
	 - Required: HIGH

ðŸ”˜ index4.behavior.on.null.values

How to handle records with a non-null key and a null value (i.e. Kafka tombstone records). Valid options are ``IGNORE``, ``DELETE`` and ``FAIL``

	 - Type: STRING
	 - Default: IGNORE
	 - Importance: LOW
	 - Required: false

ðŸ”˜ index4.batch.size

Size of the batch of records to be sent to the OpenSearch. Note that Basic and Standard Clusters may experience throughput limitations, even with a higher batch size.

	 - Type: INT
	 - Default: 1
	 - Importance: LOW
	 - Required: false

ðŸ”˜ index4.report.only.status.code.to.success.topic

Whether to report only the status code to the success topic. If the API response payload is huge, it is recommended to set this to true, for better throughput.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ index4.write.method

The write method determines how you add data to an index. The INSERT method inserts only new documents, while the UPSERT method updates existing documents or inserts new ones if they don't exist.

	 - Type: STRING
	 - Default: INSERT
	 - Importance: HIGH
	 - Required: false

==========================
Index 5 configuration
==========================
ðŸ”˜ index5.name



	 - Type: false
	 - Default: STRING
	 - Importance: The index name together with the OpenSearch Instance URL will form the complete HTTP(S) URL. This path can be templated with offset information.
	 - Required: HIGH

ðŸ”˜ index5.topic



	 - Type: false
	 - Default: STRING
	 - Importance: Topic from where data will be pulled for this Index
	 - Required: HIGH

ðŸ”˜ index5.behavior.on.null.values

How to handle records with a non-null key and a null value (i.e. Kafka tombstone records). Valid options are ``IGNORE``, ``DELETE`` and ``FAIL``

	 - Type: STRING
	 - Default: IGNORE
	 - Importance: LOW
	 - Required: false

ðŸ”˜ index5.batch.size

Size of the batch of records to be sent to the OpenSearch. Note that Basic and Standard Clusters may experience throughput limitations, even with a higher batch size.

	 - Type: INT
	 - Default: 1
	 - Importance: LOW
	 - Required: false

ðŸ”˜ index5.report.only.status.code.to.success.topic

Whether to report only the status code to the success topic. If the API response payload is huge, it is recommended to set this to true, for better throughput.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ index5.write.method

The write method determines how you add data to an index. The INSERT method inserts only new documents, while the UPSERT method updates existing documents or inserts new ones if they don't exist.

	 - Type: STRING
	 - Default: INSERT
	 - Importance: HIGH
	 - Required: false

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.replace.null.with.default

Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.schemas.enable

Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: all
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Egress allowlist
==========================
ðŸ”˜ connector.egress.whitelist



	 - Type: false
	 - Default: STRING
	 - Importance: List a comma-separated list of FQDNs, IPs, or CIDR ranges for secure, restricted network egress.
	 - Required: HIGH

