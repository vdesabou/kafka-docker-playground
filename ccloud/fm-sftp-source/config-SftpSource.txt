==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ datapreview.schemas.enable

This config key only applies to data preview requests and governs whether the data preview output has record schema with it.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
Which topic do you want to send data to?
==========================
ðŸ”˜ kafka.topic



	 - Type: true
	 - Default: STRING
	 - Importance: Identifies the topic name to write the data to.
	 - Required: HIGH

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
Output messages
==========================
ðŸ”˜ output.data.format



	 - Type: true
	 - Default: STRING
	 - Importance: Sets the output message format. Valid entries are AVRO, JSON_SR, PROTOBUF, JSON, STRING or BYTES. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF
	 - Required: HIGH

==========================
Input file parser format
==========================
ðŸ”˜ input.file.parser.format

Parser that should be used to parse fetched files from sftp directory

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: false

==========================
SFTP Details
==========================
ðŸ”˜ sftp.host



	 - Type: true
	 - Default: STRING
	 - Importance: Host address of the SFTP server.
	 - Required: HIGH

ðŸ”˜ sftp.port

Port number of the SFTP server.

	 - Type: INT
	 - Default: 22
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ sftp.username



	 - Type: true
	 - Default: STRING
	 - Importance: Username for the SFTP connection.
	 - Required: HIGH

ðŸ”˜ sftp.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Password for the SFTP connection (not required if using TLS).
	 - Required: HIGH

ðŸ”˜ tls.pemfile



	 - Type: false
	 - Default: PASSWORD
	 - Importance: PEM file to be used for authentication via TLS.
	 - Required: HIGH

ðŸ”˜ tls.passphrase



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Passphrase that will be used to decrypt the private key if the given private key is encrypted.
	 - Required: HIGH

==========================
SFTP directory
==========================
ðŸ”˜ input.path



	 - Type: true
	 - Default: STRING
	 - Importance: The SFTP directory to read files that will be processed.This directory must exist and be writable by the user running Kafka Connect.
	 - Required: HIGH

ðŸ”˜ finished.path



	 - Type: true
	 - Default: STRING
	 - Importance: The SFTP directory to place files that have been successfully processed. This directory must exist and be writable by the user running Kafka Connect.
	 - Required: HIGH

ðŸ”˜ error.path



	 - Type: true
	 - Default: STRING
	 - Importance: The SFTP directory to place files in which there are error(s). This directory must exist and be writable by the user running Kafka Connect.
	 - Required: HIGH

==========================
File System
==========================
ðŸ”˜ cleanup.policy

Determines how the connector should cleanup the files that have been successfully processed. NONE leaves the files in place which could cause them to be reprocessed if the connector is restarted. DELETE removes the file from the filesystem. MOVE will move the file to a finished directory.

	 - Type: STRING
	 - Default: MOVE
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ input.file.pattern



	 - Type: true
	 - Default: STRING
	 - Importance: Regular expression to check input file names against. This expression must match the entire filename. The equivalent of Matcher.matches().
	 - Required: HIGH

ðŸ”˜ behavior.on.error

Should the task halt when it encounters an error or continue to the next file.

	 - Type: STRING
	 - Default: FAIL
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ file.minimum.age.ms

The amount of time in milliseconds after the file was last written to before the file can be processed. For default 0, connector processes all files irrespective of age

	 - Type: LONG
	 - Default: 0
	 - Importance: LOW
	 - Required: false

==========================
Connection details
==========================
ðŸ”˜ batch.size

The number of records that should be returned with each batch.

	 - Type: INT
	 - Default: 1000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ empty.poll.wait.ms

The amount of time to wait if a poll returns an empty list of records.

	 - Type: LONG
	 - Default: 250
	 - Importance: LOW
	 - Required: false

==========================
Schema
==========================
ðŸ”˜ key.schema



	 - Type: false
	 - Default: STRING
	 - Importance: The schema for the key written to Kafka. Set the actual schema, not the schema ID. To generate the schema, use the tool available here: https://github.com/jcustenborder/kafka-connect-spooldir?tab=readme-ov-file#tip-1
	 - Required: HIGH

ðŸ”˜ value.schema



	 - Type: false
	 - Default: STRING
	 - Importance: The schema for the value written to Kafka. Set the actual schema, not the schema ID. To generate the schema, use the tool available here: https://github.com/jcustenborder/kafka-connect-spooldir?tab=readme-ov-file#tip-1
	 - Required: HIGH

==========================
Schema Generation
==========================
ðŸ”˜ schema.generation.enabled



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Flag to determine if schemas should be dynamically generated. If set  to true, `key.schema` and `value.schema` can be omitted, but `schema.generation.key.name` and `schema.generation.value.name` must be set.
	 - Required: MEDIUM

ðŸ”˜ schema.generation.key.fields



	 - Type: false
	 - Default: LIST
	 - Importance: The field(s) to use to build a key schema. This is only used during schema generation.
	 - Required: MEDIUM

ðŸ”˜ schema.generation.key.name



	 - Type: false
	 - Default: STRING
	 - Importance: The name of the generated key schema.
	 - Required: MEDIUM

ðŸ”˜ schema.generation.value.name



	 - Type: false
	 - Default: STRING
	 - Importance: The name of the generated value schema.
	 - Required: MEDIUM

==========================
Timestamps
==========================
ðŸ”˜ timestamp.mode



	 - Type: false
	 - Default: STRING
	 - Importance: Determines how the connector will set the timestamp for the ConnectRecord. If set to `FIELD` then the timestamp will be read from a field in the value. This field cannot be optional and must be a Timestamp. Specify the field in `timestamp.field`. If set to `FILE_TIME` then the last modified time of the file will be used. If set to `PROCESS_TIME` the time the record is read will be used.
	 - Required: MEDIUM

ðŸ”˜ timestamp.field



	 - Type: false
	 - Default: STRING
	 - Importance: The field in the value schema that will contain the parsed timestamp for the record. This field cannot be marked as optional and must be a [Timestamp] (https://kafka.apache.org/0102/javadoc/org/apache/kafka/connect/data/Schema.html)
	 - Required: MEDIUM

ðŸ”˜ parser.timestamp.timezone



	 - Type: false
	 - Default: STRING
	 - Importance: The timezone that all of the dates will be parsed with.
	 - Required: LOW

ðŸ”˜ parser.timestamp.date.formats



	 - Type: false
	 - Default: LIST
	 - Importance: The date formats that are expected in the file. This is a list of strings that will be used to parse the date fields in order. The most accurate date format should be the first in the list. Take a look at the Java documentation for more info. https://docs.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html
	 - Required: LOW

==========================
CSV Parsing
==========================
ðŸ”˜ csv.skip.lines

Number of lines to skip in the beginning of the file.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.separator.char

The character that separates each field in the form of an integer. Typically in a CSV this is a ,(44) character. A TSV would use a tab(9) character. If `csv.separator.char` is defined as a null(0), then the RFC 4180 parser must be utilized by default. This is the equivalent of `csv.rfc.4180.parser.enabled = true`.

	 - Type: INT
	 - Default: 44
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.quote.char

The character that is used to quote a field. Typically in a CSV this is a "(34) character. This typically happens when the csv.separator.char character is within the data.

	 - Type: INT
	 - Default: 34
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.escape.char

The character as an integer to use when a special character is encountered. The default escape character is typically a \(92)

	 - Type: INT
	 - Default: 92
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.strict.quotes

Sets the strict quotes setting - if true, characters outside the quotes are ignored.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.ignore.leading.whitespace



	 - Type: false
	 - Default: STRING
	 - Importance: Sets the ignore leading whitespace setting - if true, white space in front of a quote in a field is ignored.
	 - Required: LOW

ðŸ”˜ csv.ignore.quotations

Sets the ignore quotations mode - if true, quotations are ignored.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.keep.carriage.return

Flag to determine if the carriage return at the end of the line should be maintained. 

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.null.field.indicator

Indicator to determine how the CSV Reader can determine if a field is null. Valid values are EMPTY_SEPARATORS, EMPTY_QUOTES, BOTH, NEITHER. For more information see http://opencsv.sourceforge.net/apidocs/com/opencsv/enums/CSVReaderNullFieldIndicator.html.

	 - Type: STRING
	 - Default: NEITHER
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.first.row.as.header



	 - Type: false
	 - Default: STRING
	 - Importance: Flag to indicate if the fist row of data contains the header of the file. If true the position of the columns will be determined by the first row to the CSV. The column position will be inferred from the position of the schema supplied in `value.schema`. If set to true the number of columns must be greater than or equal to the number of fields in the schema.
	 - Required: MEDIUM

ðŸ”˜ csv.file.charset

Character set to read wth file with.

	 - Type: STRING
	 - Default: UTF-8
	 - Importance: LOW
	 - Required: false

ðŸ”˜ ui.csv.pre.validate.file.enabled

Flag to enable validating the integrity of all records in the CSV file before processing any of its records.  For example, if any of the records have a linefeed within an unquoted field, which would incorrectly break the record at that point, then the entire fil will  be considered erroneous and no records from that file will be processed.  The failed file would be moved to the configured error path.  Important: If the number of records in a file is larger than the configured batch size, then portions of the file may be retrieved from the sftp server by the connector more than once.

	 - Type: STRING
	 - Default: NO
	 - Importance: LOW
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: none
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Egress allowlist
==========================
ðŸ”˜ connector.egress.whitelist



	 - Type: false
	 - Default: STRING
	 - Importance: List a comma-separated list of FQDNs, IPs, or CIDR ranges for secure, restricted network egress.
	 - Required: HIGH

