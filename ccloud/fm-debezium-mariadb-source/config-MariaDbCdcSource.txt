==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ datapreview.schemas.enable

This config key only applies to data preview requests and governs whether the data preview output has record schema with it.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
How should we connect to your database?
==========================
ðŸ”˜ database.hostname



	 - Type: true
	 - Default: STRING
	 - Importance: IP address or hostname of the MariaDB database server.
	 - Required: HIGH

ðŸ”˜ database.port



	 - Type: true
	 - Default: INT
	 - Importance: Port number of the MariaDB database server.
	 - Required: HIGH

ðŸ”˜ database.user



	 - Type: true
	 - Default: STRING
	 - Importance: The name of the MariaDB database user that has the required authorization.
	 - Required: HIGH

ðŸ”˜ database.password



	 - Type: true
	 - Default: PASSWORD
	 - Importance: Password for the MariaDB database user that has the required authorization.
	 - Required: HIGH

ðŸ”˜ database.ssl.mode

Whether to use an encrypted connection to the MariaDB server. Possible settings are ``disable``, ``trust``, ``verify-ca``, and ``verify-full``. 

	 - Type: STRING
	 - Default: disable
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ database.ssl.keystore



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Path to the SSL keystore file for MariaDB connection. This property is only required when you enable SSL certificate verification (``verify-ca`` or ``verify-full`` modes)
	 - Required: MEDIUM

ðŸ”˜ database.ssl.keystore.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Password for the SSL keystore file for MariaDB connection. This property is only required when you enable SSL certificate verification (``verify-ca`` or ``verify-full`` modes)
	 - Required: MEDIUM

ðŸ”˜ database.ssl.truststore



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Path to the SSL truststore file for MariaDB connection. This property is only required when you enable SSL certificate verification (``verify-ca`` or ``verify-full`` modes)
	 - Required: MEDIUM

ðŸ”˜ database.ssl.truststore.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Password for the SSL truststore file for MariaDB connection. This property is only required when you enable SSL certificate verification (``verify-ca`` or ``verify-full`` modes)
	 - Required: MEDIUM

==========================
Output messages
==========================
ðŸ”˜ output.data.format

Sets the output Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.

	 - Type: STRING
	 - Default: AVRO
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ output.key.format

Sets the output Kafka record key format. Valid entries are AVRO, JSON_SR, PROTOBUF, STRING or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ after.state.only

Controls whether the generated Kafka record should contain only the state of the row after the event occurred.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ tombstones.on.delete

Controls whether a tombstone event should be generated after a delete event. 

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
How should we name your topic(s)?
==========================
ðŸ”˜ topic.prefix



	 - Type: true
	 - Default: STRING
	 - Importance: Topic prefix that provides a namespace (logical server name) for the particular MariaDB database server or cluster in which Debezium is capturing changes. The prefix should be unique across all other connectors, since it is used as a topic name prefix for all Kafka topics that receive records from this connector. Only alphanumeric characters, hyphens, dots and underscores must be used. The connector automatically creates Kafka topics using the naming convention: `<topic.prefix>.<databaseName>.<tableName>`.
	 - Required: HIGH

ðŸ”˜ schema.history.internal.kafka.topic

The name of the topic for the database schema history. A new topic with provided name is created, if it doesn't already exist. If the topic already exists, ensure that it has a single partition, infinite retention period and is not in use by any other connector. If no value is provided, the name defaults to ``dbhistory.<topic-prefix>.<lcc-id>``.

	 - Type: STRING
	 - Default: dbhistory.${topic.prefix}.{{.logicalClusterId}}
	 - Importance: HIGH
	 - Required: false

==========================
CSFLE
==========================
ðŸ”˜ csfle.enabled

Determines whether the connector honours CSFLE rules or not

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ sr.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: Select the service account that has appropriate permissions to schemas and encryption keys in the Schema Registry.
	 - Required: HIGH

==========================
Database config
==========================
ðŸ”˜ signal.data.collection



	 - Type: false
	 - Default: STRING
	 - Importance: Fully-qualified name of the data collection that needs to be used to send signals to the connector. Use the following format to specify the fully-qualified collection name: `databaseName.tableName` 
	 - Required: MEDIUM

ðŸ”˜ database.include.list

To match the name of a database, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the database; it does not match substrings that might be present in a database name.

	 - Type: false
	 - Default: LIST
	 - Importance: An optional, comma-separated list of regular expressions that match the names of the databases for which to capture changes. The connector does not capture changes in any database whose name is not in this list. By default, the connector captures changes in all databases.
	 - Required: MEDIUM

ðŸ”˜ database.exclude.list



	 - Type: false
	 - Default: LIST
	 - Importance: An optional, comma-separated list of regular expressions that match the names of databases from which you do not want the connector to capture changes. The connector captures changes in any database that is not named in the database.exclude.list.
	 - Required: MEDIUM

ðŸ”˜ gtid.source.includes



	 - Type: false
	 - Default: LIST
	 - Importance: A comma-separated list of regular expressions that match source domain IDs in the GTID set used that the connector uses to find the binlog position on the MariaDB server. When this property is set, the connector uses only the GTID ranges that have source UUIDs that match one of the specified include patterns.
	 - Required: MEDIUM

ðŸ”˜ gtid.source.excludes



	 - Type: false
	 - Default: LIST
	 - Importance: A comma-separated list of regular expressions that match source domain IDs in the GTID set that the connector uses to find the binlog position on the MariaDB server. When this property is set, the connector uses only the GTID ranges that have source UUIDs that do not match any of the specified exclude patterns.
	 - Required: MEDIUM

ðŸ”˜ table.ignore.builtin

A Boolean value that specifies whether built-in system tables should be ignored. This applies regardless of the table include and exclude lists. By default, changes that occur to the values in system tables are excluded from capture, and Debezium does not generate events for system table changes.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Connector config
==========================
ðŸ”˜ snapshot.mode

Specifies the criteria for running a snapshot when the connector starts. Possible settings are: `initial`, `initial_only`, `when_needed`, `no_data`, and `recovery`. 

	 - Type: STRING
	 - Default: initial
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ snapshot.locking.mode

Controls whether and how long the connector holds the global MariaDB read lock, which prevents any updates to the database, while the connector is performing a snapshot. Possible settings are: `minimal`, `extended`, and `none`. 

	 - Type: STRING
	 - Default: minimal
	 - Importance: LOW
	 - Required: false

ðŸ”˜ incremental.snapshot.allow.schema.changes

Specifies whether the connector allows schema changes during an incremental snapshot. When the value is set to true, the connector detects schema change during an incremental snapshot, and re-select a current chunk to avoid locking DDLs.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ table.include.list

To match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the table; it does not match substrings that might be present in a table name. 

	 - Type: false
	 - Default: LIST
	 - Importance: An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you want to capture. When this property is set, the connector captures changes only from the specified tables. Each identifier is of the form `schemaName.tableName`. By default, the connector captures changes in every non-system table in each schema whose changes are being captured. 
	 - Required: MEDIUM

ðŸ”˜ table.exclude.list

To match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the table; it does not match substrings that might be present in a table name. 

	 - Type: false
	 - Default: LIST
	 - Importance: An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you do not want to capture. Each identifier is of the form `schemaName.tableName`. When this property is set, the connector captures changes from every table that you do not specify. 
	 - Required: MEDIUM

ðŸ”˜ event.processing.failure.handling.mode

Specifies how the connector should react to exceptions during processing of events. Possible settings are: `fail`, `skip`, and `warn`. 

	 - Type: STRING
	 - Default: fail
	 - Importance: LOW
	 - Required: false

ðŸ”˜ column.exclude.list

To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; it does not match substrings that might be present in a column name. Do not set ``column.include.list`` if you set this property.

	 - Type: false
	 - Default: LIST
	 - Importance: An optional, comma-separated list of regular expressions that match the fully-qualified names of columns to exclude from change event record values. Fully-qualified names for columns are of the form `databaseName.tableName.columnName`. 
	 - Required: MEDIUM

ðŸ”˜ schema.name.adjustment.mode

Specifies how schema names should be adjusted for compatibility with the message converter used by the connector. Possible settings are: `none`, `avro`, and `avro_unicode`. 

	 - Type: STRING
	 - Default: none
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ field.name.adjustment.mode

Specifies how field names should be adjusted for compatibility with the message converter used by the connector. Possible settings are: `none`, `avro`, and `avro_unicode`. 

	 - Type: STRING
	 - Default: none
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ heartbeat.interval.ms

Controls how frequently the connector sends heartbeat messages to a Kafka topic. The behavior of default value 0 is that the connector does not send heartbeat messages. Heartbeat messages are useful for monitoring whether the connector is receiving change events from the database. Heartbeat messages might help decrease the number of change events that need to be re-sent when a connector restarts. To send heartbeat messages, set this property to a positive integer, which indicates the number of milliseconds between heartbeat messages.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ connect.timeout.ms

A positive integer value that specifies the maximum time in milliseconds that the connector waits to establish a connection to the MariaDB database server before the connection request times out.

	 - Type: INT
	 - Default: 5000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ use.nongraceful.disconnect

A Boolean value that specifies whether the binary log client's keepalive thread sets the SO_LINGER socket option to 0 to immediately close stale TCP connections.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ key.converter.reference.subject.name.strategy

Set the subject reference name strategy for key. Valid entries are `DefaultReferenceSubjectNameStrategy` or `QualifiedReferenceSubjectNameStrategy`. Note that the subject reference name strategy can be selected only for `PROTOBUF` format with the default strategy being `DefaultReferenceSubjectNameStrategy`.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ include.schema.changes

Boolean value that specifies whether the connector publishes changes in the database schema to a Kafka topic with the same name as the topic prefix. The connector records each schema change with a key that contains the database name, and a value that is a JSON structure that describes the schema update. This mechanism for recording schema changes is independent of the connector's internal recording of changes to the database schema history.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ schema.history.internal.skip.unparseable.ddl

A Boolean value that specifies whether the connector should ignore malformed or unknown database statements (`true`), or stop processing so a human can fix the issue (`false`). Defaults to `false`. Consider setting this to `true` to ignore unparseable statements.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ schema.history.internal.store.only.captured.tables.ddl

A Boolean value that specifies whether the connector records schema structures from all tables in a schema or database, or only from tables that are designated for capture. Defaults to `false`. 

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ schema.history.internal.kafka.recovery.poll.interval.ms

An integer value that specifies the maximum number of milliseconds the connector should wait during startup/recovery while polling for persisted data.

	 - Type: INT
	 - Default: 100
	 - Importance: LOW
	 - Required: false

ðŸ”˜ schema.history.internal.kafka.query.timeout.ms

An integer value that specifies the maximum number of milliseconds the connector should wait while fetching cluster information using Kafka admin client.

	 - Type: INT
	 - Default: 3000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ schema.history.internal.kafka.create.timeout.ms

An integer value that specifies the maximum number of milliseconds the connector should wait while create kafka history topic using Kafka admin client.

	 - Type: INT
	 - Default: 30000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ schema.history.internal.store.only.captured.databases.ddl

A Boolean value that specifies whether the connector records schema structures from all logical databases in the database instance. Specify one of the following values:

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
How should we handle data types?
==========================
ðŸ”˜ decimal.handling.mode

Specifies how the connector should handle values for `DECIMAL` and `NUMERIC` columns. Possible settings are: `precise`, `double`, and `string`. 

	 - Type: STRING
	 - Default: precise
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ event.converting.failure.handling.mode

Specifies how the connector responds when it cannot convert a table record due to a mismatch between the data type of a column and the type specified by the Debezium internal schema.

	 - Type: STRING
	 - Default: warn
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ enable.time.adjuster

Boolean value that indicates whether the connector converts a 2-digit year specification to 4 digits. Set the value to false when conversion is fully delegated to the database.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ time.precision.mode

Time, date, and timestamps can be represented with different kinds of precisions: 

	 - Type: STRING
	 - Default: adaptive_time_microseconds
	 - Importance: LOW
	 - Required: false

ðŸ”˜ bigint.unsigned.handling.mode

Specifies how the connector represents BIGINT UNSIGNED columns in change events. Set one of the following options: long, or precise (BigDecimal)

	 - Type: STRING
	 - Default: long
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ inconsistent.schema.handling.mode

Specifies how the connector should react to binlog events that belong to a table missing from internal schema representation. Possible settings are: `fail`, `skip`, and `warn`. 

	 - Type: STRING
	 - Default: fail
	 - Importance: MEDIUM
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.replace.null.with.default

Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.schemas.enable

Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: none
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Egress allowlist
==========================
ðŸ”˜ connector.egress.whitelist



	 - Type: false
	 - Default: STRING
	 - Importance: List a comma-separated list of FQDNs, IPs, or CIDR ranges for secure, restricted network egress.
	 - Required: HIGH

