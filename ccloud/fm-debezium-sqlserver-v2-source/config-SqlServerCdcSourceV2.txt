==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ datapreview.schemas.enable

This config key only applies to data preview requests and governs whether the data preview output has record schema with it.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
How should we connect to your database?
==========================
ðŸ”˜ database.hostname



	 - Type: true
	 - Default: STRING
	 - Importance: IP address or hostname of the SQL Server database server.
	 - Required: HIGH

ðŸ”˜ database.port



	 - Type: true
	 - Default: INT
	 - Importance: Port number of the SQL Server database server.
	 - Required: HIGH

ðŸ”˜ database.user



	 - Type: true
	 - Default: STRING
	 - Importance: The name of the SQL Server database user that has the required authorization.
	 - Required: HIGH

ðŸ”˜ database.password



	 - Type: true
	 - Default: PASSWORD
	 - Importance: The password for the SQL Server database user that has the required authorization.
	 - Required: HIGH

ðŸ”˜ database.names



	 - Type: true
	 - Default: LIST
	 - Importance: The comma-separated list of the SQL Server database names from which to stream the changes.
	 - Required: HIGH

ðŸ”˜ database.encrypt

Controls SSL encryption for connections to a SQL Server database. The default value is false, indicating that the connector won't force the server to support TLS encryption. When set to true, the connector requests to use TLS encryption with the server.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

==========================
CSFLE
==========================
ðŸ”˜ csfle.enabled

Determines whether the connector honours CSFLE rules or not

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ sr.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: Select the service account that has appropriate permissions to schemas and encryption keys in the Schema Registry.
	 - Required: HIGH

==========================
Output messages
==========================
ðŸ”˜ output.data.format

Sets the output Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ output.key.format

Sets the output Kafka record key format. Valid entries are AVRO, JSON_SR, PROTOBUF, STRING or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ after.state.only

Controls whether the generated Kafka record should contain only the state of the row after the event occurred.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ tombstones.on.delete

Controls whether a tombstone event should be generated after a delete event. 

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
How should we name your topic(s)?
==========================
ðŸ”˜ topic.prefix



	 - Type: true
	 - Default: STRING
	 - Importance: Topic prefix that provides a namespace (logical server name) for the particular SQL Server database server or cluster in which Debezium is capturing changes. The prefix should be unique across all other connectors, since it is used as a topic name prefix for all Kafka topics that receive records from this connector. Only alphanumeric characters, hyphens, dots and underscores must be used. The connector automatically creates Kafka topics using the naming convention: `<topic.prefix>.<databaseName>.<schemaName>.<tableName>`.
	 - Required: HIGH

ðŸ”˜ schema.history.internal.kafka.topic

The name of the topic for the database schema history. A new topic with provided name is created, if it doesn't already exist. If the topic already exists, ensure that it has a single partition, infinite retention period and is not in use by any other connector. If no value is provided, the name defaults to ``dbhistory.<topic-prefix>.<lcc-id>``.

	 - Type: STRING
	 - Default: dbhistory.${topic.prefix}.{{.logicalClusterId}}
	 - Importance: HIGH
	 - Required: false

==========================
Database config
==========================
ðŸ”˜ signal.data.collection



	 - Type: false
	 - Default: STRING
	 - Importance: Fully-qualified name of the data collection that needs to be used to send signals to the connector. Use the following format to specify the fully-qualified collection name: `databaseName.schemaName.tableName`.
	 - Required: MEDIUM

==========================
Connector config
==========================
ðŸ”˜ snapshot.mode

Specifies the criteria for running a snapshot when the connector starts. Possible settings are: `initial`, `initial_only`, `schema_only`(deprecated), `no_data`, `recovery`. 

	 - Type: STRING
	 - Default: initial
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ driver.applicationIntent

Defines the connection property `applicationIntent` within the connection string. Possible settings are: `ReadWrite` and `ReadOnly`.

	 - Type: STRING
	 - Default: ReadWrite
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ snapshot.isolation.mode

A mode to control which transaction isolation level is used and how long the connector locks tables that are designated for capture. Possible settings are: `read_uncommitted`, `read_committed`, `repeatable_read`, `snapshot`, and `exclusive`. The `snapshot`, `read_committed` and `read_uncommitted` modes do not prevent other transactions from updating table rows during initial snapshot. The `exclusive` and `repeatable_read` modes do prevent concurrent updates. Mode choice also affects data consistency. Only `exclusive` and `snapshot` modes guarantee full consistency, that is, initial snapshot and streaming logs constitute a linear history. In case of `repeatable_read` and `read_committed` modes, it might happen that, for instance, a record added appears twice - once in initial snapshot and once in streaming phase. Nonetheless, that consistency level should do for data mirroring. For `read_uncommitted` there are no data consistency guarantees at all (some data might be lost or corrupted).

	 - Type: STRING
	 - Default: repeatable_read
	 - Importance: LOW
	 - Required: false

ðŸ”˜ table.include.list

To match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the table; it does not match substrings that might be present in a table name. 

	 - Type: false
	 - Default: LIST
	 - Importance: An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you want to capture. When this property is set, the connector captures changes only from the specified tables. Each identifier is of the form `schemaName.tableName`. By default, the connector captures changes in every non-system table in each schema whose changes are being captured. 
	 - Required: MEDIUM

ðŸ”˜ table.exclude.list

To match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the table; it does not match substrings that might be present in a table name. 

	 - Type: false
	 - Default: LIST
	 - Importance: An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you do not want to capture. Each identifier is of the form `schemaName.tableName`. When this property is set, the connector captures changes from every table that you do not specify. 
	 - Required: MEDIUM

ðŸ”˜ event.processing.failure.handling.mode

Specifies how the connector should react to exceptions during processing of events. Possible settings are: `fail`, `skip`, and `warn`. 

	 - Type: STRING
	 - Default: fail
	 - Importance: LOW
	 - Required: false

ðŸ”˜ column.exclude.list

To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; it does not match substrings that might be present in a column name.

	 - Type: false
	 - Default: LIST
	 - Importance: An optional, comma-separated list of regular expressions that match the fully-qualified names of columns to exclude from change event record values. Fully-qualified names for columns are of the form `schemaName.tableName.columnName`. 
	 - Required: MEDIUM

ðŸ”˜ schema.name.adjustment.mode

Specifies how schema names should be adjusted for compatibility with the message converter used by the connector. Possible settings are: `none`, `avro`, and `avro_unicode`. 

	 - Type: STRING
	 - Default: none
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ field.name.adjustment.mode

Specifies how field names should be adjusted for compatibility with the message converter used by the connector. Possible settings are: `none`, `avro`, and `avro_unicode`. 

	 - Type: STRING
	 - Default: none
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ heartbeat.interval.ms

Controls how frequently the connector sends heartbeat messages to a Kafka topic. The behavior of default value 0 is that the connector does not send heartbeat messages. Heartbeat messages are useful for monitoring whether the connector is receiving change events from the database. Heartbeat messages might help decrease the number of change events that need to be re-sent when a connector restarts. To send heartbeat messages, set this property to a positive integer, which indicates the number of milliseconds between heartbeat messages.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ heartbeat.action.query

This configuration helps in scenarios where the connector is capturing changes only from low-traffic database tables. Extended periods of inactivity in these tables can prevent the connector from advancing the LSN position in its stored offsets. To address this, create a heartbeat table in the database, and set this property to a DML statement that periodically updates the table by either inserting a new row or repeatedly updating the same row. Additionally, enable CDC on the heartbeat table and include the heartbeat table in the connector's capture set using the schema or table include configuration properties. This ensures the connector receives changes from the heartbeat table and can continue advancing the LSN in its stored offsets. The heartbeat query executes at regular intervals, as specified by the ``heartbeat.interval.ms`` configuration property. 

	 - Type: false
	 - Default: STRING
	 - Importance: If specified, the connector executes this query on every heartbeat against the source database. The query must be a valid SQL DML statement, typically an ``INSERT`` or ``UPDATE``, that targets a dedicated heartbeat table. 
	 - Required: LOW

ðŸ”˜ schema.history.internal.skip.unparseable.ddl

A Boolean value that specifies whether the connector should ignore malformed or unknown database statements (`true`), or stop processing so a human can fix the issue (`false`). Defaults to `false`. Consider setting this to `true` to ignore unparseable statements.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ schema.history.internal.store.only.captured.tables.ddl

A Boolean value that specifies whether the connector records schema structures from all tables in a schema or database, or only from tables that are designated for capture. Defaults to `false`. 

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ key.converter.reference.subject.name.strategy

Set the subject reference name strategy for key. Valid entries are `DefaultReferenceSubjectNameStrategy` or `QualifiedReferenceSubjectNameStrategy`. Note that the subject reference name strategy can be selected only for `PROTOBUF` format with the default strategy being `DefaultReferenceSubjectNameStrategy`.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: HIGH
	 - Required: false

==========================
How should we handle data types?
==========================
ðŸ”˜ decimal.handling.mode

Specifies how the connector should handle values for `DECIMAL` and `NUMERIC` columns. Possible settings are: `precise`, `double`, and `string`. 

	 - Type: STRING
	 - Default: precise
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ time.precision.mode

Time, date, and timestamps can be represented with different kinds of precisions: 

	 - Type: STRING
	 - Default: adaptive
	 - Importance: MEDIUM
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: false
	 - Default: INT
	 - Importance: Maximum number of tasks that the connector can use to capture data from the database instance. If the database.names list contains more than one element, you can increase the value of this property to a number less than or equal to the number of elements in the list.
	 - Required: HIGH

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.replace.null.with.default

Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.schemas.enable

Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: none
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

