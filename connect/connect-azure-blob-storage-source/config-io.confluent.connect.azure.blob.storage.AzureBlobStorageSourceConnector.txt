==========================
Transforms: AddPrefix
==========================
ðŸ”˜ transforms.AddPrefix.type

Class for the 'AddPrefix' transformation.

	 - Type: CLASS
	 - Default: null
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ transforms.AddPrefix.plugin.version

Version of the 'AddPrefix' transformation.

	 - Type: STRING
	 - Default: 8.1.0-ce
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ transforms.AddPrefix.regex

Regular expression to use for matching.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ transforms.AddPrefix.replacement

Replacement string.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ transforms.AddPrefix.negate

Whether the configured predicate should be negated.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ transforms.AddPrefix.predicate

The alias of a predicate used to determine whether to apply this transformation.

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

==========================
Storage
==========================
ðŸ”˜ mode

The connector's operation mode.

	 - Type: STRING
	 - Default: RESTORE_BACKUP
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ store.url

Store's connection URL, if applicable.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ topics.dir

Top level directory where data was stored to be re-ingested by Kafka.

	 - Type: STRING
	 - Default: topics
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ directory.delim

Directory delimiter pattern.

	 - Type: STRING
	 - Default: /
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ behavior.on.error

Error handling behavior setting for storage connectors. Must be configured to one of the following:

	 - Type: STRING
	 - Default: fail
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ format.bytearray.extension

Output file extension for Byte Array Format. Defaults to ``.bin``.

	 - Type: STRING
	 - Default: .bin
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ format.bytearray.separator



	 - Type: false
	 - Default: STRING
	 - Importance: String inserted between records for ByteArrayFormat. Defaults to ``System.lineSeparator()`` and may contain escape sequences like ``
``. An input record that contains the line separator looks like multiple records in the storage object output.
	 - Required: MEDIUM

ðŸ”˜ format.json.schema.enable

Enable reading of JSON messages with schema embedded.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

==========================
Connector
==========================
ðŸ”˜ record.batch.max.size

The maximum amount of records to return each time storage is polled.

	 - Type: INT
	 - Default: 200
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ schema.cache.size

The size of the schema cache used in the Avro converter.

	 - Type: INT
	 - Default: 50
	 - Importance: LOW
	 - Required: false

==========================
Partitioner
==========================
ðŸ”˜ partitioner.class

The partitioner to use when reading data to the store.

	 - Type: CLASS
	 - Default: io.confluent.connect.storage.partitioner.DefaultPartitioner
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ partition.field.name



	 - Type: false
	 - Default: LIST
	 - Importance: The name of the partitioning fields when FieldPartitioner is used.
	 - Required: MEDIUM

ðŸ”˜ path.format



	 - Type: false
	 - Default: STRING
	 - Importance: This configuration that was used to set the format of the data directories when partitioning with a TimeBasedPartitioner. For example, if you set ``path.format`` to ``'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH``, then a valid data directories would be: ``/year=2015/month=12/day=07/hour=15/``.
	 - Required: MEDIUM

==========================
Topic
==========================
ðŸ”˜ topic.regex.list



	 - Type: false
	 - Default: LIST
	 - Importance: A comma-separated list of pairs of type '<kafka topic>:<regex>' that is used to map file paths to Kafka topics. An example might be `topic1:.*\.json` to source all files ending in .json to a kafka topic named topic1. You can specify multiple of these topic:regex mappings to sending sets of files to different topics. Any files that aren't mapped by a regex will be ignored and not  sourced. The `topic.regex.list` property matches the full path (for example, `folder/file.txt`), not just the filename.
	 - Required: HIGH

==========================
Storage
==========================
ðŸ”˜ task.batch.size

The number of files assigned to each task at a time

	 - Type: INT
	 - Default: 10
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ file.discovery.starting.timestamp

A unix timestamp that denotes where to start processing files. Any file encountered with a creation time earlier than this will be ignored

	 - Type: LONG
	 - Default: 0
	 - Importance: MEDIUM
	 - Required: false

==========================
Connector
==========================
ðŸ”˜ bucket.listing.max.objects.threshold

	

	 - Type: INT
	 - Default: -1
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ parse.error.topic.prefix

Topic name prefix for topic where records with error information would be published, whenever connector encounters a malformed file.

	 - Type: STRING
	 - Default: error
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ bucket.scan.executor.threads

Number of threads to scan the bucket's folders in parallel. To get better performance , files should be distributed among multiple folders under root/topic directory. E.g. -> In case of topics->{dir1, dir2, dir3, dir4, dir5} connector will start scanning dir1, dir2, dir3, dir4, dir5 in parallel if `Bucket scan threads` is set to 5

	 - Type: INT
	 - Default: 5
	 - Importance: MEDIUM
	 - Required: false

==========================
csv
==========================
ðŸ”˜ value.schema



	 - Type: false
	 - Default: STRING
	 - Importance: The schema for the value written to Kafka.
	 - Required: MEDIUM

ðŸ”˜ csv.skip.lines

Number of lines to skip in the beginning of the file.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.separator.char

The character that separates each field in the form of an integer. Typically in a CSV this is a ,(44) character. A TSV would use a tab(9) character.

	 - Type: INT
	 - Default: 44
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.quote.char

The character that is used to quote a field. This typically happens when the csv.separator.char character is within the data.

	 - Type: INT
	 - Default: 34
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.escape.char

The character as an integer to use when a special character is encountered. The default escape character is typically a \(92)

	 - Type: INT
	 - Default: 92
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.strict.quotes

Sets the strict quotes setting - if true, characters outside the quotes are ignored.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.ignore.leading.whitespace

Sets the ignore leading whitespace setting - if true, white space in front of a quote in a field is ignored.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.ignore.quotations

Sets the ignore quotations mode - if true, quotations are ignored.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.keep.carriage.return

Flag to determine if the carriage return at the end of the line should be maintained. 

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.null.field.indicator

Indicator to determine how the CSV Reader can determine if a field is null. Valid values are [EMPTY_SEPARATORS, EMPTY_QUOTES, BOTH, NEITHER]. For more information see http://opencsv.sourceforge.net/apidocs/com/opencsv/enums/CSVReaderNullFieldIndicator.html.

	 - Type: STRING
	 - Default: NEITHER
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.first.row.as.header

Flag to indicate if the fist row of data contains the header of the file. If true the position of the columns will be determined by the first row to the CSV. The column position will be inferred from the position of the schema supplied in `value.schema`. If set to true the number of columns must be greater than or equal to the number of fields in the schema.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ csv.file.charset

Character set to read wth file with.

	 - Type: STRING
	 - Default: UTF-8
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.case.sensitive.field.names

Flag to determine if the field names in the header row should be treated as case sensitive.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.rfc.4180.parser.enabled

Flag to determine if the RFC 4180 parser should be used instead of the default parser.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

==========================
folders
==========================
ðŸ”˜ LIST



	 - Type: HIGH
	 - Default: false
	 - Importance: 
	 - Required: 

==========================
filename.regex
==========================
ðŸ”˜ (.+)\+(\d+)\+.+$



	 - Type: false
	 - Default: STRING
	 - Importance: 
	 - Required: MEDIUM

==========================
Connector
==========================
ðŸ”˜ azblob.poll.interval.ms

Frequency in milliseconds to poll for new or removed folders. This may result in updated task configurations starting to poll for data in added folders or stopping polling for data in removed folders.

	 - Type: LONG
	 - Default: 60000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ format.class

Class responsible for converting storage objects to source records.

	 - Type: CLASS
	 - Default: null
	 - Importance: HIGH
	 - Required: true

==========================
Azure
==========================
ðŸ”˜ azblob.account.name

Azure Blob Storage account name. Must be between 3-24 alphanumeric characters.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ azblob.account.key

The Azure Blob Storage account key. Must not be null or empty to access the account using an account key instead of ``azblob.sas.token``.

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ azblob.sas.token

A shared access signature (SAS) is a URI that grants restricted access rights to Azure Storage resources. You can provide a shared access signature to clients who should not be trusted with your storage account key but whom you wish to delegate access to certain storage account resources. By distributing a shared access signature URI to these clients, you grant them access to a resource for a specified period of time. An account-level SAS can delegate access to multiple storage services (i.e. blob, file, queue, table). Note that stored access policies are currently not supported for an account-level SAS.

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ azblob.container.name

The container name. Must be between 3-63 alphanumeric and '-' characters.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
null
==========================
ðŸ”˜ connect.azblob.useragent



	 - Type: STRING
	 - Default: APN/1.0 Confluent/1.0 AzureBlobSource/
	 - Importance: LOW
	 - Required: false

==========================
Proxy
==========================
ðŸ”˜ azblob.proxy.url



	 - Type: false
	 - Default: STRING
	 - Importance: Proxy settings encoded in URL syntax. This property is meant to be used only if you need to access Azure through a proxy.
	 - Required: LOW

ðŸ”˜ azblob.proxy.user



	 - Type: false
	 - Default: STRING
	 - Importance: Proxy Username. This property is meant to be used only if you need to access Azure through a proxy. Using ``azblob.proxy.user`` instead of embedding the username and password in ``azblob.proxy.url`` allows the password to be hidden in the logs.
	 - Required: LOW

ðŸ”˜ azblob.proxy.password

Proxy Password. This property is meant to be used only if you need to access Azure through a proxy. Using ``azblob.proxy.password`` instead of embedding the username and password in ``azblob.proxy.url`` allows the password to be hidden in the logs.

	 - Type: PASSWORD
	 - Default: null
	 - Importance: LOW
	 - Required: false

==========================
Retries
==========================
ðŸ”˜ azblob.retry.retries

Specifies the maximum number of retries attempts an operation will be tried before producing an error. A value of 1 means 1 try and 1 retries. The actual number of retry attempts is determined by the Azure client based on multiple factors, including, but not limited to - the value of this parameter, type of exception occurred, throttling settings of the underlying Azure client, etc.

	 - Type: INT
	 - Default: 3
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ azblob.retry.backoff.ms

Specifies the amount of delay to use before retrying an operation in milliseconds. The delay increases (exponentially or linearly) with each retry up to a maximum specified by MaxRetryDelay

	 - Type: LONG
	 - Default: 4000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ azblob.retry.max.backoff.ms

Specifies the maximum delay in milliseconds allowed before retrying an operation.

	 - Type: LONG
	 - Default: 120000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ azblob.retry.type

The policy specifying the type of retry pattern to use. Should be either EXPONENTIAL or FIXED. An EXPONENTIAL policy will start with a delay of the value of ``azblob.retry.backoff.ms`` in (ms) then double for each retry up to a max in time (azblob.retry.max.backoff.ms) or total retries (azblob.retry.retries). A FIXED policy will always just use the value of azblob.retry.backoff.ms (ms) as the delay up to a total number of tries equal to the value of azblob.retry.retries.

	 - Type: STRING
	 - Default: EXPONENTIAL
	 - Importance: LOW
	 - Required: false

ðŸ”˜ azblob.retry.secondary.host



	 - Type: false
	 - Default: STRING
	 - Importance: If a secondary host is specified, retries will be tried against this host. NOTE: Before setting this field, make sure you understand the issues around reading stale and potentially-inconsistent data at https://docs.microsoft.com/en-us/azure/storage/common/storage-designing-ha-apps-with-ragrs
	 - Required: LOW

ðŸ”˜ azblob.connection.timeout.ms

Indicates the maximum time allowed for any single try of an HTTP request. NOTE: When transferring large amounts of data, the default value will probably not be sufficient. You should override this value based on the bandwidth available to the host machine and proximity to the Storage service. A good starting point may be 60 seconds per MB of anticipated-payload-size.

	 - Type: LONG
	 - Default: 30000
	 - Importance: MEDIUM
	 - Required: false

