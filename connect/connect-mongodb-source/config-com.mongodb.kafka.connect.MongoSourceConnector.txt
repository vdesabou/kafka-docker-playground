==========================
Errors
==========================
ðŸ”˜ errors.tolerance

Behavior for tolerating errors during connector operation. 'none' is the default value and signals that any error will result in an immediate connector task failure; 'all' changes the behavior to skip over problematic records.

	 - Type: STRING
	 - Default: none
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ errors.log.enable

If true, write each error and the details of the failed operation and problematic record to the Connect application log. This is 'false' by default, so that only errors that are not tolerated are reported.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

==========================
Connection
==========================
ðŸ”˜ connection.uri

The connection URI as supported by the official drivers. eg: ``mongodb://user@pass@locahost/``.

	 - Type: PASSWORD
	 - Default: [hidden]
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ database



	 - Type: false
	 - Default: STRING
	 - Importance: The database to watch. If not set then all databases will be watched.
	 - Required: MEDIUM

ðŸ”˜ collection



	 - Type: false
	 - Default: STRING
	 - Importance: The collection in the database to watch. If not set then all collections will be watched.
	 - Required: MEDIUM

==========================
Server Api
==========================
ðŸ”˜ server.api.version



	 - Type: false
	 - Default: STRING
	 - Importance: The server API version to use. Disabled by default.
	 - Required: MEDIUM

ðŸ”˜ server.api.deprecation.errors

Sets whether the connector requires use of deprecated server APIs to be reported as errors.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ server.api.strict

Sets whether the application requires strict server API version enforcement.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

==========================
SSL
==========================
ðŸ”˜ connection.ssl.truststore



	 - Type: false
	 - Default: STRING
	 - Importance: A trust store certificate location to be used for SSL enabled connections
	 - Required: MEDIUM

ðŸ”˜ connection.ssl.truststorePassword

A trust store password to be used for SSL enabled connections

	 - Type: PASSWORD
	 - Default: [hidden]
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ connection.ssl.keystore



	 - Type: false
	 - Default: STRING
	 - Importance: A key store certificate location to be used for SSL enabled connections
	 - Required: MEDIUM

ðŸ”˜ connection.ssl.keystorePassword

A key store password to be used for SSL enabled connections

	 - Type: PASSWORD
	 - Default: [hidden]
	 - Importance: MEDIUM
	 - Required: false

==========================
Change stream
==========================
ðŸ”˜ pipeline

An inline JSON array with objects describing the pipeline operations to run.

	 - Type: STRING
	 - Default: []
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ batch.size

The cursor batch size.

	 - Type: INT
	 - Default: 0
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ publish.full.document.only

Only publish the actual changed document rather than the full change stream document. Automatically, sets `change.stream.full.document=updateLookup` so updated documents will be included.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ publish.full.document.only.tombstone.on.delete

Send a null value on a delete event. Requires publish.full.document.only=true.  Defaults to: false

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ change.stream.document.key.as.key

Use the document key as the source record key. Defaults to: true

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ change.stream.full.document.before.change

The pre-image is not available in source records published while copying existing data as a result of enabling `copy.existing`, and the pre-image configuration has no effect on copying.

	 - Type: false
	 - Default: STRING
	 - Importance: Specifies the pre-image configuration when creating a Change Stream.
	 - Required: HIGH

ðŸ”˜ change.stream.full.document

When set to 'updateLookup', the change stream for partial updates will include both a delta describing the changes to the document as well as a copy of the entire document that was changed from *some time* after the change occurred.

	 - Type: false
	 - Default: STRING
	 - Importance: Determines what to return for update operations when using a Change Stream.
	 - Required: HIGH

ðŸ”˜ change.stream.show.expanded.events

Determines if change streams notifies for DDL events, like the createIndexes and dropIndexes events.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ collation

Use the `Collation.asDocument().toJson()` to create the specific json representation.

	 - Type: false
	 - Default: STRING
	 - Importance: The json representation of the Collation options to use for the change stream.
	 - Required: HIGH

ðŸ”˜ poll.max.batch.size

Maximum number of change stream documents to include in a single batch when polling for new data. This setting can be used to limit the amount of data buffered internally in the connector.

	 - Type: INT
	 - Default: 1000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ poll.await.time.ms

The maximum amount of time in milliseconds the server waits for new data changes to report to the change stream cursor before returning an empty batch.

	 - Type: LONG
	 - Default: 5000
	 - Importance: LOW
	 - Required: false

==========================
Topic mapping
==========================
ðŸ”˜ topic.mapper

The class that determines the topic to write the source data to. By default this will be based on the 'ns' field in the change stream document, along with any configured prefix and suffix.

	 - Type: STRING
	 - Default: com.mongodb.kafka.connect.source.topic.mapping.DefaultTopicMapper
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ topic.separator

Separator to use when joining prefix, database & collection names, suffix to generate the name of the Kafka topic to publish data to. Used by the 'DefaultTopicMapper'.

	 - Type: STRING
	 - Default: .
	 - Importance: LOW
	 - Required: false

ðŸ”˜ topic.prefix



	 - Type: false
	 - Default: STRING
	 - Importance: Prefix to prepend to database & collection names to generate the name of the Kafka topic to publish data to. Used by the 'DefaultTopicMapper'.
	 - Required: LOW

ðŸ”˜ topic.suffix



	 - Type: false
	 - Default: STRING
	 - Importance: Suffix to append to database & collection names to generate the name of the Kafka topic to publish data to. Used by the 'DefaultTopicMapper'.
	 - Required: LOW

ðŸ”˜ topic.namespace.map

The name in each JSON name/value pair is a namespace pattern, the value is a string representing the corresponding topic name template. Pairs are ordered. When there are multiple pairs with equal namespace patterns (duplicates), they are deduplicated and only one pair is taken into account: its position is taken from the first pair among duplicates, its topic name template is taken from the last pair among duplicates. After deduplication, pairs with an empty topic name template are ignored when computing topic names. Note that a topic name computed based on this configuration is then decorated using the `topic.prefix` and `topic.suffix` configuration properties.

	 - Type: false
	 - Default: STRING
	 - Importance: A JSON object specifying how to map a MongoDB change stream document namespace to a Kafka topic name. Used by the `DefaultTopicMapper`. MongoDB change stream document namespace is a database name optionally concatenated with a collection name, separated by full stop '.'.
	 - Required: HIGH

==========================
Schema
==========================
ðŸ”˜ output.format.key

The output format of the data produced by the connector for the key. Supported formats are:

	 - Type: STRING
	 - Default: json
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ output.format.value

The output format of the data produced by the connector for the value. Supported formats are:

	 - Type: STRING
	 - Default: json
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ output.schema.infer.value

Infer the schema for the value. Each Document will be processed in isolation, which may lead to multiple schema definitions for the data. Only applied when output.format.value is set to schema.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ output.schema.key

The Avro schema definition for the key value of the SourceRecord.

	 - Type: STRING
	 - Default: {  "type": "record",  "name": "keySchema",  "fields" : [{"name": "_id", "type": "string"}]}
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ output.schema.value

The Avro schema definition for the value of the SourceRecord.

	 - Type: STRING
	 - Default: {  "name": "ChangeStream",  "type": "record",  "fields": [    { "name": "_id", "type": "string" },    { "name": "operationType", "type": ["string", "null"] },    { "name": "fullDocumentBeforeChange", "type": ["string", "null"] },    { "name": "fullDocument", "type": ["string", "null"] },    { "name": "ns",      "type": [{"name": "ns", "type": "record", "fields": [                {"name": "db", "type": "string"},                {"name": "coll", "type": ["string", "null"] } ]               }, "null" ] },    { "name": "to",      "type": [{"name": "to", "type": "record",  "fields": [                {"name": "db", "type": "string"},                {"name": "coll", "type": ["string", "null"] } ]               }, "null" ] },    { "name": "documentKey", "type": ["string", "null"] },    { "name": "updateDescription",      "type": [{"name": "updateDescription",  "type": "record", "fields": [                 {"name": "updatedFields", "type": ["string", "null"]},                 {"name": "removedFields",                  "type": [{"type": "array", "items": "string"}, "null"]},                 {"name": "truncatedArrays",                   "type": [{ "type":"array", "items": {"type": "record",                     "name": "truncatedArray", "fields": [                       {"name": "field", "type": "string"},                       {"name": "newSize", "type": "int"} ] }                   }, "null" ] },                 {"name": "disambiguatedPaths", "type": ["string", "null"]}                 ]}, "null"] },    { "name": "clusterTime", "type": ["string", "null"] },    { "name": "txnNumber", "type": ["long", "null"]},    { "name": "lsid", "type": [{"name": "lsid", "type": "record",               "fields": [ {"name": "id", "type": "string"},                             {"name": "uid", "type": "string"}] }, "null"] }  ]}
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ output.json.formatter

The output format of json strings can be configured to be either:

	 - Type: STRING
	 - Default: com.mongodb.kafka.connect.source.json.formatter.DefaultJson
	 - Importance: MEDIUM
	 - Required: false

==========================
Startup
==========================
ðŸ”˜ startup.mode

Resuming a change stream requires a resume token, which the connector stores as / reads from the source offset. If no source offset is available, the connector may either ignore all/some existing source data, or may at first copy all existing source data and then continue with processing new data. Possible values are 'latest', 'timestamp', 'copy_existing'.

	 - Type: false
	 - Default: STRING
	 - Importance: Specifies how the connector should start up when there is no source offset available.
	 - Required: MEDIUM

ðŸ”˜ startup.mode.timestamp.start.at.operation.time



	 - Type: false
	 - Default: STRING
	 - Importance: Actuated only if 'startup.mode = timestamp'. Specifies the starting point for the change stream. Must be either an integer number of seconds since the Epoch in the decimal format (example: 30), or an instant in the ISO-8601 format with one second precision (example: 'YYYY-MM-DDTHH:mm:ss'), or a BSON Timestamp in the canonical extended JSON (v2) format (example: '{"$timestamp": {"t": 30, "i": 0}}'). You may specify '0' to start at the beginning of the oplog. Requires MongoDB 4.0 or above. See https://www.mongodb.com/docs/current/reference/operator/aggregation/changeStream/.
	 - Required: MEDIUM

ðŸ”˜ startup.mode.copy.existing.max.threads

The number of threads to use when performing the data copy. Defaults to the number of processors.

	 - Type: INT
	 - Default: 4
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ startup.mode.copy.existing.queue.size

The max size of the queue to use when copying data.

	 - Type: INT
	 - Default: 16000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ startup.mode.copy.existing.pipeline

This can improve the use of indexes by the copying manager and make copying more efficient.

	 - Type: false
	 - Default: STRING
	 - Importance: An inline JSON array with objects describing the pipeline operations to run when copying existing data.
	 - Required: MEDIUM

ðŸ”˜ startup.mode.copy.existing.namespace.regex

 Example: The following regular expression will only include collections starting with `a` in the `demo` database: `demo\.a.*`.

	 - Type: false
	 - Default: STRING
	 - Importance: Use a regular expression to define from which existing namespaces data should be copied from. A namespace is the database name and collection separated by a period e.g. `database.collection`.
	 - Required: MEDIUM

ðŸ”˜ startup.mode.copy.existing.allow.disk.use

Copy existing data uses an aggregation pipeline that mimics change stream events. In certain contexts this can requirewriting to disk if the aggregation process runs out of memory.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ copy.existing

Deprecated, use 'startup.mode = copy_existing' instead; deprecated properties are overridden by normal ones if there is a conflict. Copy existing data from all the collections being used as the source then add any changes after. It should be noted that the reading of all the data during the copy and then the subsequent change stream events may produce duplicated events. During the copy, clients can make changes to the data in MongoDB, which may be represented both by the copying process and the change stream. However, as the change stream events are idempotent the changes can be applied so that the data is eventually consistent. Renaming a collection during the copying process is not supported.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ copy.existing.max.threads

Deprecated, use 'startup.mode.copy.existing.max.threads' instead; deprecated properties are overridden by normal ones if there is a conflict. The number of threads to use when performing the data copy. Defaults to the number of processors

	 - Type: INT
	 - Default: 4
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ copy.existing.queue.size

Deprecated, use 'startup.mode.copy.existing.queue.size' instead; deprecated properties are overridden by normal ones if there is a conflict. The max size of the queue to use when copying data.

	 - Type: INT
	 - Default: 16000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ copy.existing.pipeline

This can improve the use of indexes by the copying manager and make copying more efficient.

	 - Type: false
	 - Default: STRING
	 - Importance: Deprecated, use 'startup.mode.copy.existing.pipeline' instead; deprecated properties are overridden by normal ones if there is a conflict. An inline JSON array with objects describing the pipeline operations to run when copying existing data.
	 - Required: MEDIUM

ðŸ”˜ copy.existing.namespace.regex

 Example: The following regular expression will only include collections starting with `a` in the `demo` database: `demo\.a.*`

	 - Type: false
	 - Default: STRING
	 - Importance: Deprecated, use 'startup.mode.copy.existing.namespace.regex' instead; deprecated properties are overridden by normal ones if there is a conflict. Use a regular expression to define from which existing namespaces data should be copied from. A namespace is the database name and collection separated by a period e.g. `database.collection`.
	 - Required: MEDIUM

ðŸ”˜ copy.existing.allow.disk.use

Deprecated, use 'startup.mode.copy.existing.allow.disk.use' instead; deprecated properties are overridden by normal ones if there is a conflict. Copy existing data uses an aggregation pipeline that mimics change stream events. In certain contexts this can requirewriting to disk if the aggregation process runs out of memory.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Errors
==========================
ðŸ”˜ mongo.errors.tolerance

Use this property if you would like to configure the connector's error handling behavior differently from the Connect framework's.

	 - Type: STRING
	 - Default: none
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ mongo.errors.log.enable

Use this property if you would like to configure the connector's error handling behavior differently from the Connect framework's.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ errors.deadletterqueue.topic.name



	 - Type: false
	 - Default: STRING
	 - Importance: Whether to output conversion errors to the dead letter queue. Stops poison messages when using schemas, any message will be outputted as extended json on the specified topic. By default messages are not outputted to the dead letter queue. Also requires `errors.tolerance=all`.
	 - Required: MEDIUM

ðŸ”˜ mongo.errors.deadletterqueue.topic.name



	 - Type: false
	 - Default: STRING
	 - Importance: Use this property if you would like to configure the connector's error handling behavior differently from the Connect framework's.
	 - Required: MEDIUM

ðŸ”˜ heartbeat.interval.ms

The length of time between sending heartbeat messages to record the post batch resume token when no source records have been published. Improves the resumability of the connector for low volume namespaces. Use 0 to disable.

	 - Type: LONG
	 - Default: 0
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ heartbeat.topic.name

The name of the topic to publish heartbeats to. Defaults to '__mongodb_heartbeats'.

	 - Type: STRING
	 - Default: __mongodb_heartbeats
	 - Importance: MEDIUM
	 - Required: false

==========================
Partition
==========================
ðŸ”˜ offset.partition.name



	 - Type: false
	 - Default: STRING
	 - Importance: Use a custom offset partition name. If blank the default partition name based on the connection details will be used.
	 - Required: MEDIUM

==========================
null
==========================
ðŸ”˜ provider



	 - Type: false
	 - Default: STRING
	 - Importance: 
	 - Required: LOW

