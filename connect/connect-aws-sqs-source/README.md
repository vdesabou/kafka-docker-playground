# AWS SQS Source connector

![asciinema](https://github.com/vdesabou/gifs/blob/master/connect/connect-aws-sqs-source/asciinema.gif?raw=true)

## Objective

Quickly test [SQS Connector](https://docs.confluent.io/current/connect/kafka-connect-sqs/index.html#quick-start) connector.



## AWS Setup

* Make sure you have an [AWS account](https://docs.aws.amazon.com/streams/latest/dev/before-you-begin.html#setting-up-sign-up-for-aws).
* Set up [AWS Credentials](https://docs.confluent.io/current/connect/kafka-connect-kinesis/quickstart.html#aws-credentials)

This project assumes `~/.aws/credentials` and `~/.aws/config` are set, see `docker-compose.yml`file for connect:

```yaml
    connect:
    <snip>
    volumes:
        - $HOME/.aws/credentials:$CONNECT_CONTAINER_HOME_DIR/.aws/credentials:ro
        - $HOME/.aws/config:$CONNECT_CONTAINER_HOME_DIR/.aws/config:ro
```

## How to run

Simply run:

```bash
$ ./sqs.sh
```

Or using SASL_SSL authentication:

```bash
$ ./sqs-sasl-ssl.sh
```

Or using 2 way SSL authentication:

```bash
$ ./sqs-2way-ssl.sh
```

## Details of what the script is doing

### With no security in place:

Create a FIFO queue `sqs-source-connector-demo`:

```bash
$ aws sqs create-queue --queue-name sqs-source-connector-demo
```

Send messages to queue (QUEUE_URL is generated by the script)

```bash
$ aws sqs send-message-batch --queue-url $QUEUE_URL --entries file://send-message-batch.json
```

```json
[
    {
        "Id": "FuelReport-0001-2015-09-16T140731Z",
        "MessageBody": "Fuel report for account 0001 on 2015-09-16 at 02:07:31 PM.",
        "DelaySeconds": 10,
        "MessageAttributes": {
            "SellerName": {
                "DataType": "String",
                "StringValue": "Example Store"
            },
            "City": {
                "DataType": "String",
                "StringValue": "Any City"
            },
            "Region": {
                "DataType": "String",
                "StringValue": "WA"
            },
            "PostalCode": {
                "DataType": "String",
                "StringValue": "99065"
            },
            "PricePerGallon": {
                "DataType": "Number",
                "StringValue": "1.99"
            }
        }
    },
    {
        "Id": "FuelReport-0002-2015-09-16T140930Z",
        "MessageBody": "Fuel report for account 0002 on 2015-09-16 at 02:09:30 PM.",
        "DelaySeconds": 10,
        "MessageAttributes": {
            "SellerName": {
                "DataType": "String",
                "StringValue": "Example Fuels"
            },
            "City": {
                "DataType": "String",
                "StringValue": "North Town"
            },
            "Region": {
                "DataType": "String",
                "StringValue": "WA"
            },
            "PostalCode": {
                "DataType": "String",
                "StringValue": "99123"
            },
            "PricePerGallon": {
                "DataType": "Number",
                "StringValue": "1.87"
            }
        }
    }
]
```


The connector is created with:

```bash
curl -X PUT \
     -H "Content-Type: application/json" \
     --data '{
        "connector.class": "io.confluent.connect.sqs.source.SqsSourceConnector",
               "tasks.max": "1",
               "kafka.topic": "test-sqs-source",
               "sqs.url": "'"$QUEUE_URL"'",
               "confluent.license": "",
               "name": "sqs-source",
               "confluent.topic.bootstrap.servers": "broker:9092",
               "confluent.topic.replication.factor": "1"
          }' \
     http://localhost:8083/connectors/sqs-source/config | jq .
```

Verify we have received the data in test-sqs-source topic:

```bash
$ docker exec connect kafka-avro-console-consumer -bootstrap-server broker:9092 --property schema.registry.url=http://schema-registry:8081 --topic test-sqs-source --from-beginning --max-messages 2 | tail -n 5 | head -n 2 | jq .

{
  "ApproximateFirstReceiveTimestamp": 1569859344138,
  "ApproximateReceiveCount": 1,
  "SenderId": "AIDA6PUJEN7W4EMZQTVNV",
  "SentTimestamp": 1569858932274,
  "MessageDeduplicationId": null,
  "MessageGroupId": null,
  "SequenceNumber": null,
  "Body": "Fuel report for account 0001 on 2015-09-16 at 02:07:31 PM."
}
{
  "ApproximateFirstReceiveTimestamp": 1569859344140,
  "ApproximateReceiveCount": 1,
  "SenderId": "AIDA6PUJEN7W4EMZQTVNV",
  "SentTimestamp": 1569858932290,
  "MessageDeduplicationId": null,
  "MessageGroupId": null,
  "SequenceNumber": null,
  "Body": "Fuel report for account 0002 on 2015-09-16 at 02:09:30 PM."
}
```

### With SSL authentication:

Create a FIFO queue `sqs-source-connector-demo-sql`:

```bash
$ aws sqs create-queue --queue-name sqs-source-connector-demo-ssl
```

Send messages to queue (QUEUE_URL is generated by the script)

```bash
$ aws sqs send-message-batch --queue-url $QUEUE_URL --entries file://send-message-batch.json
```


The connector is created with:

```bash
curl -X PUT \
     --cert /etc/kafka/secrets/connect.certificate.pem --key /etc/kafka/secrets/connect.key --tlsv1.2 --cacert /etc/kafka/secrets/snakeoil-ca-1.crt \
     -H "Content-Type: application/json" \
     --data '{
                    "connector.class": "io.confluent.connect.sqs.source.SqsSourceConnector",
                    "tasks.max": "1",
                    "kafka.topic": "test-sqs-source-ssl",
                    "sqs.url": "'"$QUEUE_URL"'",
                    "confluent.license": "",
                    "name": "sqs-source-ssl",
                    "confluent.topic.bootstrap.servers": "broker:9092",
                    "confluent.topic.replication.factor": "1",
                    "confluent.topic.ssl.keystore.location" : "/etc/kafka/secrets/kafka.connect.keystore.jks",
                    "confluent.topic.ssl.keystore.password" : "confluent",
                    "confluent.topic.ssl.key.password" : "confluent",
                    "confluent.topic.ssl.truststore.location" : "/etc/kafka/secrets/kafka.connect.truststore.jks",
                    "confluent.topic.ssl.truststore.password" : "confluent",
                    "confluent.topic.ssl.keystore.type" : "JKS",
                    "confluent.topic.ssl.truststore.type" : "JKS",
                    "confluent.topic.security.protocol" : "SSL"
          }' \
     https://localhost:8083/connectors/sqs-source/config | jq .
```

Verify we have received the data in test-sqs-source topic:

```bash
$ docker exec connect kafka-avro-console-consumer -bootstrap-server broker:9092 --property schema.registry.url=http://schema-registry:8081 --topic test-sqs-source-ssl --from-beginning --max-messages 2 --property schema.registry.url=https://schema-registry:8085 --consumer.config /etc/kafka/secrets/client_without_interceptors_2way_ssl.config  | tail -n 3 | head -n 2 | jq .
```

### With SASL_SSL authentication:

Create a FIFO queue `sqs-source-connector-demo-sasl-sql`:

```bash
$ aws sqs create-queue --queue-name sqs-source-connector-demo-sasl-ssl
```

Send messages to queue (QUEUE_URL is generated by the script)

```bash
$ aws sqs send-message-batch --queue-url $QUEUE_URL --entries file://send-message-batch.json
```


The connector is created with:

```bash
curl -X PUT \
     --cert /etc/kafka/secrets/connect.certificate.pem --key /etc/kafka/secrets/connect.key --tlsv1.2 --cacert /etc/kafka/secrets/snakeoil-ca-1.crt \
     -H "Content-Type: application/json" \
     --data '{
                    "connector.class": "io.confluent.connect.sqs.source.SqsSourceConnector",
                    "tasks.max": "1",
                    "kafka.topic": "test-sqs-source-sasl-ssl",
                    "sqs.url": "'"$QUEUE_URL"'",
                    "confluent.license": "",
                    "name": "sqs-source-sasl-ssl",
                    "confluent.topic.bootstrap.servers": "broker:9092",
                    "confluent.topic.replication.factor": "1",
                    "confluent.topic.ssl.keystore.location" : "/etc/kafka/secrets/kafka.connect.keystore.jks",
                    "confluent.topic.ssl.keystore.password" : "confluent",
                    "confluent.topic.ssl.key.password" : "confluent",
                    "confluent.topic.ssl.truststore.location" : "/etc/kafka/secrets/kafka.connect.truststore.jks",
                    "confluent.topic.ssl.truststore.password" : "confluent",
                    "confluent.topic.security.protocol" : "SASL_SSL",
                    "confluent.topic.sasl.mechanism": "PLAIN",
                    "confluent.topic.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required  username=\"client\" password=\"client-secret\";"
          }' \
     https://localhost:8083/connectors/sqs-source/config | jq .
```

Verify we have received the data in test-sqs-source topic:

```bash
$ docker exec connect kafka-avro-console-consumer -bootstrap-server broker:9092 --property schema.registry.url=http://schema-registry:8081 --topic test-sqs-source-sasl-ssl --from-beginning --max-messages 2 --property schema.registry.url=https://schema-registry:8085 --consumer.config /etc/kafka/secrets/client_without_interceptors.config  | tail -n 3 | head -n 2 | jq .
```

N.B: Control Center is reachable at [http://127.0.0.1:9021](http://127.0.0.1:9021])
