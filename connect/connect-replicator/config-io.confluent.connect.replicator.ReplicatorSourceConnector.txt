==========================
Destination Data Conversion
==========================
ðŸ”˜ key.converter

Converter for the key field of messages going to the destination cluster.

	 - Type: CLASS
	 - Default: io.confluent.connect.replicator.util.ByteArrayConverter
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter

Converter for the value field of messages going to the destination cluster.

	 - Type: CLASS
	 - Default: io.confluent.connect.replicator.util.ByteArrayConverter
	 - Importance: LOW
	 - Required: false

ðŸ”˜ header.converter

HeaderConverter class used to convert serialized Kafka headers to Kafka Connect headers. Default value is ByteArrayConverter, which simply passes the input bytes into the destination record.

	 - Type: CLASS
	 - Default: io.confluent.connect.replicator.util.ByteArrayConverter
	 - Importance: LOW
	 - Required: false

==========================
Confluent Platform
==========================
ðŸ”˜ confluent.license

Confluent will issue a license key to each subscriber. The license key will be a short snippet of text that you can copy and paste. Without the license key, you can use the Replicator for a 30-day trial period. If you are a subscriber, please contact Confluent Support for more information.

	 - Type: PASSWORD
	 - Default: [hidden]
	 - Importance: HIGH
	 - Required: false

==========================
Source Topics
==========================
ðŸ”˜ topic.regex

Regex of topics to replicate to the destination cluster.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ topic.whitelist



	 - Type: false
	 - Default: LIST
	 - Importance: Whitelist of topics to be replicated.
	 - Required: HIGH

ðŸ”˜ topic.blacklist



	 - Type: false
	 - Default: LIST
	 - Importance: Topics to exclude from replication.
	 - Required: HIGH

ðŸ”˜ topic.poll.interval.ms

How often to poll the source cluster for new topics matching `topic.whitelist` or `topic.regex`.

	 - Type: INT
	 - Default: 120000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ consumer.poll.timeout.interval.ms

This config is used to avoid indefinite poll() to the source cluster. The poll() returns null records if there is no new data.

	 - Type: INT
	 - Default: 5000
	 - Importance: LOW
	 - Required: false

==========================
Source Data Conversion
==========================
ðŸ”˜ src.key.converter

Converter for the key field of messages retrieved from the source cluster.

	 - Type: CLASS
	 - Default: io.confluent.connect.replicator.util.ByteArrayConverter
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.value.converter

Converter for the value field of messages retrieved from the source cluster.

	 - Type: CLASS
	 - Default: io.confluent.connect.replicator.util.ByteArrayConverter
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.header.converter

HeaderConverter class used to convert serialized Kafka headers to Kafka Connect headers. Default value is ByteArrayConverter, which simply passes the input bytes into the destination record.

	 - Type: CLASS
	 - Default: io.confluent.connect.replicator.util.ByteArrayConverter
	 - Importance: LOW
	 - Required: false

==========================
Source Kafka
==========================
ðŸ”˜ src.kafka.bootstrap.servers



	 - Type: false
	 - Default: LIST
	 - Importance: A list of host/port pairs used to establish the initial connection to the Kafka cluster. Clients use this list to bootstrap and discover the full set of Kafka brokers. While the order of servers in the list does not matter, we recommend including more than one server to ensure resilience if any servers are down. This list does not need to contain the entire set of brokers, as Kafka clients automatically manage and update connections to the cluster efficiently. This list must be in the form ``host1:port1,host2:port2,...``.
	 - Required: HIGH

ðŸ”˜ src.kafka.client.id



	 - Type: false
	 - Default: STRING
	 - Importance: An id string to pass to the server when making requests. The purpose of this is to be able to track the source of requests beyond just ip/port by allowing a logical application name to be included in server-side request logging.
	 - Required: LOW

ðŸ”˜ src.kafka.request.timeout.ms

The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted.

	 - Type: INT
	 - Default: 30000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.retry.backoff.ms

The amount of time to wait before attempting to retry a failed request to a given topic partition. This avoids repeatedly sending requests in a tight loop under some failure scenarios. This value is the initial backoff value and will increase exponentially for each failed request, up to the ``retry.backoff.max.ms`` value.

	 - Type: LONG
	 - Default: 100
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.connections.max.idle.ms

Close idle connections after the number of milliseconds specified by this config.

	 - Type: LONG
	 - Default: 540000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.reconnect.backoff.ms

The base amount of time to wait before attempting to reconnect to a given host. This avoids repeatedly connecting to a host in a tight loop. This backoff applies to all connection attempts by the client to a broker. This value is the initial backoff value and will increase exponentially for each consecutive connection failure, up to the ``reconnect.backoff.max.ms`` value.

	 - Type: LONG
	 - Default: 50
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.metric.reporters

A list of classes to use as metrics reporters. Implementing the ``org.apache.kafka.common.metrics.MetricsReporter`` interface allows plugging in classes that will be notified of new metric creation. When custom reporters are set and ``org.apache.kafka.common.metrics.JmxReporter`` is needed, it has to be explicitly added to the list.

	 - Type: LIST
	 - Default: org.apache.kafka.common.metrics.JmxReporter
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.metrics.num.samples

The number of samples maintained to compute metrics.

	 - Type: INT
	 - Default: 2
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.metrics.sample.window.ms

The window of time a metrics sample is computed over.

	 - Type: LONG
	 - Default: 30000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.send.buffer.bytes

The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used.

	 - Type: INT
	 - Default: 131072
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.receive.buffer.bytes

The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS default will be used.

	 - Type: INT
	 - Default: 65536
	 - Importance: MEDIUM
	 - Required: false

==========================
Source Kafka: Security
==========================
ðŸ”˜ src.kafka.security.protocol

Protocol used to communicate with brokers.

	 - Type: STRING
	 - Default: PLAINTEXT
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.kerberos.service.name

The Kerberos principal name that Kafka runs as. This can be defined either in Kafka's JAAS config or in Kafka's config.

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.kerberos.kinit.cmd

Kerberos kinit command path.

	 - Type: STRING
	 - Default: /usr/bin/kinit
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.kerberos.ticket.renew.window.factor

Login thread will sleep until the specified window factor of time from last refresh to ticket's expiry has been reached, at which time it will try to renew the ticket.

	 - Type: DOUBLE
	 - Default: 0.8
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.kerberos.ticket.renew.jitter

Percentage of random jitter added to the renewal time.

	 - Type: DOUBLE
	 - Default: 0.05
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.kerberos.min.time.before.relogin

Login thread sleep time between refresh attempts.

	 - Type: LONG
	 - Default: 60000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.login.refresh.window.factor

Login refresh thread will sleep until the specified window factor relative to the credential's lifetime has been reached, at which time it will try to refresh the credential. Legal values are between 0.5 (50%) and 1.0 (100%) inclusive; a default value of 0.8 (80%) is used if no value is specified. Currently applies only to OAUTHBEARER.

	 - Type: DOUBLE
	 - Default: 0.8
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.login.refresh.window.jitter

The maximum amount of random jitter relative to the credential's lifetime that is added to the login refresh thread's sleep time. Legal values are between 0 and 0.25 (25%) inclusive; a default value of 0.05 (5%) is used if no value is specified. Currently applies only to OAUTHBEARER.

	 - Type: DOUBLE
	 - Default: 0.05
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.login.refresh.min.period.seconds

The desired minimum time for the login refresh thread to wait before refreshing a credential, in seconds. Legal values are between 0 and 900 (15 minutes); a default value of 60 (1 minute) is used if no value is specified.  This value and  sasl.login.refresh.buffer.seconds are both ignored if their sum exceeds the remaining lifetime of a credential. Currently applies only to OAUTHBEARER.

	 - Type: SHORT
	 - Default: 60
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.login.refresh.buffer.seconds

The amount of buffer time before credential expiration to maintain when refreshing a credential, in seconds. If a refresh would otherwise occur closer to expiration than the number of buffer seconds then the refresh will be moved up to maintain as much of the buffer time as possible. Legal values are between 0 and 3600 (1 hour); a default value of  300 (5 minutes) is used if no value is specified. This value and sasl.login.refresh.min.period.seconds are both ignored if their sum exceeds the remaining lifetime of a credential. Currently applies only to OAUTHBEARER.

	 - Type: SHORT
	 - Default: 300
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.mechanism

SASL mechanism used for client connections. This may be any mechanism for which a security provider is available. GSSAPI is the default mechanism.

	 - Type: STRING
	 - Default: GSSAPI
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.jaas.config

JAAS login context parameters for SASL connections in the format used by JAAS configuration files. JAAS configuration file format is described <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/LoginConfigFile.html">here</a>. The format for the value is: ``loginModuleClass controlFlag (optionName=optionValue)*;``. For brokers, the config must be prefixed with listener prefix and SASL mechanism name in lower-case. For example, listener.name.sasl_ssl.scram-sha-256.sasl.jaas.config=com.example.ScramLoginModule required;

	 - Type: PASSWORD
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.jaas.config.jndi.allowlist

This system property is used to allow list the JNDI properties that can be configured in the SASL JAAS configuration. This property accepts comma-separated list of JNDI properties. By default, all of the properties prefixed with java.naming are denied.If users want to use any of the java.naming JNDI properties, they need to explicitly set the system property as shown below to allow list the same. We advise the users to validate configurations and only allow trusted JNDI properties. For more details<a href="https://kafka.apache.org/cve-list#CVE-2023-25194">CVE-2023-25194</a> To add more JNDI properties, explicitly update the system property with comma-separated values. For example, to allow <b>java.naming.factory.initial</b> and <b>java.naming.provider.url</b>, set the property as follows: ``-Dsasl.jaas.config.jndi.allowlist=java.naming.factory.initial,java.naming.provider.url``

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.client.callback.handler.class

The fully qualified name of a SASL client callback handler class that implements the AuthenticateCallbackHandler interface.

	 - Type: CLASS
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.login.callback.handler.class

The fully qualified name of a SASL login callback handler class that implements the AuthenticateCallbackHandler interface. For brokers, login callback handler config must be prefixed with listener prefix and SASL mechanism name in lower-case. For example, listener.name.sasl_ssl.scram-sha-256.sasl.login.callback.handler.class=com.example.CustomScramLoginCallbackHandler

	 - Type: CLASS
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.login.class

The fully qualified name of a class that implements the Login interface. For brokers, login config must be prefixed with listener prefix and SASL mechanism name in lower-case. For example, listener.name.sasl_ssl.scram-sha-256.sasl.login.class=com.example.CustomScramLogin

	 - Type: CLASS
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.login.connect.timeout.ms

The (optional) value in milliseconds for the external authentication provider connection timeout. Currently applies only to OAUTHBEARER.

	 - Type: INT
	 - Default: null
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.login.read.timeout.ms

The (optional) value in milliseconds for the external authentication provider read timeout. Currently applies only to OAUTHBEARER.

	 - Type: INT
	 - Default: null
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.login.retry.backoff.max.ms

The (optional) value in milliseconds for the maximum wait between login attempts to the external authentication provider. Login uses an exponential backoff algorithm with an initial wait based on the sasl.login.retry.backoff.ms setting and will double in wait length between attempts up to a maximum wait length specified by the sasl.login.retry.backoff.max.ms setting. Currently applies only to OAUTHBEARER.

	 - Type: LONG
	 - Default: 10000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.login.retry.backoff.ms

The (optional) value in milliseconds for the initial wait between login attempts to the external authentication provider. Login uses an exponential backoff algorithm with an initial wait based on the sasl.login.retry.backoff.ms setting and will double in wait length between attempts up to a maximum wait length specified by the sasl.login.retry.backoff.max.ms setting. Currently applies only to OAUTHBEARER.

	 - Type: LONG
	 - Default: 100
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.scope.claim.name

The OAuth claim for the scope is often named "scope", but this (optional) setting can provide a different name to use for the scope included in the JWT payload's claims if the OAuth/OIDC provider uses a different name for that claim.

	 - Type: STRING
	 - Default: scope
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.sub.claim.name

The OAuth claim for the subject is often named "sub", but this (optional) setting can provide a different name to use for the subject included in the JWT payload's claims if the OAuth/OIDC provider uses a different name for that claim.

	 - Type: STRING
	 - Default: sub
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.token.endpoint.url

The URL for the OAuth/OIDC identity provider. If the URL is HTTP(S)-based, it is the issuer's token endpoint URL to which requests will be made to login based on the configuration in ``sasl.oauthbearer.jwt.retriever.class``. If the URL is file-based, it specifies a file containing an access token (in JWT serialized form) issued by the OAuth/OIDC identity provider to use for authorization.

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.jwks.endpoint.url

The OAuth/OIDC provider URL from which the provider's <a href="https://datatracker.ietf.org/doc/html/rfc7517#section-5">JWKS (JSON Web Key Set)</a> can be retrieved. The URL can be HTTP(S)-based or file-based. If the URL is HTTP(S)-based, the JWKS data will be retrieved from the OAuth/OIDC provider via the configured URL on broker startup. All then-current keys will be cached on the broker for incoming requests. If an authentication request is received for a JWT that includes a "kid" header claim value that isn't yet in the cache, the JWKS endpoint will be queried again on demand. However, the broker polls the URL every sasl.oauthbearer.jwks.endpoint.refresh.ms milliseconds to refresh the cache with any forthcoming keys before any JWT requests that include them are received. If the URL is file-based, the broker will load the JWKS file from a configured location on startup. In the event that the JWT includes a "kid" header value that isn't in the JWKS file, the broker will reject the JWT and authentication will fail.

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.jwks.endpoint.refresh.ms

The (optional) value in milliseconds for the broker to wait between refreshing its JWKS (JSON Web Key Set) cache that contains the keys to verify the signature of the JWT.

	 - Type: LONG
	 - Default: 3600000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms

The (optional) value in milliseconds for the maximum wait between attempts to retrieve the JWKS (JSON Web Key Set) from the external authentication provider. JWKS retrieval uses an exponential backoff algorithm with an initial wait based on the sasl.oauthbearer.jwks.endpoint.retry.backoff.ms setting and will double in wait length between attempts up to a maximum wait length specified by the sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms setting.

	 - Type: LONG
	 - Default: 10000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.jwks.endpoint.retry.backoff.ms

The (optional) value in milliseconds for the initial wait between JWKS (JSON Web Key Set) retrieval attempts from the external authentication provider. JWKS retrieval uses an exponential backoff algorithm with an initial wait based on the sasl.oauthbearer.jwks.endpoint.retry.backoff.ms setting and will double in wait length between attempts up to a maximum wait length specified by the sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms setting.

	 - Type: LONG
	 - Default: 100
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.clock.skew.seconds

The (optional) value in seconds to allow for differences between the time of the OAuth/OIDC identity provider and the broker.

	 - Type: INT
	 - Default: 30
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.expected.audience

The (optional) comma-delimited setting for the broker to use to verify that the JWT was issued for one of the expected audiences. The JWT will be inspected for the standard OAuth "aud" claim and if this value is set, the broker will match the value from JWT's "aud" claim  to see if there is an exact match. If there is no match, the broker will reject the JWT and authentication will fail.

	 - Type: LIST
	 - Default: null
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.expected.issuer

The (optional) setting for the broker to use to verify that the JWT was created by the expected issuer. The JWT will be inspected for the standard OAuth "iss" claim and if this value is set, the broker will match it exactly against what is in the JWT's "iss" claim. If there is no match, the broker will reject the JWT and authentication will fail.

	 - Type: STRING
	 - Default: null
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.header.urlencode

The (optional) setting to enable the OAuth client to URL-encode the client_id and client_secret in the authorization header in accordance with RFC6749, see <a href="https://datatracker.ietf.org/doc/html/rfc6749#section-2.3.1">here</a> for more details. The default value is set to 'false' for backward compatibility

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.jti.validation.enabled

Setting this flag true, would mandate the presence of `jti` (JWT ID) claim in the token.However, there is no validation on the value of this field

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.iat.validation.enabled

Setting this flag true, would mandate the presence of `iat` (Issued At) claim in the token.However, there is no validation on the value of this field

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.jwt.retriever.class

<p>The fully-qualified class name of a ``JwtRetriever`` implementation used to request tokens from the identity provider.</p><p>The default configuration value represents a class that maintains backward compatibility with previous versions of Apache Kafka. The default implementation uses the configuration to determine which concrete implementation to create.<p>Other implementations that are provided include:</p><ul><li>``org.apache.kafka.common.security.oauthbearer.ClientCredentialsJwtRetriever``</li><li>``org.apache.kafka.common.security.oauthbearer.DefaultJwtRetriever``</li><li>``org.apache.kafka.common.security.oauthbearer.FileJwtRetriever``</li><li>``org.apache.kafka.common.security.oauthbearer.JwtBearerJwtRetriever``</li></ul>

	 - Type: CLASS
	 - Default: org.apache.kafka.common.security.oauthbearer.DefaultJwtRetriever
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.jwt.validator.class

<p>The fully-qualified class name of a ``JwtValidator`` implementation used to validate the JWT from the identity provider.</p><p>The default validator (``org.apache.kafka.common.security.oauthbearer.DefaultJwtValidator``) maintains backward compatibility with previous versions of Apache Kafka. The default validator uses configuration to determine which concrete implementation to create.<p>The built-in ``JwtValidator`` implementations are:</p><ul><li>``org.apache.kafka.common.security.oauthbearer.BrokerJwtValidator``</li><li>``org.apache.kafka.common.security.oauthbearer.ClientJwtValidator``</li><li>``org.apache.kafka.common.security.oauthbearer.DefaultJwtValidator``</li></ul>

	 - Type: CLASS
	 - Default: org.apache.kafka.common.security.oauthbearer.DefaultJwtValidator
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.scope

<p>This is the level of access a client application is granted to a resource or API which is included in the token request. If provided, it should match one or more scopes configured in the identity provider.</p><p>The scope was previously stored as part of the ``sasl.jaas.config`` configuration with the key ``scope``. For backward compatibility, the ``scope`` JAAS option can still be used, but it is deprecated and will be removed in a future version.</p><p>Order of precedence:</p><ul><li>``sasl.oauthbearer.scope`` from configuration</li><li>``scope`` from JAAS</li></ul>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.client.credentials.client.id

<p>The ID (defined in/by the OAuth identity provider) to identify the client requesting the token.</p><p>The client ID was previously stored as part of the ``sasl.jaas.config`` configuration with the key ``clientId``. For backward compatibility, the ``clientId`` JAAS option can still be used, but it is deprecated and will be removed in a future version.</p><p>Order of precedence:</p><ul><li>``sasl.oauthbearer.client.credentials.client.id`` from configuration</li><li>``clientId`` from JAAS</li></ul>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.client.credentials.client.secret

<p>The secret (defined by either the user or preassigned, depending on the identity provider) of the client requesting the token.</p><p>The client secret was previously stored as part of the ``sasl.jaas.config`` configuration with the key ``clientSecret``. For backward compatibility, the ``clientSecret`` JAAS option can still be used, but it is deprecated and will be removed in a future version.</p><p>Order of precedence:</p><ul><li>``sasl.oauthbearer.client.credentials.client.secret`` from configuration</li><li>``clientSecret`` from JAAS</li></ul>

	 - Type: PASSWORD
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.assertion.algorithm

<p>The algorithm the Apache Kafka client should use to sign the assertion sent to the identity provider. It is also used as the value of the OAuth ``alg`` (Algorithm) header in the JWT assertion.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: STRING
	 - Default: RS256
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.assertion.claim.aud

<p>The JWT ``aud`` (Audience) claim which will be included in the  client JWT assertion created locally.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.assertion.claim.exp.seconds

<p>The number of seconds <em>in the future</em> for which the JWT is valid. The value is used to determine the JWT ``exp`` (Expiration) claim based on the current system time when the JWT is created.</p><p>The formula to generate the ``exp`` claim is very simple:</p><pre>Let:

	 - Type: INT
	 - Default: 300
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.assertion.claim.iss

<p>The value to be used as the ``iss`` (Issuer) claim which will be included in the client JWT assertion created locally.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.assertion.claim.jti.include

<p>Flag that determines if the JWT assertion should generate a unique ID for the JWT and include it in the ``jti`` (JWT ID) claim.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.assertion.claim.nbf.seconds

<p>The number of seconds <em>in the past</em> from which the JWT is valid. The value is used to determine the JWT ``nbf`` (Not Before) claim based on the current system time when the JWT is created.</p><p>The formula to generate the ``nbf`` claim is very simple:</p><pre>Let:

	 - Type: INT
	 - Default: 60
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.assertion.claim.sub

<p>The value to be used as the ``sub`` (Subject) claim which will be included in the client JWT assertion created locally.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.assertion.claim.exp.minutes

The (Optional) expiration time for the client assertion in minutesThe default value is 5 minutes

	 - Type: INT
	 - Default: 5
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.assertion.file

<p>File that contains a <em>pre-generated</em> JWT assertion.</p><p>The underlying implementation caches the file contents to avoid the performance hit of loading the file on each access. The caching mechanism will detect whenthe file changes to allow for the file to be reloaded on modifications. This allows for &quot;live&quot; assertion rotation without restarting the Kafka client.</p><p>The file contains the assertion in the serialized, three part JWT format:</p><ol><li>The <em>header</em> section is a base 64-encoded JWT header that contains values like ``alg`` (Algorithm), ``typ`` (Type, always the literal value ``JWT``), etc.</li><li>The <em>payload</em> section includes the base 64-encoded set of JWT claims, such as ``aud`` (Audience), ``iss`` (Issuer), ``sub`` (Subject), etc.</li><li>The <em>signature</em> section is the concatenated <em>header</em> and <em>payload</em> sections that was signed using a private key</li></ol><p>See <a href="https://datatracker.ietf.org/doc/html/rfc7519">RFC 7519</a> and <a href="https://datatracker.ietf.org/doc/html/rfc7515">RFC 7515</a> for more details on the JWT and JWS formats.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, all other ``sasl.oauthbearer.assertion.*`` configurations are ignored.</p>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.assertion.claim.nbf.include

The (optional) setting for specifying whether to include "not before" (nbf) claim or notIf set to true, nbf claim with (current time - 1 minute) will be included in the client assertion

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.assertion.private.key.file

<p>File that contains a private key in the standard PEM format which is used to sign the JWT assertion sent to the identity provider.</p><p>The underlying implementation caches the file contents to avoid the performance hit of loading the file on each access. The caching mechanism will detect when the file changes to allow for the file to be reloaded on modifications. This allows for &quot;live&quot; private key rotation without restarting the Kafka client.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.assertion.private.key.passphrase

<p>The optional passphrase to decrypt the private key file specified by ``sasl.oauthbearer.assertion.private.key.file``.</p><p><em>Note</em>: If the file referred to by ``sasl.oauthbearer.assertion.private.key.file`` is modified on the file system at runtime and it was created with a <em>different</em> passphrase than it was previously, the client will not be able to access the private key file because the passphrase is now out of date. For that reason, when using private key passphrases, either use the same passphrase each time, or â€” for improved security â€” restart the Kafka client using the new passphrase configuration.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: PASSWORD
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.sasl.oauthbearer.assertion.template.file

<p>This optional configuration specifies the file containing the JWT headers and/or payload claims to be used when creating the JWT assertion.</p><p>Not all identity providers require the same set of claims; some may require a given claim while others may prohibit it. In order to provide the most flexibility, this configuration allows the user to provide the static header values and claims that are to be included in the JWT.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.ssl.protocol

The SSL protocol used to generate the SSLContext. The default is 'TLSv1.3', which should be fine for most use cases. A typical alternative to the default is 'TLSv1.2'. Allowed values for this config are dependent on the JVM. Clients using the defaults for this config and 'ssl.enabled.protocols' will downgrade to 'TLSv1.2' if the server does not support 'TLSv1.3'. If this config is set to 'TLSv1.2', however, clients will not use 'TLSv1.3' even if it is one of the values in `ssl.enabled.protocols` and the server only supports 'TLSv1.3'.

	 - Type: STRING
	 - Default: TLSv1.3
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.ssl.provider

The name of the security provider used for SSL connections. Default value is the default security provider of the JVM.

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.ssl.cipher.suites

A list of cipher suites. This is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. By default all the available cipher suites are supported.

	 - Type: LIST
	 - Default: null
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.ssl.enabled.protocols

The list of protocols enabled for SSL connections. The default is 'TLSv1.2,TLSv1.3'. This means that clients and servers will prefer TLSv1.3 if both support it and fallback to TLSv1.2 otherwise (assuming both support at least TLSv1.2). This default should be fine for most use cases. Also see the config documentation for `ssl.protocol` to understand how it can impact the TLS version negotiation behavior.

	 - Type: LIST
	 - Default: TLSv1.2,TLSv1.3
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.ssl.keystore.type

The file format of the key store file. This is optional for client. The values currently supported by the default `ssl.engine.factory.class` are [JKS, PKCS12, PEM].

	 - Type: STRING
	 - Default: JKS
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.ssl.keystore.location

The location of the key store file. This is optional for client and can be used for two-way authentication for client.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ src.kafka.ssl.keystore.password

The store password for the key store file. This is optional for client and only needed if 'ssl.keystore.location' is configured. Key store password is not supported for PEM format.

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ src.kafka.ssl.key.password

The password of the private key in the key store file or the PEM key specified in 'ssl.keystore.key'.

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ src.kafka.ssl.keystore.key

Private key in the format specified by 'ssl.keystore.type'. Default SSL engine factory supports only PEM format with PKCS#8 keys. If the key is encrypted, key password must be specified using 'ssl.key.password'

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ src.kafka.ssl.keystore.certificate.chain

Certificate chain in the format specified by 'ssl.keystore.type'. Default SSL engine factory supports only PEM format with a list of X.509 certificates

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ src.kafka.ssl.truststore.certificates

Trusted certificates in the format specified by 'ssl.truststore.type'. Default SSL engine factory supports only PEM format with X.509 certificates.

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ src.kafka.ssl.truststore.type

The file format of the trust store file. The values currently supported by the default `ssl.engine.factory.class` are [JKS, PKCS12, PEM].

	 - Type: STRING
	 - Default: JKS
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.kafka.ssl.truststore.location

The location of the trust store file.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ src.kafka.ssl.truststore.password

The password for the trust store file. If a password is not set, trust store file configured will still be used, but integrity checking is disabled. Trust store password is not supported for PEM format.

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ src.kafka.ssl.keymanager.algorithm

The algorithm used by key manager factory for SSL connections. Default value is the key manager factory algorithm configured for the Java Virtual Machine.

	 - Type: STRING
	 - Default: SunX509
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.ssl.trustmanager.algorithm

The algorithm used by trust manager factory for SSL connections. Default value is the trust manager factory algorithm configured for the Java Virtual Machine.

	 - Type: STRING
	 - Default: PKIX
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.ssl.endpoint.identification.algorithm

The endpoint identification algorithm to validate server hostname using server certificate.

	 - Type: STRING
	 - Default: https
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.ssl.secure.random.implementation

The SecureRandom PRNG implementation to use for SSL cryptography operations.

	 - Type: STRING
	 - Default: null
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.kafka.ssl.engine.factory.class

The class of type org.apache.kafka.common.security.auth.SslEngineFactory to provide SSLEngine objects. Default value is org.apache.kafka.common.security.ssl.DefaultSslEngineFactory. Alternatively, setting this to org.apache.kafka.common.security.ssl.CommonNameLoggingSslEngineFactory will log the common name of expired SSL certificates used by clients to authenticate at any of the brokers with log level INFO. Note that this will cause a tiny delay during establishment of new connections from mTLS clients to brokers due to the extra code for examining the certificate chain provided by the client. Note further that the implementation uses a custom truststore based on the standard Java truststore and thus might be considered a security risk due to not being as mature as the standard one.

	 - Type: CLASS
	 - Default: null
	 - Importance: LOW
	 - Required: false

==========================
Destination Kafka: Security
==========================
ðŸ”˜ dest.kafka.security.protocol

Protocol used to communicate with brokers.

	 - Type: STRING
	 - Default: PLAINTEXT
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.kerberos.service.name

The Kerberos principal name that Kafka runs as. This can be defined either in Kafka's JAAS config or in Kafka's config.

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.kerberos.kinit.cmd

Kerberos kinit command path.

	 - Type: STRING
	 - Default: /usr/bin/kinit
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.kerberos.ticket.renew.window.factor

Login thread will sleep until the specified window factor of time from last refresh to ticket's expiry has been reached, at which time it will try to renew the ticket.

	 - Type: DOUBLE
	 - Default: 0.8
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.kerberos.ticket.renew.jitter

Percentage of random jitter added to the renewal time.

	 - Type: DOUBLE
	 - Default: 0.05
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.kerberos.min.time.before.relogin

Login thread sleep time between refresh attempts.

	 - Type: LONG
	 - Default: 60000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.login.refresh.window.factor

Login refresh thread will sleep until the specified window factor relative to the credential's lifetime has been reached, at which time it will try to refresh the credential. Legal values are between 0.5 (50%) and 1.0 (100%) inclusive; a default value of 0.8 (80%) is used if no value is specified. Currently applies only to OAUTHBEARER.

	 - Type: DOUBLE
	 - Default: 0.8
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.login.refresh.window.jitter

The maximum amount of random jitter relative to the credential's lifetime that is added to the login refresh thread's sleep time. Legal values are between 0 and 0.25 (25%) inclusive; a default value of 0.05 (5%) is used if no value is specified. Currently applies only to OAUTHBEARER.

	 - Type: DOUBLE
	 - Default: 0.05
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.login.refresh.min.period.seconds

The desired minimum time for the login refresh thread to wait before refreshing a credential, in seconds. Legal values are between 0 and 900 (15 minutes); a default value of 60 (1 minute) is used if no value is specified.  This value and  sasl.login.refresh.buffer.seconds are both ignored if their sum exceeds the remaining lifetime of a credential. Currently applies only to OAUTHBEARER.

	 - Type: SHORT
	 - Default: 60
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.login.refresh.buffer.seconds

The amount of buffer time before credential expiration to maintain when refreshing a credential, in seconds. If a refresh would otherwise occur closer to expiration than the number of buffer seconds then the refresh will be moved up to maintain as much of the buffer time as possible. Legal values are between 0 and 3600 (1 hour); a default value of  300 (5 minutes) is used if no value is specified. This value and sasl.login.refresh.min.period.seconds are both ignored if their sum exceeds the remaining lifetime of a credential. Currently applies only to OAUTHBEARER.

	 - Type: SHORT
	 - Default: 300
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.mechanism

SASL mechanism used for client connections. This may be any mechanism for which a security provider is available. GSSAPI is the default mechanism.

	 - Type: STRING
	 - Default: GSSAPI
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.jaas.config

JAAS login context parameters for SASL connections in the format used by JAAS configuration files. JAAS configuration file format is described <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/LoginConfigFile.html">here</a>. The format for the value is: ``loginModuleClass controlFlag (optionName=optionValue)*;``. For brokers, the config must be prefixed with listener prefix and SASL mechanism name in lower-case. For example, listener.name.sasl_ssl.scram-sha-256.sasl.jaas.config=com.example.ScramLoginModule required;

	 - Type: PASSWORD
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.jaas.config.jndi.allowlist

This system property is used to allow list the JNDI properties that can be configured in the SASL JAAS configuration. This property accepts comma-separated list of JNDI properties. By default, all of the properties prefixed with java.naming are denied.If users want to use any of the java.naming JNDI properties, they need to explicitly set the system property as shown below to allow list the same. We advise the users to validate configurations and only allow trusted JNDI properties. For more details<a href="https://kafka.apache.org/cve-list#CVE-2023-25194">CVE-2023-25194</a> To add more JNDI properties, explicitly update the system property with comma-separated values. For example, to allow <b>java.naming.factory.initial</b> and <b>java.naming.provider.url</b>, set the property as follows: ``-Dsasl.jaas.config.jndi.allowlist=java.naming.factory.initial,java.naming.provider.url``

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.client.callback.handler.class

The fully qualified name of a SASL client callback handler class that implements the AuthenticateCallbackHandler interface.

	 - Type: CLASS
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.login.callback.handler.class

The fully qualified name of a SASL login callback handler class that implements the AuthenticateCallbackHandler interface. For brokers, login callback handler config must be prefixed with listener prefix and SASL mechanism name in lower-case. For example, listener.name.sasl_ssl.scram-sha-256.sasl.login.callback.handler.class=com.example.CustomScramLoginCallbackHandler

	 - Type: CLASS
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.login.class

The fully qualified name of a class that implements the Login interface. For brokers, login config must be prefixed with listener prefix and SASL mechanism name in lower-case. For example, listener.name.sasl_ssl.scram-sha-256.sasl.login.class=com.example.CustomScramLogin

	 - Type: CLASS
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.login.connect.timeout.ms

The (optional) value in milliseconds for the external authentication provider connection timeout. Currently applies only to OAUTHBEARER.

	 - Type: INT
	 - Default: null
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.login.read.timeout.ms

The (optional) value in milliseconds for the external authentication provider read timeout. Currently applies only to OAUTHBEARER.

	 - Type: INT
	 - Default: null
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.login.retry.backoff.max.ms

The (optional) value in milliseconds for the maximum wait between login attempts to the external authentication provider. Login uses an exponential backoff algorithm with an initial wait based on the sasl.login.retry.backoff.ms setting and will double in wait length between attempts up to a maximum wait length specified by the sasl.login.retry.backoff.max.ms setting. Currently applies only to OAUTHBEARER.

	 - Type: LONG
	 - Default: 10000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.login.retry.backoff.ms

The (optional) value in milliseconds for the initial wait between login attempts to the external authentication provider. Login uses an exponential backoff algorithm with an initial wait based on the sasl.login.retry.backoff.ms setting and will double in wait length between attempts up to a maximum wait length specified by the sasl.login.retry.backoff.max.ms setting. Currently applies only to OAUTHBEARER.

	 - Type: LONG
	 - Default: 100
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.scope.claim.name

The OAuth claim for the scope is often named "scope", but this (optional) setting can provide a different name to use for the scope included in the JWT payload's claims if the OAuth/OIDC provider uses a different name for that claim.

	 - Type: STRING
	 - Default: scope
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.sub.claim.name

The OAuth claim for the subject is often named "sub", but this (optional) setting can provide a different name to use for the subject included in the JWT payload's claims if the OAuth/OIDC provider uses a different name for that claim.

	 - Type: STRING
	 - Default: sub
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.token.endpoint.url

The URL for the OAuth/OIDC identity provider. If the URL is HTTP(S)-based, it is the issuer's token endpoint URL to which requests will be made to login based on the configuration in ``sasl.oauthbearer.jwt.retriever.class``. If the URL is file-based, it specifies a file containing an access token (in JWT serialized form) issued by the OAuth/OIDC identity provider to use for authorization.

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.jwks.endpoint.url

The OAuth/OIDC provider URL from which the provider's <a href="https://datatracker.ietf.org/doc/html/rfc7517#section-5">JWKS (JSON Web Key Set)</a> can be retrieved. The URL can be HTTP(S)-based or file-based. If the URL is HTTP(S)-based, the JWKS data will be retrieved from the OAuth/OIDC provider via the configured URL on broker startup. All then-current keys will be cached on the broker for incoming requests. If an authentication request is received for a JWT that includes a "kid" header claim value that isn't yet in the cache, the JWKS endpoint will be queried again on demand. However, the broker polls the URL every sasl.oauthbearer.jwks.endpoint.refresh.ms milliseconds to refresh the cache with any forthcoming keys before any JWT requests that include them are received. If the URL is file-based, the broker will load the JWKS file from a configured location on startup. In the event that the JWT includes a "kid" header value that isn't in the JWKS file, the broker will reject the JWT and authentication will fail.

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.jwks.endpoint.refresh.ms

The (optional) value in milliseconds for the broker to wait between refreshing its JWKS (JSON Web Key Set) cache that contains the keys to verify the signature of the JWT.

	 - Type: LONG
	 - Default: 3600000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms

The (optional) value in milliseconds for the maximum wait between attempts to retrieve the JWKS (JSON Web Key Set) from the external authentication provider. JWKS retrieval uses an exponential backoff algorithm with an initial wait based on the sasl.oauthbearer.jwks.endpoint.retry.backoff.ms setting and will double in wait length between attempts up to a maximum wait length specified by the sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms setting.

	 - Type: LONG
	 - Default: 10000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.jwks.endpoint.retry.backoff.ms

The (optional) value in milliseconds for the initial wait between JWKS (JSON Web Key Set) retrieval attempts from the external authentication provider. JWKS retrieval uses an exponential backoff algorithm with an initial wait based on the sasl.oauthbearer.jwks.endpoint.retry.backoff.ms setting and will double in wait length between attempts up to a maximum wait length specified by the sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms setting.

	 - Type: LONG
	 - Default: 100
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.clock.skew.seconds

The (optional) value in seconds to allow for differences between the time of the OAuth/OIDC identity provider and the broker.

	 - Type: INT
	 - Default: 30
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.expected.audience

The (optional) comma-delimited setting for the broker to use to verify that the JWT was issued for one of the expected audiences. The JWT will be inspected for the standard OAuth "aud" claim and if this value is set, the broker will match the value from JWT's "aud" claim  to see if there is an exact match. If there is no match, the broker will reject the JWT and authentication will fail.

	 - Type: LIST
	 - Default: null
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.expected.issuer

The (optional) setting for the broker to use to verify that the JWT was created by the expected issuer. The JWT will be inspected for the standard OAuth "iss" claim and if this value is set, the broker will match it exactly against what is in the JWT's "iss" claim. If there is no match, the broker will reject the JWT and authentication will fail.

	 - Type: STRING
	 - Default: null
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.header.urlencode

The (optional) setting to enable the OAuth client to URL-encode the client_id and client_secret in the authorization header in accordance with RFC6749, see <a href="https://datatracker.ietf.org/doc/html/rfc6749#section-2.3.1">here</a> for more details. The default value is set to 'false' for backward compatibility

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.jti.validation.enabled

Setting this flag true, would mandate the presence of `jti` (JWT ID) claim in the token.However, there is no validation on the value of this field

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.iat.validation.enabled

Setting this flag true, would mandate the presence of `iat` (Issued At) claim in the token.However, there is no validation on the value of this field

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.jwt.retriever.class

<p>The fully-qualified class name of a ``JwtRetriever`` implementation used to request tokens from the identity provider.</p><p>The default configuration value represents a class that maintains backward compatibility with previous versions of Apache Kafka. The default implementation uses the configuration to determine which concrete implementation to create.<p>Other implementations that are provided include:</p><ul><li>``org.apache.kafka.common.security.oauthbearer.ClientCredentialsJwtRetriever``</li><li>``org.apache.kafka.common.security.oauthbearer.DefaultJwtRetriever``</li><li>``org.apache.kafka.common.security.oauthbearer.FileJwtRetriever``</li><li>``org.apache.kafka.common.security.oauthbearer.JwtBearerJwtRetriever``</li></ul>

	 - Type: CLASS
	 - Default: org.apache.kafka.common.security.oauthbearer.DefaultJwtRetriever
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.jwt.validator.class

<p>The fully-qualified class name of a ``JwtValidator`` implementation used to validate the JWT from the identity provider.</p><p>The default validator (``org.apache.kafka.common.security.oauthbearer.DefaultJwtValidator``) maintains backward compatibility with previous versions of Apache Kafka. The default validator uses configuration to determine which concrete implementation to create.<p>The built-in ``JwtValidator`` implementations are:</p><ul><li>``org.apache.kafka.common.security.oauthbearer.BrokerJwtValidator``</li><li>``org.apache.kafka.common.security.oauthbearer.ClientJwtValidator``</li><li>``org.apache.kafka.common.security.oauthbearer.DefaultJwtValidator``</li></ul>

	 - Type: CLASS
	 - Default: org.apache.kafka.common.security.oauthbearer.DefaultJwtValidator
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.scope

<p>This is the level of access a client application is granted to a resource or API which is included in the token request. If provided, it should match one or more scopes configured in the identity provider.</p><p>The scope was previously stored as part of the ``sasl.jaas.config`` configuration with the key ``scope``. For backward compatibility, the ``scope`` JAAS option can still be used, but it is deprecated and will be removed in a future version.</p><p>Order of precedence:</p><ul><li>``sasl.oauthbearer.scope`` from configuration</li><li>``scope`` from JAAS</li></ul>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.client.credentials.client.id

<p>The ID (defined in/by the OAuth identity provider) to identify the client requesting the token.</p><p>The client ID was previously stored as part of the ``sasl.jaas.config`` configuration with the key ``clientId``. For backward compatibility, the ``clientId`` JAAS option can still be used, but it is deprecated and will be removed in a future version.</p><p>Order of precedence:</p><ul><li>``sasl.oauthbearer.client.credentials.client.id`` from configuration</li><li>``clientId`` from JAAS</li></ul>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.client.credentials.client.secret

<p>The secret (defined by either the user or preassigned, depending on the identity provider) of the client requesting the token.</p><p>The client secret was previously stored as part of the ``sasl.jaas.config`` configuration with the key ``clientSecret``. For backward compatibility, the ``clientSecret`` JAAS option can still be used, but it is deprecated and will be removed in a future version.</p><p>Order of precedence:</p><ul><li>``sasl.oauthbearer.client.credentials.client.secret`` from configuration</li><li>``clientSecret`` from JAAS</li></ul>

	 - Type: PASSWORD
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.assertion.algorithm

<p>The algorithm the Apache Kafka client should use to sign the assertion sent to the identity provider. It is also used as the value of the OAuth ``alg`` (Algorithm) header in the JWT assertion.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: STRING
	 - Default: RS256
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.assertion.claim.aud

<p>The JWT ``aud`` (Audience) claim which will be included in the  client JWT assertion created locally.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.assertion.claim.exp.seconds

<p>The number of seconds <em>in the future</em> for which the JWT is valid. The value is used to determine the JWT ``exp`` (Expiration) claim based on the current system time when the JWT is created.</p><p>The formula to generate the ``exp`` claim is very simple:</p><pre>Let:

	 - Type: INT
	 - Default: 300
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.assertion.claim.iss

<p>The value to be used as the ``iss`` (Issuer) claim which will be included in the client JWT assertion created locally.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.assertion.claim.jti.include

<p>Flag that determines if the JWT assertion should generate a unique ID for the JWT and include it in the ``jti`` (JWT ID) claim.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.assertion.claim.nbf.seconds

<p>The number of seconds <em>in the past</em> from which the JWT is valid. The value is used to determine the JWT ``nbf`` (Not Before) claim based on the current system time when the JWT is created.</p><p>The formula to generate the ``nbf`` claim is very simple:</p><pre>Let:

	 - Type: INT
	 - Default: 60
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.assertion.claim.sub

<p>The value to be used as the ``sub`` (Subject) claim which will be included in the client JWT assertion created locally.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.assertion.claim.exp.minutes

The (Optional) expiration time for the client assertion in minutesThe default value is 5 minutes

	 - Type: INT
	 - Default: 5
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.assertion.file

<p>File that contains a <em>pre-generated</em> JWT assertion.</p><p>The underlying implementation caches the file contents to avoid the performance hit of loading the file on each access. The caching mechanism will detect whenthe file changes to allow for the file to be reloaded on modifications. This allows for &quot;live&quot; assertion rotation without restarting the Kafka client.</p><p>The file contains the assertion in the serialized, three part JWT format:</p><ol><li>The <em>header</em> section is a base 64-encoded JWT header that contains values like ``alg`` (Algorithm), ``typ`` (Type, always the literal value ``JWT``), etc.</li><li>The <em>payload</em> section includes the base 64-encoded set of JWT claims, such as ``aud`` (Audience), ``iss`` (Issuer), ``sub`` (Subject), etc.</li><li>The <em>signature</em> section is the concatenated <em>header</em> and <em>payload</em> sections that was signed using a private key</li></ol><p>See <a href="https://datatracker.ietf.org/doc/html/rfc7519">RFC 7519</a> and <a href="https://datatracker.ietf.org/doc/html/rfc7515">RFC 7515</a> for more details on the JWT and JWS formats.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, all other ``sasl.oauthbearer.assertion.*`` configurations are ignored.</p>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.assertion.claim.nbf.include

The (optional) setting for specifying whether to include "not before" (nbf) claim or notIf set to true, nbf claim with (current time - 1 minute) will be included in the client assertion

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.assertion.private.key.file

<p>File that contains a private key in the standard PEM format which is used to sign the JWT assertion sent to the identity provider.</p><p>The underlying implementation caches the file contents to avoid the performance hit of loading the file on each access. The caching mechanism will detect when the file changes to allow for the file to be reloaded on modifications. This allows for &quot;live&quot; private key rotation without restarting the Kafka client.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.assertion.private.key.passphrase

<p>The optional passphrase to decrypt the private key file specified by ``sasl.oauthbearer.assertion.private.key.file``.</p><p><em>Note</em>: If the file referred to by ``sasl.oauthbearer.assertion.private.key.file`` is modified on the file system at runtime and it was created with a <em>different</em> passphrase than it was previously, the client will not be able to access the private key file because the passphrase is now out of date. For that reason, when using private key passphrases, either use the same passphrase each time, or â€” for improved security â€” restart the Kafka client using the new passphrase configuration.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: PASSWORD
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.sasl.oauthbearer.assertion.template.file

<p>This optional configuration specifies the file containing the JWT headers and/or payload claims to be used when creating the JWT assertion.</p><p>Not all identity providers require the same set of claims; some may require a given claim while others may prohibit it. In order to provide the most flexibility, this configuration allows the user to provide the static header values and claims that are to be included in the JWT.</p><p><em>Note</em>: If a value for ``sasl.oauthbearer.assertion.file`` is provided, this configuration will be ignored.</p>

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.ssl.protocol

The SSL protocol used to generate the SSLContext. The default is 'TLSv1.3', which should be fine for most use cases. A typical alternative to the default is 'TLSv1.2'. Allowed values for this config are dependent on the JVM. Clients using the defaults for this config and 'ssl.enabled.protocols' will downgrade to 'TLSv1.2' if the server does not support 'TLSv1.3'. If this config is set to 'TLSv1.2', however, clients will not use 'TLSv1.3' even if it is one of the values in `ssl.enabled.protocols` and the server only supports 'TLSv1.3'.

	 - Type: STRING
	 - Default: TLSv1.3
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.ssl.provider

The name of the security provider used for SSL connections. Default value is the default security provider of the JVM.

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.ssl.cipher.suites

A list of cipher suites. This is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. By default all the available cipher suites are supported.

	 - Type: LIST
	 - Default: null
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.ssl.enabled.protocols

The list of protocols enabled for SSL connections. The default is 'TLSv1.2,TLSv1.3'. This means that clients and servers will prefer TLSv1.3 if both support it and fallback to TLSv1.2 otherwise (assuming both support at least TLSv1.2). This default should be fine for most use cases. Also see the config documentation for `ssl.protocol` to understand how it can impact the TLS version negotiation behavior.

	 - Type: LIST
	 - Default: TLSv1.2,TLSv1.3
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.ssl.keystore.type

The file format of the key store file. This is optional for client. The values currently supported by the default `ssl.engine.factory.class` are [JKS, PKCS12, PEM].

	 - Type: STRING
	 - Default: JKS
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.ssl.keystore.location

The location of the key store file. This is optional for client and can be used for two-way authentication for client.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ dest.kafka.ssl.keystore.password

The store password for the key store file. This is optional for client and only needed if 'ssl.keystore.location' is configured. Key store password is not supported for PEM format.

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ dest.kafka.ssl.key.password

The password of the private key in the key store file or the PEM key specified in 'ssl.keystore.key'.

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ dest.kafka.ssl.keystore.key

Private key in the format specified by 'ssl.keystore.type'. Default SSL engine factory supports only PEM format with PKCS#8 keys. If the key is encrypted, key password must be specified using 'ssl.key.password'

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ dest.kafka.ssl.keystore.certificate.chain

Certificate chain in the format specified by 'ssl.keystore.type'. Default SSL engine factory supports only PEM format with a list of X.509 certificates

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ dest.kafka.ssl.truststore.certificates

Trusted certificates in the format specified by 'ssl.truststore.type'. Default SSL engine factory supports only PEM format with X.509 certificates.

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ dest.kafka.ssl.truststore.type

The file format of the trust store file. The values currently supported by the default `ssl.engine.factory.class` are [JKS, PKCS12, PEM].

	 - Type: STRING
	 - Default: JKS
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.ssl.truststore.location

The location of the trust store file.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ dest.kafka.ssl.truststore.password

The password for the trust store file. If a password is not set, trust store file configured will still be used, but integrity checking is disabled. Trust store password is not supported for PEM format.

	 - Type: PASSWORD
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ dest.kafka.ssl.keymanager.algorithm

The algorithm used by key manager factory for SSL connections. Default value is the key manager factory algorithm configured for the Java Virtual Machine.

	 - Type: STRING
	 - Default: SunX509
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.ssl.trustmanager.algorithm

The algorithm used by trust manager factory for SSL connections. Default value is the trust manager factory algorithm configured for the Java Virtual Machine.

	 - Type: STRING
	 - Default: PKIX
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.ssl.endpoint.identification.algorithm

The endpoint identification algorithm to validate server hostname using server certificate.

	 - Type: STRING
	 - Default: https
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.ssl.secure.random.implementation

The SecureRandom PRNG implementation to use for SSL cryptography operations.

	 - Type: STRING
	 - Default: null
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.ssl.engine.factory.class

The class of type org.apache.kafka.common.security.auth.SslEngineFactory to provide SSLEngine objects. Default value is org.apache.kafka.common.security.ssl.DefaultSslEngineFactory. Alternatively, setting this to org.apache.kafka.common.security.ssl.CommonNameLoggingSslEngineFactory will log the common name of expired SSL certificates used by clients to authenticate at any of the brokers with log level INFO. Note that this will cause a tiny delay during establishment of new connections from mTLS clients to brokers due to the extra code for examining the certificate chain provided by the client. Note further that the implementation uses a custom truststore based on the standard Java truststore and thus might be considered a security risk due to not being as mature as the standard one.

	 - Type: CLASS
	 - Default: null
	 - Importance: LOW
	 - Required: false

==========================
Source Kafka: Consumer
==========================
ðŸ”˜ src.consumer.interceptor.classes



	 - Type: false
	 - Default: LIST
	 - Importance: A list of classes to use as interceptors. Implementing the ``org.apache.kafka.clients.consumer.ConsumerInterceptor`` interface allows you to intercept (and possibly mutate) records received by the consumer. By default, there are no interceptors.
	 - Required: LOW

ðŸ”˜ src.consumer.fetch.max.wait.ms

The maximum amount of time the server will block before answering the fetch request there isn't sufficient data to immediately satisfy the requirement given by fetch.min.bytes.

	 - Type: INT
	 - Default: 500
	 - Importance: LOW
	 - Required: false

ðŸ”˜ src.consumer.fetch.min.bytes

The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request. The default setting of 1 byte means that fetch requests are answered as soon as that many byte(s) of data is available or the fetch request times out waiting for data to arrive. Setting this to a larger value will cause the server to wait for larger amounts of data to accumulate which can improve server throughput a bit at the cost of some additional latency.

	 - Type: INT
	 - Default: 1
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ src.consumer.fetch.max.bytes

The maximum amount of data the server should return for a fetch request. Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not a absolute maximum. The maximum record batch size accepted by the broker is defined via ``message.max.bytes`` (broker config) or ``max.message.bytes`` (topic config). A fetch request consists of many partitions, and there is another setting that controls how much data is returned for each partition in a fetch request - see ``max.partition.fetch.bytes``. Note that there is a current limitation when performing remote reads from tiered storage (KIP-405) - only one partition out of the fetch request is fetched from the remote store (KAFKA-14915). Note also that the consumer performs multiple fetches in parallel.

	 - Type: INT
	 - Default: 52428800
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.consumer.max.partition.fetch.bytes

The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer. If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. The maximum record batch size accepted by the broker is defined via ``message.max.bytes`` (broker config) or ``max.message.bytes`` (topic config). See fetch.max.bytes for limiting the consumer request size. Consider increasing ``max.partition.fetch.bytes`` especially in the cases of remote storage reads (KIP-405), because currently only one partition per fetch request is served from the remote store (KAFKA-14915).

	 - Type: INT
	 - Default: 1048576
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ src.consumer.max.poll.interval.ms

The maximum delay between invocations of poll() when using consumer group management. This places an upper bound on the amount of time that the consumer can be idle before fetching more records. If poll() is not called before expiration of this timeout, then the consumer is considered failed and the group will rebalance in order to reassign the partitions to another member. For consumers using a non-null ``group.instance.id`` which reach this timeout, partitions will not be immediately reassigned. Instead, the consumer will stop sending heartbeats and partitions will be reassigned after expiration of the session timeout (defined by the client config ``session.timeout.ms`` if using the Classic rebalance protocol, or by the broker config ``group.consumer.session.timeout.ms`` if using the Consumer protocol). This mirrors the behavior of a static consumer which has shutdown.

	 - Type: INT
	 - Default: 300000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.consumer.max.poll.records

The maximum number of records returned in a single call to poll(). Note, that ``max.poll.records`` does not impact the underlying fetching behavior. The consumer will cache the records from each fetch request and returns them incrementally from each poll.

	 - Type: INT
	 - Default: 500
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ src.consumer.check.crcs

Automatically check the CRC32 of the records consumed. This ensures no on-the-wire or on-disk corruption to the messages occurred. This check adds some overhead, so it may be disabled in cases seeking extreme performance.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

==========================
Destination Kafka
==========================
ðŸ”˜ dest.kafka.bootstrap.servers



	 - Type: false
	 - Default: LIST
	 - Importance: A list of host/port pairs used to establish the initial connection to the Kafka cluster. Clients use this list to bootstrap and discover the full set of Kafka brokers. While the order of servers in the list does not matter, we recommend including more than one server to ensure resilience if any servers are down. This list does not need to contain the entire set of brokers, as Kafka clients automatically manage and update connections to the cluster efficiently. This list must be in the form ``host1:port1,host2:port2,...``.
	 - Required: HIGH

ðŸ”˜ dest.kafka.client.id



	 - Type: false
	 - Default: STRING
	 - Importance: An id string to pass to the server when making requests. The purpose of this is to be able to track the source of requests beyond just ip/port by allowing a logical application name to be included in server-side request logging.
	 - Required: LOW

ðŸ”˜ dest.kafka.request.timeout.ms

The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted.

	 - Type: INT
	 - Default: 30000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.retry.backoff.ms

The amount of time to wait before attempting to retry a failed request to a given topic partition. This avoids repeatedly sending requests in a tight loop under some failure scenarios. This value is the initial backoff value and will increase exponentially for each failed request, up to the ``retry.backoff.max.ms`` value.

	 - Type: LONG
	 - Default: 100
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.connections.max.idle.ms

Close idle connections after the number of milliseconds specified by this config.

	 - Type: LONG
	 - Default: 540000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.reconnect.backoff.ms

The base amount of time to wait before attempting to reconnect to a given host. This avoids repeatedly connecting to a host in a tight loop. This backoff applies to all connection attempts by the client to a broker. This value is the initial backoff value and will increase exponentially for each consecutive connection failure, up to the ``reconnect.backoff.max.ms`` value.

	 - Type: LONG
	 - Default: 50
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.metric.reporters

A list of classes to use as metrics reporters. Implementing the ``org.apache.kafka.common.metrics.MetricsReporter`` interface allows plugging in classes that will be notified of new metric creation. When custom reporters are set and ``org.apache.kafka.common.metrics.JmxReporter`` is needed, it has to be explicitly added to the list.

	 - Type: LIST
	 - Default: org.apache.kafka.common.metrics.JmxReporter
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.metrics.num.samples

The number of samples maintained to compute metrics.

	 - Type: INT
	 - Default: 2
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.metrics.sample.window.ms

The window of time a metrics sample is computed over.

	 - Type: LONG
	 - Default: 30000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ dest.kafka.send.buffer.bytes

The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used.

	 - Type: INT
	 - Default: 131072
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ dest.kafka.receive.buffer.bytes

The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS default will be used.

	 - Type: INT
	 - Default: 65536
	 - Importance: MEDIUM
	 - Required: false

==========================
Destination Topics
==========================
ðŸ”˜ topic.rename.format

A format string for the topic name in the destination cluster, which may contain '${topic}' as a placeholder for the originating topic name. For example, ``dc_${topic}`` for the topic 'orders' will map to the destination topic name 'dc_orders'.

	 - Type: STRING
	 - Default: ${topic}
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ topic.auto.create

Whether to automatically create topics in the destination cluster if required.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ topic.preserve.partitions

Whether to automatically increase the number of partitions in the destination cluster to match the source cluster and ensure that messages replicated from the source cluster use the same partition in the destination cluster.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ topic.create.backoff.ms

Time to wait before retrying auto topic creation or expansion.

	 - Type: INT
	 - Default: 120000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ topic.config.sync

Whether to periodically sync topic configuration to the destination cluster.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ topic.config.sync.interval.ms

How often to check for configuration changes when ``topic.config.sync`` is enabled.

	 - Type: INT
	 - Default: 120000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ topic.timestamp.type

The timestamp type for the topics in the destination cluster.

	 - Type: STRING
	 - Default: CreateTime
	 - Importance: LOW
	 - Required: false

ðŸ”˜ confluent.topic

Topic used for Confluent Platform configuration, including licensing information.

	 - Type: STRING
	 - Default: _confluent-command
	 - Importance: LOW
	 - Required: false

ðŸ”˜ provenance.header.enable

Whether to enable the use of provenance headers during replication. If true, Replicator will not replicate messages with a provenance header and will add a provenance header to messages that it does replicate.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ provenance.header.filter.overrides



	 - Type: false
	 - Default: STRING
	 - Importance: Filter overrides of the form <clusterId>,<topic>,<startTimeMs>-<endTimeMs>. These overrides are matched against the cluster ID, topic, and timestamp in the provenance header of records that would normally be filtered out during replication. If a match occurs, the filter is ignored and the record is replicated. The start time is inclusive and the end time is exclusive. Multiple overrides must be separated by semicolons.
	 - Required: LOW

ðŸ”˜ dest.topic.replication.factor

Sets the replication factor for the topic at the destination cluster

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

==========================
Offset Management
==========================
ðŸ”˜ offset.start

Specify the preference for determining where to start replication. Use ``connect`` to prefer the Connect offset if it exists, and the consumer offset otherwise. Use ``consumer`` to prefer the consumer offset if it exists, and the Connect offset otherwise. If neither the Connect offset nor the consumer offset exist, the starting offset will be the beginning of the topic.

	 - Type: STRING
	 - Default: connect
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ offset.topic.commit

Whether to commit Replicator's consumer offsets to the source Kafka cluster after the messages have been written to the destination cluster.  These consumer offsets can be used to easily track the lag of Replicator.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ offset.topic.commit.batch.period.ms

The period in milliseconds during which Replicator flushes consumer offsets to source Kafka cluster after the messages have been written to the destination cluster. If -1 is provided, Replicator uses the Connect framework commit offset flush period.

	 - Type: INT
	 - Default: 60000
	 - Importance: MEDIUM
	 - Required: false

==========================
Offset Translation
==========================
ðŸ”˜ offset.translator.tasks.max

The maximum number of Replicator tasks that will perform offset translation.  If -1 (the default), all tasks will perform offset translation.

	 - Type: INT
	 - Default: -1
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ offset.translator.tasks.separate

Whether to translate offsets in separate tasks from those performing topic replication.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ offset.translator.batch.period.ms

The period in milliseconds during which offset translation requests will be batched.

	 - Type: INT
	 - Default: 60000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ offset.translator.batch.size

The maximum size of a batch for offset translation requests.

	 - Type: INT
	 - Default: 2147483647
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ offset.timestamps.commit

Whether to commit timestamps for Replicator's own consumer group. These are used in active-passive scenarios after a failover, when another instance of Replicator is configured with the same group ID in order to replicate messages written to the secondary data center back to the primary data center.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Schema Translation
==========================
ðŸ”˜ schema.registry.topic

The topic that acts as the durable log for the schema registry.

	 - Type: STRING
	 - Default: null
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ schema.registry.url

Comma-separated list of URLs for schema registry instances that can be used to register or look up schemas.

	 - Type: LIST
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ schema.registry.max.schemas.per.subject

Maximum number of schemas to cache locally.

	 - Type: INT
	 - Default: 1000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ schema.registry.client.basic.auth.credentials.source

Specify how to pick the credentials for Basic Auth header. The supported values are URL, USER_INFO and SASL_INHERIT

	 - Type: STRING
	 - Default: URL
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ schema.registry.client.basic.auth.user.info

Specify the user info for Basic Auth in the form of {username}:{password}

	 - Type: PASSWORD
	 - Default: [hidden]
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ schema.subject.translator.class

Translator for the schema subject, or null if no translation is to be performed. Properties with the prefix 'schema.subject.translator.' will be passed to the configure() method of the translator.

	 - Type: CLASS
	 - Default: null
	 - Importance: LOW
	 - Required: false

==========================
null
==========================
ðŸ”˜ header.converter.converter.type

How this converter will be used.

	 - Type: STRING
	 - Default: null
	 - Importance: LOW
	 - Required: true

ðŸ”˜ key.converter.converter.type

How this converter will be used.

	 - Type: STRING
	 - Default: null
	 - Importance: LOW
	 - Required: true

ðŸ”˜ value.converter.converter.type

How this converter will be used.

	 - Type: STRING
	 - Default: null
	 - Importance: LOW
	 - Required: true

